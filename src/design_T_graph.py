import os
import re
import ast
import json
import base64
from dotenv import load_dotenv
from langgraph.graph.state import StateGraph, START, END
from typing import List
from langgraph.graph import add_messages
from langchain_core.messages import BaseMessage
from typing import TypedDict, Annotated, Sequence
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from src.memory import get_long_term_store, get_short_term_memory
from src.tools.img_recognition import img_recognition
# from src.tools.IMG_rag_tool import IMG_rag_tool
from src.tools.prompt_generation import prompt_generation
from src.tools.case_render_image import case_render_image
from src.tools.simulate_future_image import simulate_future_image
from src.tools.ARCH_rag_tool import ARCH_rag_tool
from src.tools.video_recognition import video_recognition
from src.tools.generate_3D import generate_3D


# è¼‰å…¥ .env è¨­å®š
load_dotenv()

# ä½¿ç”¨ tools_memory æä¾›çš„çŸ­æœŸè¨˜æ†¶èˆ‡é•·æœŸè¨˜æ†¶å­˜å„²
short_term = get_short_term_memory()
long_term = get_long_term_store()

# =============================================================================
# å»ºç«‹ LLM å¯¦ä¾‹ï¼ˆä¸å†è¨­å®š system_message å±¬æ€§ï¼‰
# =============================================================================
llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.7
    )

# ä¾éœ€æ±‚ç”¢ç”Ÿä¸åŒç”¨é€”çš„ LLM å¯¦ä¾‹
llm_with_img = llm.bind_tools([img_recognition])
# llm_with_3d = llm.bind_tools({"3D_recognition": tools["3D_recognition"]})
# llm_with_IMGrag = llm.bind_tools([IMG_rag_tool])
llm_with_ARCHrag = llm.bind_tools([ARCH_rag_tool])
llm_with_prompt = llm.bind_tools([prompt_generation])
llm_with_gen2 = llm.bind_tools([simulate_future_image])


# =============================================================================
# å®šç¾©å…¨å±€ç‹€æ…‹ (GlobalState)
# =============================================================================
class GlobalState(TypedDict, total=False):
    è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½ : Annotated[Sequence[BaseMessage], add_messages]
    design_summary: str
    analysis_img: str
    site_analysis: str
    design_advice: list
    outer_prompt: list
    case_image: list
    future_image: list
    perspective_3D: list 
    model_3D: list
    GATE1: str
    GATE2: str 
    GATE_REASON1: str
    GATE_REASON2: str
    current_round: int
    evaluation_count: int
    evaluation_result: list
    evaluation_status: str
    final_evaluation: str

def custom_add_messages(existing: list, new: list) -> list:
    # ç¢ºä¿ new ç‚ºåˆ—è¡¨
    if not isinstance(new, list):
        new = [new]
    processed_new = []
    for msg in new:
        # å¦‚æœè¨Šæ¯å…·æœ‰ role å±¬æ€§ä¸”é è¨­ç‚º "human"ï¼Œå‰‡ä¿®æ”¹ç‚º "system"
        if hasattr(msg, "role"):
            if msg.role == "human":
                msg.role = "system"
        processed_new.append(msg)
    return existing + processed_new

# =============================================================================
# å„ä»»å‹™å®šç¾©
# =============================================================================

# å•é¡Œä»£ç†ä»»å‹™ï¼šæç¤ºç”¨æˆ¶è¼¸å…¥è¨­è¨ˆç›®æ¨™ã€è¨­è¨ˆéœ€æ±‚ã€æ–¹æ¡ˆé¡å‹ã€æ–¹æ¡ˆåå¥½ OK
class QuestionTask:
    def __init__(self, state: GlobalState):
        self.state = state
    
    def run(self, state=None):
        if state is not None:
            self.state = state
        user_input = self.state["è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½"][0].content
        print("âœ… ç”¨æˆ¶çš„è¨­è¨ˆéœ€æ±‚å·²è¨˜éŒ„ï¼š", user_input)

        # Step 1: LLM æŸ¥çœ‹ç”¨æˆ¶è¼¸å…¥ï¼Œç”Ÿæˆé—œéµè©
        keyword_prompt = (
            "è«‹å¾ç”¨æˆ¶è¼¸å…¥æ–‡æœ¬ç”Ÿæˆä¸­è‹±æ–‡é—œéµè©ä»¥ä¾¿æ–¼æª¢ç´¢å»ºç¯‰è¨­è¨ˆç›®æ¨™ã€è¨­è¨ˆéœ€æ±‚ã€æ–¹æ¡ˆåå¥½ç­‰ç›¸é—œè³‡è¨Šã€‚"
            "éœ€è¦ç‰¹åˆ¥é—œæ³¨æœ¨æ§‹é€ ã€æ•¸ä½è£½é€ å·¥æ³•ã€å‚³çµ±è£½é€ å·¥æ³•ã€æœ¨çµæ§‹ç­‰é …ç›®ã€‚"
            "è«‹ä½¿ç”¨ç”¨æˆ¶çš„è¼¸å…¥èªè¨€ä¾†å›ç­”"
            f"{user_input}"
        )
        keywords_msg = llm.invoke([SystemMessage(content=keyword_prompt)])
        keywords = keywords_msg.content.strip()
        print("ç”Ÿæˆçš„é—œéµè©ï¼š", keywords)

        # Step 2: æ ¹æ“šç”¨æˆ¶è¼¸å…¥å’Œé—œéµè©æ§‹å»º RAG prompt
        rag_prompt = (f"{keywords}")

        # ä½¿ç”¨ç¶å®šå·¥å…·çš„ llm_with_ARCHrag é€²è¡Œ RAG æª¢ç´¢
        RAG_msg = ARCH_rag_tool.invoke(rag_prompt)
        print("RAGæª¢ç´¢çµæœï¼š", RAG_msg)

        # Step 3: å°‡ RAG è£œå……è³‡è¨Šèˆ‡åŸå§‹ç”¨æˆ¶è¼¸å…¥çµåˆï¼Œç”Ÿæˆæœ€çµ‚ç¸½çµå ±å‘Š
        summary_input = (
            "å»ºç¯‰é¡å‹æ˜¯Timber Curve Pavilionï¼Œä»¥ç”¨æˆ¶çš„è¨­è¨ˆç›®æ¨™ç‚ºä¸»ï¼Œæ ¹æ“šè£œå……è³‡è¨Šï¼Œ"
            "èªªæ˜è¨­è¨ˆæ–¹æ¡ˆå¯èƒ½è¦é”æˆç”šéº¼æ¨£çš„è¨­è¨ˆæ±ºç­–æ–¹å‘ä»¥æ»¿è¶³ç”¨æˆ¶çš„è¨­è¨ˆç›®æ¨™ã€‚"
            "è«‹ä½¿ç”¨ç”¨æˆ¶çš„è¼¸å…¥èªè¨€ä¾†å›ç­”"
            f"å»ºç¯‰è¨­è¨ˆç›®æ¨™:{user_input}/nè£œå……è³‡è¨Š:{RAG_msg}"
        )
        summary_msg = llm.invoke([SystemMessage(content=summary_input)])
        self.state["design_summary"] = f"ç”¨æˆ¶éœ€æ±‚:{user_input}/n{summary_msg.content}"
        print("âœ… è¨­è¨ˆç›®æ¨™ç¸½çµå®Œæˆï¼")
        print(summary_msg.content)
        return {          
            "è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½": user_input,
            "design_summary": self.state["design_summary"]
            }

# å ´åœ°åˆ†æä»»å‹™ï¼šè®€å– JSON ä¸¦åˆ†æåŸºåœ°è³‡è¨Šï¼ŒåŒæ™‚é€é LLM å‘¼å«é€²ä¸€æ­¥è§£è®€è³‡æ–™ OK
class SiteAnalysisTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        base_map_path = "./input/2D/base_map.png"
        project_data_path = "./input/project_data.json"

        if not os.path.exists(base_map_path) or not os.path.exists(project_data_path):
            print("âŒ ç¼ºå°‘å ´åœ°åˆ†ææ‰€éœ€çš„æ–‡ä»¶ï¼")
            return {
                "site_analysis": self.state.get("site_analysis"),
                "design_advice": self.state.get("design_advice")
            }

        with open(project_data_path, "r", encoding="utf-8") as f:
            project_data = json.load(f)

        geo_location = project_data.get("geoLocation", "æœªçŸ¥åœ°é»")
        region = project_data.get("region", "æœªçŸ¥å€åŸŸ")
        # north_direction = project_data.get("northDirection", "æœªçŸ¥æ–¹ä½")

        # img_recognitionçš„prompt
        prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„éƒ½å¸‚è¦–è¦ºè³‡è¨Šåˆ†æå·¥å…·ï¼Œä½ çš„ä»»å‹™æ˜¯åˆ†æä½¿ç”¨è€…æä¾›çš„åŸºåœ°åœ–ç‰‡ï¼Œè¾¨è­˜ä¸¦æ¨™è¨»éƒ½å¸‚ç’°å¢ƒç‰©ä»¶ï¼Œæè¿°é‡è¦ç‰¹å¾µï¼Œä¸¦ç”Ÿæˆè¦–è¦ºæ‘˜è¦ã€‚
        è¼¸å‡ºçµæ§‹åŒ–è³‡æ–™ï¼ŒåŒ…å«ç‰©ä»¶æ¨™è¨» (é¡å‹ã€ä½ç½®)ã€ç‰¹å¾µæè¿° (å»ºç¯‰ã€é“è·¯ã€ç¶ åœ°ã€æ°´é«”ã€ç’°å¢ƒè„ˆçµ¡ç‰¹å¾µ) ã€‚
        ä»¥åœ–ç‰‡çš„ä¸Šæ–¹ç‚ºåŒ—æ–¹ï¼Œåˆæ­¥æ¨ç†åŸºæ–¼æ–¹ä½ä¾†èªªï¼Œæ—¥ç…§ã€ç†±ç’°å¢ƒã€é¢¨ç’°å¢ƒã€å™ªéŸ³ã€æ™¯è§€ã€äº¤é€šå°æ–¼åŸºåœ°çš„å½±éŸ¿ã€‚
        è¨­è¨ˆä½ç½®:{region}ï¼Œç¶“ç·¯åº¦:{geo_location}
        """

        # èª¿ç”¨å·¥å…·é€²è¡Œå ´åœ°åˆ†æ
        analysis_img = img_recognition.invoke({
            "image_paths": base_map_path,
            "prompt": prompt,
        })

        # llm_with_ARCHragçš„prompt
        LLMprompt = f"""
        ä½œç‚ºå»ºç¯‰å¸«åŠç©ºé–“åˆ†æå°ˆå®¶ï¼Œä½ æ“…é•·æ•´åˆæä¾›çš„è³‡è¨Šé€²è¡ŒåŸºåœ°åˆ†æï¼ŒåŸºæ–¼åœ–ç‰‡çš„è¾¨è­˜çµæœã€‚
        æŸ¥è©¢ä¸¦æ•´åˆåœ°é»ã€éƒ½å¸‚è¨ˆç•«è¦ç¯„ã€å»ºç¯‰æ³•è¦ã€æ°£å€™è³‡æ–™ã€äººæ–‡æ­·å²ç‰¹è‰²ã€å…¶ä»–ç‰¹æ®Šåœ°è³ªæˆ–éƒ½å¸‚æƒ…å½¢ã€ç‰¹æ®Šæ°£å€™æƒ…å½¢ç­‰èƒŒæ™¯è³‡æ–™ã€‚ä¸¦æ•´ç†ç‚ºæ›´æ·±å…¥çš„åŸºåœ°åˆ†æå ±å‘Šã€‚
        è¨­è¨ˆä½ç½®:{region}ï¼Œç¶“ç·¯åº¦:{geo_location}ã€‚
        åœ–ç‰‡è¾¨è­˜çµæœ:{analysis_img}ã€‚
        """

        analysis_result = llm.invoke([SystemMessage(content=LLMprompt)])

        self.state["analysis_img"] = analysis_img
        self.state["site_analysis"] = analysis_result.content

        # ç¢ºä¿å ´åœ°åœ–åƒå­˜åœ¨
        if not os.path.exists(base_map_path):
            print(f"âŒ ç¼ºå°‘åŸºåœ°åœ–åƒ: {base_map_path}")
            return {
                "analysis_img": self.state.get("analysis_img"),
                "site_analysis": self.state.get("site_analysis"),
            }
        
        print("âœ… å ´åœ°åˆ†æå®Œæˆï¼")
        print(analysis_result.content)
        return {
            "analysis_img": self.state["analysis_img"],
            "site_analysis": self.state["site_analysis"],
        }

# è¨­è¨ˆæ–¹æ¡ˆä»»å‹™ OK
class RAGdesignThinking:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state        

        # è®€å–è¨­è¨ˆåƒæ•¸
        design_goal = self.state.get("design_summary", "ç„¡ç›®æ¨™")
        analysis_result = self.state.get("site_analysis", "ç„¡åŸºåœ°æ¢ä»¶")
        current_round = self.state.get("current_round", 0)
        improvement = self.state.get("GATE_REASON1", "")

        # llm1 prompt
        prompt_keywords_preliminary = f"""
        ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œçš„è³‡æ·±å»ºç¯‰è¨­è¨ˆé¡§å•ï¼Œ
        åŸºæ–¼ä»¥ä¸‹è³‡æ–™ï¼š
        è¨­è¨ˆç›®æ¨™ï¼š{design_goal}
        åŸºåœ°åˆ†æå ±å‘Šï¼š{analysis_result}
        
        è«‹ç‚º Timber pavilion è¨­è¨ˆæä¾›å…·é«”ä¸”ç´°ç·»çš„åƒè€ƒè³‡æ–™é—œéµè©ã€‚
        **è«‹åˆ—å‡ºRAGä¸­è‹±æ–‡é—œéµå­—ï¼Œæ ¼å¼ç‚º:ä¸­æ–‡(è‹±æ–‡)**ã€‚æè¿°ç›¸é—œæ¡ˆä¾‹ã€æ§‹é€ ç´°ç¯€åŠè£½é€ å·¥æ³•ã€‚
        """
        response_kp = llm.invoke([SystemMessage(content=prompt_keywords_preliminary)])
        response_text = response_kp.content

        print("ç”Ÿæˆçš„é—œéµè©ï¼š", response_text)

        # Step 2: ä½¿ç”¨é—œéµå­—é€²è¡Œ RAG æª¢ç´¢ä»¥ç²å–åƒè€ƒè³‡æ–™
        rag_prompt = f"è«‹æ ¹æ“šé—œéµå­—æŸ¥è©¢ç›¸é—œæ¡ˆä¾‹ã€æ§‹é€ ç´°ç¯€åŠè£½é€ å·¥æ³•ã€æ¸›ç¢³åŠå¾ªç’°æ°¸çºŒæ€§ã€æŠ€è¡“ç´°ç¯€åŠç ”ç©¶ç†è«–ï¼š{response_text}ã€‚"
        RAG_msg = ARCH_rag_tool.invoke({"query": rag_prompt})
        print("RAG æª¢ç´¢çµæœï¼š", RAG_msg)

        # Step 3: åˆ©ç”¨åˆæ­¥æ–¹æ¡ˆèˆ‡ RAG åƒè€ƒè³‡æ–™ç”Ÿæˆå®Œæ•´æ–¹æ¡ˆ
        prompt_complete = f"""
        ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œçš„è³‡æ·±å»ºç¯‰è¨­è¨ˆé¡§å•ï¼Œæ ¹æ“šä»¥ä¸‹æ–¹é¢ç”Ÿæˆå®Œæ•´çš„è¨­è¨ˆæ–¹æ¡ˆï¼š
        ä¸»è¦è¨­è¨ˆæ±ºç­–é‡å°**å¹¾ä½•å½¢ç‹€(æ¯”å¦‚æ–¹ã€åœ“ã€ä¸‰è§’ã€éŒå‹ã€å¡”å‹ç­‰)ã€å¤–æ®¼å½¢å¼(æ¯”å¦‚å¹³é¢ã€å–®æ›²é¢ã€é›™æ›²é¢ã€è‡ªç”±æ›²é¢ç­‰)ã€æœ¨æ§‹é€ ç´°ç¯€**ã€‚
        åŠæ¬¡è¦è¨­è¨ˆæ±ºç­–é‡å°æ—¥ç…§ã€ç†±ç’°å¢ƒã€é¢¨ç’°å¢ƒã€å™ªéŸ³ã€æ™¯è§€ã€åŸºåœ°å‘¨é­ç´‹ç†ç­‰ã€‚å°ˆæ³¨æ–¼å¤–æ®¼è¨­è¨ˆï¼Œéœ€è¦å…·æœ‰åƒæ•¸å¼è¨­è¨ˆçš„ç¾æ„Ÿã€é«˜åº¦å‰µæ„æ€§åŠå‰è¡›æ€§ã€‚
        è«‹ç¶œåˆä»¥ä¸‹å…§å®¹ï¼Œä»¥è¨­è¨ˆç›®æ¨™åŠæ”¹é€²å»ºè­°ç‚ºé‡é»ï¼Œæå‡ºä¸€å€‹å®Œæ•´ã€å…·å‚™ç´°ç¯€ä¸”å…·å‰µæ–°æ€§ã€å¯è¡Œæ€§çš„è¨­è¨ˆæ–¹æ¡ˆã€‚
        **è¨­è¨ˆç›®æ¨™**ï¼š{design_goal}
        **æ”¹é€²å»ºè­°**: {improvement}
        åŸºåœ°åˆ†æå ±å‘Šï¼š{analysis_result}
        åƒè€ƒè³‡æ–™ï¼š{RAG_msg}
        """
        complete_response = llm.invoke([SystemMessage(content=prompt_complete)])
        complete_scheme = complete_response.content

        new_scheme_entry = {"round":int(current_round),"proposal":str(complete_scheme)}

        # ä½¿ç”¨ custom_add_messages ç´¯åŠ å­˜å…¥ state (design_advice ä¹Ÿä½¿ç”¨ç´¯åŠ åŠŸèƒ½)
        existing_advice = self.state.get("design_advice", [])
        updated_advice = custom_add_messages(existing_advice, [new_scheme_entry])
        self.state["design_advice"] = updated_advice

        print("âœ… è¨­è¨ˆå»ºè­°å·²å®Œæˆï¼")
        print(f"æœ€çµ‚è¨­è¨ˆå»ºè­°ï¼š{self.state['design_advice']}")

        return {"design_advice": self.state["design_advice"]}

# GATE æª¢æŸ¥æ–¹æ¡ˆï¼ˆè«‹å›ç­”ï¼šæœ‰/æ²’æœ‰ï¼‰ OK
class GateCheck1:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        design_advice_raw = self.state.get("design_advice", [])
        site_analysis = self.state.get("site_analysis", "ç„¡åŸºåœ°æ¢ä»¶")
        design_summary = self.state.get("design_summary", "ç„¡ç›®æ¨™")

        def ensure_dict(item):
            if isinstance(item, dict):
                return item
            try:
                return json.loads(item)
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è§£æé …ç›®: {item}ï¼ŒéŒ¯èª¤ï¼š{e}")
                return None

        # - formatted_current: æ²’æœ‰ state éµçš„å°è±¡
        # - formatted_previous: å·²åŒ…å« state éµçš„å°è±¡
        design_advice_list = []
        for item in design_advice_raw:
            d = ensure_dict(item)
            if d is not None:
                design_advice_list.append(d)

        # ä½¿ç”¨æ–°æ¢ä»¶ï¼šæ²’æœ‰ "state" éµçš„å°è±¡ä½œç‚º current proposals
        current_proposals = [
            advice for advice in design_advice_list if "state" not in advice
        ]
        historical_proposals = [
            advice for advice in design_advice_list if "state" in advice
        ]

        if not current_proposals:
            print("âš ï¸ ç•¶å‰ç„¡ç¬¦åˆæ¢ä»¶çš„è¨­è¨ˆå»ºè­°æ–¹æ¡ˆï¼ˆæœªæ‰¾åˆ°ä¸å« state éµçš„å°è±¡ï¼‰ã€‚")
            self.state["GATE1"] = "æ²’æœ‰"
            self.state["GATE_REASON1"] = "ç•¶å‰ç„¡ç¬¦åˆæ¢ä»¶çš„è¨­è¨ˆå»ºè­°æ–¹æ¡ˆï¼ˆæœªæ‰¾åˆ°ä¸å« state éµçš„å°è±¡ï¼‰"
            return {"GATE1": self.state["GATE1"], "GATE_REASON1": self.state["GATE_REASON1"]}

        # æ ¼å¼åŒ–è¼¸å‡ºä»¥ä¾› prompt ä½¿ç”¨
        formatted_current = json.dumps(current_proposals, ensure_ascii=False, indent=2)
        formatted_previous = json.dumps(historical_proposals, ensure_ascii=False, indent=2)

        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å»ºç¯‰æ–¹æ¡ˆè©•å¯©å“¡ã€‚
        è«‹æ ¹æ“šä»¥ä¸‹è¨­è¨ˆå»ºè­°æä¾›åˆ¤æ–·åŠè©•æ¯”ï¼Œé ˆå°æ–¼è¨­è¨ˆéœ€æ±‚å…·æœ‰å›æ‡‰æ€§ï¼Œä¸”èˆ‡ä¹‹å‰è¼ªæ¬¡çš„æ–¹æ¡ˆä¸éæ–¼æ¥è¿‘ã€‚

        1.å¾ªç’°ç¶“æ¿Ÿæ½›åŠ› (Circular Economy Potential): æ–¹æ¡ˆæ˜¯å¦å±•ç¾æœå‘ææ–™å¾ªç’°åˆ©ç”¨ã€æ°¸çºŒæœ¨æä¾†æºçš„æ½›åŠ›ï¼Ÿ
        åˆ¤æ–·é»: æ–¹æ¡ˆæ˜¯å¦å…·æœ‰ç™¼å±•ææ–™å†åˆ©ç”¨ã€å›æ”¶ã€æ¨¡çµ„åŒ–æˆ–çµ„è£æ•ˆç‡ç­‰è™•ç†è¨ˆç•«çš„æ©Ÿæœƒ (å³ä½¿æ²’æœ‰è©³ç´°è¨ˆç•«)ï¼Ÿ
        2.ææ–™æ•ˆç‡æ½›åŠ› (Material Efficiency Potential): æ–¹æ¡ˆæ˜¯å¦å±•ç¾æ¸›å°‘ææ–™æµªè²»ã€æå‡ææ–™åˆ©ç”¨ç‡çš„æ½›åŠ›ï¼Ÿ
        åˆ¤æ–·é»: æ–¹æ¡ˆæ˜¯å¦å…·æœ‰ç™¼å±•è¦åŠƒå„ªåŒ–è¨­è¨ˆã€æ•¸ä½è£½é€ ã€é›†æˆæœ¨æç­‰æ–¹æ³•çš„æ©Ÿæœƒ (å³ä½¿æ²’æœ‰å…·é«”æ•¸æ“š)ï¼Ÿ
        3.è£½é€ æ•ˆç‡æ½›åŠ› (Manufacturing Efficiency Potential): æ–¹æ¡ˆæ˜¯å¦å±•ç¾æå‡è£½é€ èˆ‡æ–½å·¥æ•ˆç‡çš„æ½›åŠ›ï¼Ÿ
        åˆ¤æ–·é»: æ–¹æ¡ˆæ˜¯å¦å…·æœ‰ç™¼å±•è¦åŠƒé è£½åŒ–ã€æ¨¡çµ„åŒ–ã€è‡ªå‹•åŒ–ç”Ÿç”¢ã€ç°¡åŒ–æ–½å·¥ç­‰ç­–ç•¥çš„æ©Ÿæœƒ (å³ä½¿æ²’æœ‰è©³ç´°æµç¨‹)ï¼Ÿ
        4.æ°¸çºŒç’°ä¿æ½›åŠ› (Environmental Sustainability Potential): æ–¹æ¡ˆæ˜¯å¦å±•ç¾é™ä½ç’°å¢ƒè¶³è·¡ã€ç¬¦åˆæ°¸çºŒç’°ä¿åŸå‰‡çš„æ½›åŠ›ï¼Ÿ
        åˆ¤æ–·é»: æ–¹æ¡ˆæ˜¯å¦å…·æœ‰ç™¼å±•è¦åŠƒæœ¨æçš„æ¸›å°‘æµªè²»ã€ç’°å¢ƒå‹å–„ã€æ¸›å°‘æ±¡æŸ“çš„è£½é€ ç­–ç•¥çš„æ©Ÿæœƒ (å³ä½¿æ²’æœ‰é‡åŒ–æ•¸æ“š)ï¼Ÿ
        5.æ¸›ç¢³æ½›åŠ› (Carbon Reduction Potential): æ–¹æ¡ˆæ˜¯å¦å±•ç¾ç¢³å°å­˜ã€æ¸›å°‘ç¢³æ’æ”¾çš„æ½›åŠ›ï¼Ÿ
        åˆ¤æ–·é»: æ–¹æ¡ˆæ˜¯å¦å…·æœ‰ç™¼å±•è¦åŠƒæœ¨æ§‹é€ çš„åœ¨åœ°æ€§ã€æ¸›ç¢³æ•ˆç›Šã€ç¢³å°å­˜ç­‰ç­–ç•¥çš„æ©Ÿæœƒ (å³ä½¿æ²’æœ‰ç¢³æ’è¨ˆç®—)ï¼Ÿ
        
        åªæœ‰åœ¨ç¬¦åˆè¨­è¨ˆéœ€æ±‚çš„å‰æä¸‹ï¼Œå…¶ä»–æ–¹é¢éƒ½å…·å‚™æ½›åŠ›ï¼Œæ‰æ˜¯"æœ‰"ã€‚åä¹‹å°±æ˜¯"æ²’æœ‰ã€‚
        **è«‹å›è¦†å…©è¡Œï¼šç¬¬ä¸€è¡Œåƒ…åŒ…å«åˆ¤æ–·å¾Œçš„"æœ‰"æˆ–"æ²’æœ‰"ï¼›ç¬¬äºŒè¡Œè«‹èªªæ˜æ”¹é€²çš„å»ºè­°ã€‚**
        **è¨­è¨ˆéœ€æ±‚**ï¼š{design_summary}
        ç•¶å‰è¼ªæ¬¡æ–¹æ¡ˆï¼š{formatted_current}
        ä¹‹å‰è¼ªæ¬¡æ–¹æ¡ˆï¼š{formatted_previous}
        """.strip()
#        åŸºåœ°åˆ†æï¼š{site_analysis}

        # èª¿ç”¨ LLM ä¸¦å–å¾—å›è¦†
        llm_response = llm.invoke([SystemMessage(content=prompt)])
        response_lines = [line.strip() for line in llm_response.content.splitlines() if line.strip()]
        if not response_lines:
            print("âš ï¸ LLM å›è¦†ç‚ºç©ºï¼Œè«‹æª¢æŸ¥æç¤ºæ ¼å¼ã€‚")
            evaluation_result = "æ²’æœ‰"
            reason = "LLM å›è¦†ç‚ºç©ºï¼Œè«‹æª¢æŸ¥æç¤ºæ ¼å¼"
        else:
            evaluation_result = response_lines[0]
            reason = response_lines[1] if len(response_lines) > 1 else "ç©º"

        # åˆ¤æ–·è©•ä¼°çµæœï¼Œæ±ºå®š state éµå€¼
        state_value = True if evaluation_result == "æœ‰" else False

        # ç‚ºæ¯å€‹ç•¶å‰æ–¹æ¡ˆå­—å…¸æ–°å¢ state éµ
        for advice in current_proposals:
            advice["state"] = state_value

        # è¦†è“‹ design_advice[-1]ï¼šè‹¥å­˜åœ¨å‰‡ç§»é™¤æœ€å¾Œä¸€å€‹ï¼Œå†æ–°å¢ current_proposals
        existing_advice = self.state.get("design_advice", [])
        if existing_advice:
            # ç§»é™¤æœ€å¾Œä¸€å€‹è¨­è¨ˆå»ºè­°
            existing_advice.pop()
            updated_advice = existing_advice + current_proposals
        else:
            updated_advice = current_proposals

        self.state["design_advice"] = updated_advice

        # æœ€å¾Œ self.state["GATE1"] åƒ…è¿”å›è©•åˆ¤çµæœï¼ˆ"æœ‰" æˆ– "æ²¡æœ‰"ï¼‰
        self.state["GATE1"] = evaluation_result
        self.state["GATE_REASON1"] = reason

        print(f"ã€GateCheckã€‘å·²æ”¶åˆ°è©•å¯©çµæœï¼š{evaluation_result}ï¼ŒåŸå› ï¼š{reason}")
        return {"GATE1": self.state["GATE1"], "GATE_REASON1": self.state["GATE_REASON1"], "design_advice": self.state["design_advice"]}
        

# å¤–æ®¼ Prompt ç”Ÿæˆï¼šå‘¼å« LLMï¼ˆä½¿ç”¨ prompt ç”Ÿæˆå·¥å…·ï¼‰æ ¹æ“šåŸºåœ°è³‡è¨Šèˆ‡èåˆåœ–ç”Ÿæˆè¨­è¨ˆ prompt OK
class OuterShellPromptTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        # å–å¾—ç´¯ç©çš„è¨­è¨ˆå»ºè­°
        current_round = self.state.get("current_round", 0)
        design_advice_list = self.state.get("design_advice", [])
        improvement = self.state.get("GATE_REASON1", "")
        
        # éæ¿¾å‡ºç•¶å‰è¼ªæ¬¡ä¸” state ç‚º True çš„è¨­è¨ˆæ–¹æ¡ˆï¼ˆå¿…é ˆæ˜¯å­—å…¸æ ¼å¼ï¼‰
        valid_advices = [
            advice for advice in design_advice_list
            if isinstance(advice, dict) and advice.get("round") == current_round and advice.get("state") == True
        ]
        
        # å¾æœ‰æ•ˆçš„è¨­è¨ˆæ–¹æ¡ˆä¸­å–å‡º "proposal" ä½œç‚º advice_text
        if valid_advices:
            selected_advice = valid_advices[0]
            advice_text = selected_advice.get("proposal", "ç„¡ç›®æ¨™")
        else:
            advice_text = "ç„¡ç›®æ¨™"
        
        gpt_prompt = (
            f"ä½œç‚ºå»ºç¯‰å¸«èˆ‡Prompt engineeringï¼Œè«‹åƒè€ƒä»¥ä¸‹è¨­è¨ˆåƒè€ƒå»ºè­°ä¾†æ¨æ¸¬æœªä¾†æ­¤å°å‹pavilionçš„æ¨£è²Œã€‚"
            f"ä½¿ç”¨è‹±æ–‡ promptï¼Œåªéœ€ positive promptï¼Œè¦ä»”ç´°ã€å…·é«”ã€ä½¿ç”¨å°ˆæ¥­çš„å»ºç¯‰æœ¨æ§‹é€ è¨­è¨ˆèªæ³•ï¼Œ**æœ€å¤§tokenä¸å¯è¶…é77**ã€‚"
            f"Prompt ä¸»è¦æ ¹æ“šè¨­è¨ˆææ¡ˆæè¿°æ­¤å»ºç¯‰è¨­è¨ˆå¤–è§€ã€é€ å‹æ›²é¢å‹å¼ã€æœ¨æ§‹é€ ç´°éƒ¨è¨­è¨ˆåŠç¶²æ ¼åˆ†å‰²å½¢å¼ã€æ•´é«”é¢¨æ ¼èˆ‡æ°›åœã€‚"      
            f"**åœ¨å…·æœ‰ç´°ç¯€ä¸”æ§‹é€ åˆç†çš„æƒ…æ³ä¸‹ï¼Œéœ€è¦é¿å…æœ¨æ§‹é€ æ›²é¢ã€ç¶²æ ¼åˆ†å‰²éåº¦è¤‡é›œ**ã€‚"
            f"è¦–è§’å¿…é ˆè¦çœ‹åˆ°å»ºç¯‰æ•´é«”ï¼Œé«˜è³ªæ„Ÿï¼Œé€è¦–åœ–ã€‚**ä¸ç”Ÿæˆå…§éƒ¨éš”é–“ã€å®¶å…·ã€ç»ç’ƒã€äºº**ã€‚"
            f"è¨­è¨ˆææ¡ˆ: {advice_text}"
            f"æ”¹é€²å»ºè­°:{improvement}"
        )

        gpt_output = llm.invoke([SystemMessage(content=gpt_prompt)])
        final_prompt = gpt_output.content if hasattr(gpt_output, "content") else "âŒ GPT ç”Ÿæˆå¤±æ•—"

        lora_prompt = (
            f"è«‹ç”Ÿæˆä¸€å€‹é©åˆçš„ LoRA æ¬Šé‡æ•¸å€¼ï¼Œå…¶æ•¸å€¼å¿…é ˆåœ¨ 0.3 åˆ° 0.7 ä¹‹é–“ã€‚"
            f"æ¬Šé‡è¶Šé‡ï¼ˆæ¥è¿‘ 0.7ï¼‰è¡¨ç¤ºç”Ÿæˆçµæœæœƒæ›´è¶¨æ–¼å½¢å¼å›ºå®šæ²’æœ‰å‰µæ„æ€§çš„æ›²é¢ï¼Œä½†æœ¨ç¶²æ ¼æ§‹é€ æ¸…æ™°é©åˆç”Ÿæˆå…·æœ‰ç°ç©ºæœ¨ç¶²æ ¼çš„æ§‹é€ ï¼›"
            f"æ¬Šé‡è¶Šè¼•ï¼ˆæ¥è¿‘ 0.3ï¼‰å‰‡ç”Ÿæˆçµæœæœƒæ›´å…·è¨­è¨ˆç™¼æ•£æ€§ä½†å¤±å»ç¶²æ ¼æ§‹é€ æˆ–ç¶²æ ¼ä¸æ¸…æ™°ï¼Œæ•´é«”é©åˆç”Ÿæˆè¼ƒç‚ºç°¡ç´„çš„é€ å‹ã€‚"
            f"è«‹æ ¹æ“šè¨­è¨ˆææ¡ˆå‹•æ…‹ç”Ÿæˆé©åˆçš„ LoRA æ¬Šé‡ã€‚**åƒ…å›ç­”æ¬Šé‡çš„æ•¸å­—**"
            f"è¨­è¨ˆææ¡ˆ: {final_prompt}"
        )

        gpt_output2 = llm.invoke([SystemMessage(content=lora_prompt)])
        lora_prompt = gpt_output2.content

        new_prompt_entry = {"round":int(current_round),"prompt": str(final_prompt),"lora":str(lora_prompt)}
        existing_prompts = self.state.get("outer_prompt", [])
        if not isinstance(existing_prompts, list):
            existing_prompts = []
        self.state["outer_prompt"] = custom_add_messages(existing_prompts, [new_prompt_entry])

        print("âœ… ç”Ÿæˆå¤–æ®¼ Prompt å®Œæˆï¼")
        print(f"ğŸ“Œ å¤–æ®¼ Prompt: {final_prompt}")
        return {"outer_prompt": self.state["outer_prompt"]}

# æ–¹æ¡ˆæƒ…å¢ƒç”Ÿæˆï¼šå‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡ç”Ÿæˆå·¥å…·ï¼‰æ ¹æ“šå¤–æ®¼ prompt èˆ‡èåˆåœ–ç”Ÿæˆæœªä¾†æƒ…å¢ƒåœ– OK
class CaseScenarioGenerationTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        current_round = self.state.get("current_round", 0)
        outer_prompt = self.state.get("outer_prompt", [])

        # ç¯©é¸å‡ºä¸å« "state" éµçš„å­—å…¸ï¼Œä¸¦æå–å®ƒå€‘çš„ "prompt" å€¼
        prompt_values = [item["prompt"] for item in outer_prompt
                        if isinstance(item, dict) and "state" not in item]

        lora_values = [item["lora"] for item in outer_prompt
                        if isinstance(item, dict) and "state" not in item]

        prompt_str = " ".join(prompt_values)
        lora_str = " ".join(lora_values)

        # å¾ªç’°å››æ¬¡ç”Ÿæˆåœ–ç‰‡ï¼Œä¸¦å°‡è¿”å›çš„æª”åä»¥å­—å…¸æ ¼å¼å­˜æ”¾ï¼Œæ ¼å¼ä¾‹å¦‚ï¼š{1: "shell_result_{current_round}_1.png"}
        combined_images = []
        render_cache_dir = os.path.join(os.getcwd(), "output", "render_cache")
        for i in range(1, 5):  # å¾ªç’° 1~4 æ¬¡
            # å‘¼å«åœ–ç‰‡ç”Ÿæˆå·¥å…·ï¼Œå‚³å…¥ç•¶å‰è¼ªæ¬¡ã€outer_prompt ä»¥åŠç•¶å‰ç”Ÿæˆæ¬¡æ•¸ i
            case_image_path = case_render_image.invoke({
                "current_round": current_round,
                "outer_prompt": prompt_str,
                "i": i,
                "strength":lora_str
            })
            
            # å¾ render_cache ç›®éŒ„ä¸­å–å¾—åœ–ç‰‡æª”æ¡ˆ
            image_path = os.path.join(render_cache_dir, case_image_path)
            if os.path.exists(image_path):
                with open(image_path, "rb") as image_file:
                    encoded_image = base64.b64encode(image_file.read()).decode('utf-8')
                image_url = f"data:image/png;base64,{encoded_image}"
            else:
                image_url = "æœªç”Ÿæˆ"

            # å°‡æ¯å€‹ç”Ÿæˆçµæœä»¥å­—å…¸å½¢å¼å­˜æ”¾ï¼Œkey ç‚ºç”Ÿæˆæ¬¡æ•¸ï¼Œå€¼ç‚ºåŒ…å«æª”åèˆ‡ URL çš„å­—å…¸
            combined_images.append({i: case_image_path, "output": image_url})

        # ä½¿ç”¨ custom_add_messages ç´¯åŠ å­˜å…¥ state["case_image"]
        existing_images = self.state.get("case_image", [])
        updated_images = custom_add_messages(existing_images, combined_images)
        self.state["case_image"] = updated_images

        print(f"âœ… æœªä¾†æƒ…å¢ƒåœ–ç”Ÿæˆå®Œæˆï¼Œåœ–ç‰‡è³‡è¨Š: {combined_images}")
        return {"case_image": self.state["case_image"]}    

# GATE æª¢æŸ¥æ–¹æ¡ˆï¼ˆè«‹å›ç­”ï¼šæœ‰/æ²’æœ‰ï¼‰ OK
class GateCheck2:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        # è·å–å½“å‰è½®æ¬¡ä¸ç”Ÿæˆçš„å›¾ç‰‡åˆ—è¡¨
        current_round = self.state.get("current_round", 0)
        case_images = self.state.get("case_image", [])
        design_advice_list = self.state.get("design_advice", [])
        
        # éæ¿¾å‡ºç•¶å‰è¼ªæ¬¡ä¸” state ç‚º True çš„è¨­è¨ˆæ–¹æ¡ˆï¼ˆå¿…é ˆæ˜¯å­—å…¸æ ¼å¼ï¼‰
        valid_advices = [
            advice for advice in design_advice_list
            if isinstance(advice, dict) and advice.get("round") == current_round and advice.get("state") == True
        ]
        
        # å¾æœ‰æ•ˆçš„è¨­è¨ˆæ–¹æ¡ˆä¸­å–å‡º "proposal" ä½œç‚º advice_text
        if valid_advices:
            selected_advice = valid_advices[0]
            advice_text = selected_advice.get("proposal", "ç„¡ç›®æ¨™")
        else:
            advice_text = "ç„¡ç›®æ¨™"

        # æå–æ¯ä¸ªå­—å…¸ä¸­æ•´æ•°é”®å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶å
        image_dict = {}
        for item in case_images:
            if isinstance(item, dict):
                for key, value in item.items():
                    if isinstance(key, int):
                        image_dict[key] = value

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¦åˆæ¡ä»¶çš„å›¾ç‰‡
        if not image_dict:
            print("âš ï¸ å½“å‰è½®æ¬¡æ— ç¬¦åˆæ¡ä»¶çš„ç”Ÿæˆå›¾")
            self.state["GATE2"] = "æ²¡æœ‰"
            self.state["GATE_REASON2"] = "å½“å‰è½®æ¬¡æ— ç¬¦åˆæ¡ä»¶çš„ç”Ÿæˆå›¾"
            return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"]}

        # ä¸ºæ¯ä¸ªå›¾ç‰‡æ–‡ä»¶åæ·»åŠ å®Œæ•´è·¯å¾„
        OUTPUT_SHELL_CACHE_DIR = "./output/render_cache"
        full_paths = {key: os.path.join(OUTPUT_SHELL_CACHE_DIR, filename) for key, filename in image_dict.items()}

        # å°†å›¾ç‰‡æ–‡ä»¶åæ•´ç†æˆå¤šè¡Œå­—ç¬¦ä¸²ä¾› prompt ä½¿ç”¨ï¼ˆåªæ˜¾ç¤ºæ–‡ä»¶åï¼Œä¸å«è·¯å¾„ï¼‰
        image_list_str = "\n".join(image_dict.values())

        # å‡†å¤‡ promptï¼Œè¦æ±‚ LLM æ ¹æ®è®¾è®¡è¦æ±‚è¯„ä¼°å¹¶é€‰å‡ºæœ€ä½³ç”Ÿæˆå›¾
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å»ºç¯‰åœ–åƒè©•å¯©å“¡ï¼Œå°ˆç²¾æ–¼å¾åœ–ç‰‡è©•ä¼°å»ºç¯‰æ§‹é€ èˆ‡è£½é€ çš„å¯èƒ½æ€§ã€‚
        **è«‹æ ¹æ“šä»¥ä¸‹æ¢ä»¶é€²è¡Œåš´æ ¼è©•ä¼°ã€‚

        **å„ªå…ˆé …ç›®**
        è¨­è¨ˆç¬¦åˆæ€§èˆ‡åˆç†æ€§ï¼š **éœ€åš´æ ¼ç¢ºä¿åœ–ç‰‡ç¬¦åˆè¨­è¨ˆææ¡ˆæ‰€è¿°å¤–è§€**ã€‚çµæ§‹èˆ‡é€ å‹éœ€åˆç†ä¸”ç¬¦åˆé æœŸï¼Œå±•ç¾è‰¯å¥½çš„å»ºç¯‰è¨­è¨ˆé‚è¼¯ã€‚
        **åœ–ç‰‡å„ªåŠ£è©•æ¯”é …ç›®**
        åœ–ç‰‡å“è³ªèˆ‡ç´°ç¯€ï¼š åœ–ç‰‡å¿…é ˆæ¸…æ™°ï¼Œç´°ç¯€è¡¨ç¾è‰¯å¥½æ²’æœ‰æ‰­æ›²æˆ–é€è¦–éŒ¯èª¤ã€‚
        æ›²é¢ç°¡æ½”åº¦ï¼š æ›²é¢ç·šæ¢æ˜¯å¦ç°¡æ½”æµæš¢ï¼Œé¿å…éæ–¼è¤‡é›œç ´ç¢ã€‚ä»¥æœ‰æ•ˆç‡çš„ä½¿ç”¨ææ–™å’Œç°¡åŒ–è£½é€ æµç¨‹ã€‚
        æ¥åˆæ§‹é€ ç°¡æ½”æ€§ï¼š æœ¨æ§‹ä»¶æ¥åˆæ–¹å¼æ˜¯å¦ç°¡æ½”æ˜ç­ï¼Œé¿å…éæ–¼è¤‡é›œç¹ç‘£ã€‚ä»¥æé«˜å¾ªç’°ä½¿ç”¨æ½›åŠ›ã€é™ä½çµ„è£é›£åº¦ï¼Œä¸¦æ¸›å°‘æ½›åœ¨çš„çµæ§‹é¢¨éšªã€‚
        è¡¨é¢è™•ç†å®Œæ•´æ€§ï¼š æœ¨æè¡¨é¢æ˜¯å¦æœ‰å¡—å±¤ã€å°é‚Šæˆ–å…¶ä»–ä¿è­·è™•ç†ï¼Œè™•ç†æ˜¯å¦å‡å‹»å®Œæ•´ï¼Œæ˜¯å¦èƒ½çœ‹å‡ºé‡å°åŸºåœ°ç’°å¢ƒæ°£å€™çš„é˜²è­·è€ƒé‡ã€‚
        çµæ§‹ç³»çµ±æ•ˆç‡æ€§ï¼š çµæ§‹ç³»çµ±è¨­è¨ˆæ˜¯å¦æœ‰æ•ˆåˆ©ç”¨ææ–™ç‰¹æ€§ï¼Œä»¥è¼ƒå°‘ææ–™é”æˆæ‰€éœ€æ•ˆèƒ½ã€‚ææ–™çš„ä½¿ç”¨é‚è¼¯æ˜¯å¦èƒ½é¿å…è£½é€ ä¸Šçš„æµªè²»ã€‚
        é€ å‹ç¾è§€å”èª¿æ€§ï¼š æ•´é«”é€ å‹æ˜¯å¦ç¾è§€ï¼Œèˆ‡å‘¨åœç’°å¢ƒæ˜¯å¦å”èª¿ã€‚

        **ç”Ÿæˆåœ–åé †åºï¼š {image_list_str}
        **è«‹å›å¾©å…©è¡Œï¼Œå„ªå…ˆæª¢æŸ¥å¦‚æœæ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¨­è¨ˆææ¡ˆæ‰€è¿°å¤–è§€ï¼Œè«‹åƒ…å›å¾©ã€Œæ²’æœ‰ã€ï¼š
        ç¬¬ä¸€è¡Œï¼šåƒ…å›å¾©æœ€ä½³åœ–ç‰‡æ–‡ä»¶åä¸­çš„ id æ•¸å­—éƒ¨åˆ† (æ•´æ•¸)ã€‚ï¼ˆä¾‹å¦‚ï¼š"shell_result_{current_round}_id.png"ï¼Œå‰‡å›è¦† idï¼‰ã€‚
        ç¬¬äºŒè¡Œï¼šå›å¾©ã€Œæœ‰ã€æ™‚ç¶œåˆèªªæ˜æ‰€æœ‰æ–¹æ¡ˆçš„å„ªåŠ£ï¼Œä¸¦è©³ç´°è§£é‡‹é¸æ“‡æ­¤æœ€ä½³æ–¹æ¡ˆçš„åŸå› ã€‚å¦‚æœå›å¾©ã€Œæ²’æœ‰ã€å‰‡èªªæ˜æ”¹é€²å»ºè­°ã€‚

        **è¨­è¨ˆææ¡ˆï¼š{advice_text} 
        """.strip()

        # è°ƒç”¨ img_recognition.invoke å¤„ç†æ‰€æœ‰å›¾ç‰‡
        analysis_result = img_recognition.invoke({
            "image_paths": list(full_paths.values()),
            "prompt": prompt,
        })

        result = analysis_result.strip() if isinstance(analysis_result, str) else ""
        # å°†å›å¤æŒ‰è¡Œåˆ†å‰²ï¼Œè§£æç¬¬ä¸€è¡Œä½œä¸ºæœ€ä½³å›¾ç‰‡ idï¼Œç¬¬äºŒè¡Œä½œä¸ºé€‰æ‹©åŸå› 
        lines = [line.strip() for line in result.splitlines() if line.strip()]
        if lines:
            first_line = lines[0]
            # å°è¯•è§£æç¬¬ä¸€è¡Œæ•°å­—
            if first_line.isdigit():
                best_id = int(first_line)
                self.state["GATE2"] = best_id
            elif "æ²¡æœ‰" in first_line or "no" in first_line.lower():
                self.state["GATE2"] = "æ²¡æœ‰"
            else:
                digit_matches = re.findall(r'\d+', first_line)
                if digit_matches:
                    best_id = int(digit_matches[0])
                    self.state["GATE2"] = best_id
                else:
                    print("âš ï¸ æ— æ³•è§£æ LLM å›å¤ä¸­çš„æœ€ä½³æ–¹æ¡ˆ idã€‚")
                    self.state["GATE2"] = "æ²¡æœ‰"
            
            # è§£æç¬¬äºŒè¡Œä½œä¸ºé€‰æ‹©åŸå› ï¼Œè‹¥æœ‰æä¾›åˆ™å­˜å…¥ GATE_REASON2
            if len(lines) >= 2:
                self.state["GATE_REASON2"] = lines[1]
            else:
                self.state["GATE_REASON2"] = ""
        else:
            print("âš ï¸ LLM å›å¤ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ prompt æ ¼å¼ã€‚")
            self.state["GATE2"] = "æ²¡æœ‰"
            self.state["GATE_REASON2"] = ""

        # æ ¹æ“š GATE2 åˆ¤æ–·è©•ä¼°çµæœï¼šè‹¥ç‚º "æ²¡æœ‰"ï¼Œå‰‡ state_value ç‚º Falseï¼Œå¦å‰‡ç‚º True
        state_value = False if self.state["GATE2"] == "æ²¡æœ‰" else True

        # åƒ…ç‚º outer_prompt åˆ—è¡¨ä¸­çš„æœ€å¾Œä¸€å€‹å°è±¡æ–°å¢ state éµ
        outer_prompt = self.state.get("outer_prompt", [])
        if outer_prompt and isinstance(outer_prompt[-1], dict):
            outer_prompt[-1]["state"] = state_value
        # å°‡æ›´æ–°å¾Œçš„ outer_prompt å›å¯«å› state
        self.state["outer_prompt"] = outer_prompt

        # å°‡æ›´æ–°å¾Œçš„ outer_prompt å­˜å› state
        self.state["outer_prompt"] = outer_prompt                

        print(f"ã€GateCheckCaseImageã€‘ä»¥æ”¶åˆ°æœ€ä½³è©•ä¼°çµæœï¼š{self.state.get('GATE2')}ï¼ŒåŸå› ï¼š{self.state.get('GATE_REASON2')} ğŸ˜Š")
        return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"], "outer_prompt": self.state["outer_prompt"]}

# æœªä¾†æƒ…å¢ƒç”Ÿæˆï¼šå‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡ç”Ÿæˆå·¥å…·ï¼‰æ ¹æ“šå¤–æ®¼ prompt èˆ‡èåˆåœ–ç”Ÿæˆæœªä¾†æƒ…å¢ƒåœ– OK
class FutureScenarioGenerationTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        # 1ï¸âƒ£ ç²å–ç•¶å‰è¼ªæ¬¡ã€åœ–ç‰‡åˆ—è¡¨èˆ‡æœ€ä½³æ–¹æ¡ˆ ID (gate2)
        current_round = self.state.get("current_round", 0)
        case_images = self.state.get("case_image", [])
        gate2 = self.state.get("GATE2", None)

        # ç¢ºä¿ gate2 ç‚ºæ•´æ•¸å‹æ…‹
        if not isinstance(gate2, int):
            print("âš ï¸ GATE2 çš„å€¼ç„¡æ•ˆæˆ–æœªæ‰¾åˆ°")
            self.state["future_image"] = [{"future_image": "æ²¡æœ‰"}]
            return {"future_image": self.state["future_image"]}

        best_id = gate2

        # 2ï¸âƒ£ éæ¿¾ç¬¦åˆæ¢ä»¶çš„åœ–ç‰‡ï¼šæ‰¾åˆ° key ç‚º best_idï¼Œä¸” value ä»¥ "shell_result_{current_round}_" ç‚ºå‰ç¶´çš„é …ç›®
        expected_prefix = f"shell_result_{current_round}_"
        result = None
        for item in case_images:
            if isinstance(item, dict) and best_id in item:
                case_image_path = item[best_id]
                if isinstance(case_image_path, str) and case_image_path.startswith(expected_prefix):
                    result = item
                    break

        # 3ï¸âƒ£ è‹¥æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åœ–ç‰‡ï¼Œå‰‡è¿”å›æç¤º
        if not result:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç”Ÿæˆå›¾ï¼Œè½®æ¬¡ï¼š{current_round}ï¼Œæ–¹æ¡ˆ IDï¼š{best_id}")
            self.state["future_image"] = [{"future_image": "æ²¡æœ‰"}]
            return {"future_image": self.state["future_image"]}

        # 4ï¸âƒ£ æ›´æ–°ç‹€æ…‹ä¸¦è¿”å›çµæœï¼šå°‡çµæœåŒ…è£åœ¨åˆ—è¡¨ä¸­
        self.state["future_image"] = [result]
        print(f"âœ… æœªæ¥æƒ…å¢ƒå›¾ç”Ÿæˆå®Œæˆï¼Œå›¾ç‰‡ä¿å­˜ä¸º: {result}")
        return {"future_image": self.state["future_image"]}

# ç”Ÿæˆ 3D =ï¼šæ ¹æ“š Glb æª”å‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡ç”Ÿæˆå·¥å…·ï¼‰ç”Ÿæˆ 3D 
class Generate3DPerspective:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state        

        # 1ï¸âƒ£ ç²å–ç•¶å‰è¼ªæ¬¡ã€åœ–ç‰‡åˆ—è¡¨èˆ‡æœ€ä½³æ–¹æ¡ˆ ID (gate2)
        current_round = self.state.get("current_round", 0)
        case_images = self.state.get("case_image", [])
        gate2 = self.state.get("GATE2", None)

        # ç¢ºä¿ gate2 ç‚ºæ•´æ•¸å‹æ…‹
        if not isinstance(gate2, int):
            print("âš ï¸ GATE2 çš„å€¼ç„¡æ•ˆæˆ–æœªæ‰¾åˆ°")
            self.state["perspective_3D"] = "æ²¡æœ‰"
            return {"perspective_3D": self.state["perspective_3D"]}
        best_id = gate2

        # 2ï¸âƒ£ éæ¿¾ç¬¦åˆæ¢ä»¶çš„åœ–ç‰‡ï¼šæ‰¾åˆ° key ç‚º best_idï¼Œä¸” value ä»¥ "shell_result_{current_round}_" ç‚ºå‰ç¶´çš„é …ç›®
        expected_prefix = f"shell_result_{current_round}"
        selected_image = None
        for item in case_images:
            if isinstance(item, dict) and best_id in item:
                value = item[best_id]
                if isinstance(value, str) and value.startswith(expected_prefix):
                    selected_image = value
                    break

        # è‹¥æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åœ–ç‰‡ï¼Œå‰‡è¿”å›æç¤º
        if not selected_image:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç”Ÿæˆå›¾ï¼Œè½®æ¬¡ï¼š{current_round}ï¼Œæ–¹æ¡ˆ IDï¼š{best_id}")
            self.state["perspective_3D"] = "æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç”Ÿæˆå›¾"
            return {"perspective_3D": self.state["perspective_3D"]}

        # 3ï¸âƒ£ è‹¥é¸ä¸­çš„åœ–ç‰‡è·¯å¾‘ä¸æ˜¯å®Œæ•´è·¯å¾‘ï¼Œå‰‡è£œä¸Šç›®éŒ„ "./output/render_cache"
        OUTPUT_SHELL_CACHE_DIR = "./output/render_cache"
        if not os.path.isabs(selected_image):
            selected_image = os.path.join(OUTPUT_SHELL_CACHE_DIR, selected_image)

        # 4ï¸âƒ£ å‘¼å« generate_3D æ™‚ï¼Œä½¿ç”¨éµ "image_path" ä¸¦å‚³å…¥ selected_image
        object_file = generate_3D.invoke({
            "image_path": selected_image,
            "current_round": current_round,
        })
        object_video = object_file.get("video", "ç„¡ç”Ÿæˆçµæœ")
        object_glb = object_file.get("model", "ç„¡æ¨¡å‹")

        # æ›´æ–° 3D å½±ç‰‡èˆ‡æ¨¡å‹è³‡è¨Š
        existing_3D = self.state.get("perspective_3D", [])
        updated_3D = custom_add_messages(existing_3D, object_video)
        self.state["perspective_3D"] = updated_3D

        existing_model = self.state.get("model_3D", [])
        updated_model = custom_add_messages(existing_model, object_glb)
        self.state["model_3D"] = updated_model

        print(f"âœ… ç”Ÿæˆ 3D ä½ç½®: å½±ç‰‡:{object_video}ã€æ¨¡å‹:{object_glb}")
        return {"perspective_3D": self.state["perspective_3D"], "model_3D": self.state["model_3D"]}

# class Generate3DPerspectiveé¡¯ç¤ºæ¸¬è©¦:
#     def __init__(self, state: GlobalState):
#         self.state = state

#     def encode_file_to_data_url(self, file_path, mime_type):
#         """è®€å–æª”æ¡ˆä¸¦è½‰æ›æˆ data URL æ ¼å¼"""
#         if os.path.exists(file_path):
#             with open(file_path, "rb") as f:
#                 encoded = base64.b64encode(f.read()).decode("utf-8")
#             return f"data:{mime_type};base64,{encoded}"
#         else:
#             return None

#     def run(self, state=None):
#         if state is not None:
#             self.state = state        

#         # è£œä¸Šç›®éŒ„ "./output/model_cache"ï¼Œä¸¦æŒ‡å®šæ¸¬è©¦æª”æ¡ˆåç¨±
#         OUTPUT_MODEL_CACHE_DIR = "./output/model_cache"
#         selected_model = os.path.join(OUTPUT_MODEL_CACHE_DIR, "model_result_2.glb")
#         selected_mp4 = os.path.join(OUTPUT_MODEL_CACHE_DIR, "video_result_2.mp4")

#         # å°‡å½±ç‰‡èˆ‡æ¨¡å‹è½‰æ›æˆ data URLï¼Œå‚³å…¥å°æ‡‰çš„ MIME é¡å‹
#         video_data_url = self.encode_file_to_data_url(selected_mp4, "video/mp4")
#         model_data_url = self.encode_file_to_data_url(selected_model, "model/gltf-binary")

#         self.state["perspective_3D_display"] = video_data_url if video_data_url else "ç„¡ç”Ÿæˆçµæœ"
#         self.state["model_3D_display"] = model_data_url if model_data_url else "ç„¡æ¨¡å‹"

#         print(f"âœ… ç”Ÿæˆ 3D ä½ç½®: å½±ç‰‡:{selected_mp4}ã€æ¨¡å‹:{selected_model}")
#         return {
#             "perspective_3D": self.state["perspective_3D_display"],
#             "model_3D": self.state["model_3D_display"]
#         }
    
# æ·±åº¦è©•ä¼°ä»»å‹™ï¼šå‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡è¾¨è­˜å·¥å…·ï¼‰å°ç”Ÿæˆåœ–èˆ‡æœªä¾†æƒ…å¢ƒåœ–é€²è¡Œæ·±åº¦è©•ä¼° OK
class DeepEvaluationTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state      

        OUTPUT_EVAL_DIR = "./output/"
        os.makedirs(OUTPUT_EVAL_DIR, exist_ok=True) 

        current_round = self.state.get("current_round", 0) 
        eval_results_list = self.state.get("evaluation_result", [])  
        future_img = self.state.get("future_image", [])      
        perspective_3D = self.state.get("perspective_3D", [])
        design_advice_list = self.state.get("design_advice", [])
        
        # éæ¿¾å‡ºç•¶å‰è¼ªæ¬¡ä¸” state ç‚º True çš„è¨­è¨ˆæ–¹æ¡ˆï¼ˆå¿…é ˆæ˜¯å­—å…¸æ ¼å¼ï¼‰
        valid_advices = [
            advice for advice in design_advice_list
            if isinstance(advice, dict) and advice.get("round") == current_round and advice.get("state") == True
        ]
        
        # å¾æœ‰æ•ˆçš„è¨­è¨ˆæ–¹æ¡ˆä¸­å–å‡º "proposal" ä½œç‚º advice_text
        if valid_advices:
            selected_advice = valid_advices[0]
            advice_text = selected_advice.get("proposal", "ç„¡ç›®æ¨™")
        else:
            advice_text = "ç„¡ç›®æ¨™"

        # --- æå– image ---
        expected_prefix = f"shell_result_{current_round}"
        images = []  # å­˜æ”¾ç¬¦åˆæ¢ä»¶çš„ image å­—ä¸²
        # éæ­· future_img ä¸­çš„æ‰€æœ‰é …ç›®ï¼ˆæ¯å€‹é …ç›®çš†ç‚ºå­—å…¸ï¼‰
        for item in future_img:
            if isinstance(item, dict):
                for key, value in item.items():
                    # æª¢æŸ¥ key ç‚º int ä¸” value ç‚ºå­—ä¸²ï¼Œä¸”å‰ç¶´ç¬¦åˆ
                    if isinstance(key, int) and isinstance(value, str) and value.startswith(expected_prefix):
                        images.append(value)
                        # å¦‚æœå¸Œæœ›æ¯å€‹å­—å…¸åªæå–ä¸€å€‹ç¬¦åˆæ¢ä»¶çš„å€¼ï¼Œå¯ä»¥ break é€€å‡ºå…§å±¤è¿´åœˆ
                        break

        # --- æå– video ---
        expected_prefix2 = f"video_result_{current_round}"
        videos = []  # å­˜æ”¾ç¬¦åˆæ¢ä»¶çš„ video å­—ä¸²
        # perspective_3D é æœŸç‚º listï¼Œæ¯å€‹å…ƒç´ çš†ç‚ºå­—ä¸²
        for item in perspective_3D:
            if isinstance(item, str) and item.startswith(expected_prefix2):
                videos.append(item)

        # --- è·¯å¾‘çµ„åˆ ---
        OUTPUT_SHELL_CACHE_DIR = "./output/render_cache"
        image_paths = [os.path.join(OUTPUT_SHELL_CACHE_DIR, img) for img in images] if images else []

        OUTPUT_3D_CACHE_DIR = "./output/model_cache"
        video_paths = [os.path.join(OUTPUT_3D_CACHE_DIR, vid) for vid in videos] if videos else []

        # **é—œéµå­—ç”Ÿæˆæ–¹å‘ï¼š
        # çµåˆè¨­è¨ˆææ¡ˆç‰¹æ€§ï¼šè«‹è€ƒé‡è¨­è¨ˆææ¡ˆå¯èƒ½åŒ…å«çš„å…ƒç´ ï¼Œä¾‹å¦‚ï¼šå»ºç¯‰é¡å‹ã€æ›²é¢å½¢å¼ (ä¾‹å¦‚ï¼šé›™æ›²é¢ã€è‡ªç”±æ›²é¢ã€æ ¼æŸµæ›²é¢ã€è–„æ®¼æ›²é¢...)ã€ææ–™ç¨®é¡ (ä¾‹å¦‚ï¼šé›†æˆæã€è† åˆæœ¨ã€CLT...)ã€æ§‹é€ å·¥æ³•ã€å…¶ä»–è€ƒé‡ç­‰ã€‚
        # è¾¨è­˜åœ–ç‰‡äº†è§£æƒ…æ³ï¼šå‡è¨­å·²é€éåœ–ç‰‡è¾¨è­˜åˆæ­¥äº†è§£è¨­è¨ˆæ–¹æ¡ˆçš„è¦–è¦ºç‰¹å¾µï¼Œä¾‹å¦‚ï¼šæ›²é¢çš„è¤‡é›œç¨‹åº¦ã€çµæ§‹ç³»çµ±çš„é¡å‹ç­‰ã€‚è«‹æ ¹æ“šé€™äº›å¯èƒ½çš„åœ–ç‰‡è³‡è¨Šï¼Œç”Ÿæˆæ›´ç²¾ç¢ºçš„é—œéµå­—ã€‚   
        # è¨­è¨ˆææ¡ˆï¼š{advice_text} 

        # Step 1: é—œéµè© 
        keyword_prompt = (f"""
            è«‹ç”Ÿæˆé©ç”¨æ–¼æª¢ç´¢åƒè€ƒåšæ³•çš„ä¸­è‹±æ–‡é—œéµå­—ã€‚**æ ¼å¼ç‚º:ä¸­æ–‡(è‹±æ–‡)**
            æª¢ç´¢ç›®æ¨™ï¼šæ ¹æ“šåœ–ç‰‡ä¸­çš„å»ºç¯‰è¦ç´ å°‹æ‰¾é—œæ–¼æ›²é¢æœ¨æ§‹é€ å»ºç¯‰çš„è¨­è¨ˆæ¦‚å¿µã€æ–¹æ¡ˆã€æ¡ˆä¾‹ç ”ç©¶ç­‰è³‡æ–™ã€‚
            å°‹æ‰¾æ›²é¢æœ¨æ§‹é€ åœ¨è¨­è¨ˆã€ææ–™ã€å·¥æ³•ã€å¾ªç’°æ€§ã€æ°¸çºŒæ€§ç­‰æ–¹é¢çš„è¦ç¯„ã€æŠ€è¡“æŒ‡å—ã€å°ˆå®¶å»ºè­°ç­‰åƒè€ƒè³‡è¨Šã€‚ 
            """
        )
        img_key_output = img_recognition.invoke({
            "image_paths": image_paths,
            "prompt": keyword_prompt
        })        
        keywords = img_key_output.strip() if isinstance(img_key_output, str) else ""
        print("ç”Ÿæˆçš„é—œéµè©ï¼š", keywords)

        # Step 2: RAG prompt
        RAG_msg = ARCH_rag_tool.invoke(f"{keywords}")
        print("RAGæª¢ç´¢çµæœï¼š", RAG_msg)

            # æ•¸ä½è£½é€ èƒŒæ™¯çŸ¥è­˜: (ä¾‹å¦‚ï¼šæ©Ÿæ¢°æ‰‹è‡‚æœ¨æ§‹åŠ å·¥åŸç†ã€æ›²é¢åˆ†å‰²èˆ‡å±•é–‹æ¼”ç®—æ³•ã€åƒæ•¸åŒ–è¨­è¨ˆåœ¨æœ¨æ§‹å»ºç¯‰çš„æ‡‰ç”¨ã€æ•¸ä½çµ„è£æµç¨‹èˆ‡ç²¾åº¦æ§åˆ¶ç­‰ç›¸é—œæ–‡ç»ã€æŠ€è¡“æŒ‡å—ã€æ¡ˆä¾‹ç ”ç©¶é€£çµ)
            # Timber Curve Frame Pavilion è¨­è¨ˆè¦ç¯„: (ä¾‹å¦‚ï¼šè¨­è¨ˆåœ–èªªã€çµæ§‹åˆ†æå ±å‘Šã€ææ–™é¸ç”¨èªªæ˜ã€åˆæ­¥çš„è£½é€ æµç¨‹è¦åŠƒã€è¨­è¨ˆç›®æ¨™èˆ‡é æœŸæˆæœæè¿°ç­‰)
            # ç›¸é—œæ¡ˆä¾‹åƒè€ƒ: (ä¾‹å¦‚ï¼šå·²æˆåŠŸæ•¸ä½è£½é€ çš„æ›²é¢æœ¨æ§‹å»ºç¯‰æ¡ˆä¾‹ã€é¡ä¼¼çµæ§‹å½¢å¼çš„æ¡ˆä¾‹åˆ†æã€æ•¸ä½è£½é€ å·¥æ³•æ‡‰ç”¨æ¡ˆä¾‹ç­‰ï¼Œå¯æä¾›åœ–ç‰‡æˆ–é€£çµ)

        img_prompt = (   
            f"é‡å° timber curve frame pavilion è¨­è¨ˆæ–¹æ¡ˆæ¸²æŸ“åœ–é€²è¡Œæ·±å…¥è©•ä¼°ã€‚"
            f"ä½œç‚ºè³‡æ·±å»ºç¯‰è¨­è¨ˆè©•å¯©å§”å“¡ï¼Œè«‹é‡å°è£œå……æ¢ä»¶å‹•æ…‹èª¿æ•´è©•ä¼°æº–å‰‡ï¼Œä¸¦æä¾›**å…¬æ­£ä¸”æœ‰é‘‘åˆ¥åº¦çš„è©•åˆ†**ã€‚"
            f"ä½ çš„ä»»å‹™æ˜¯åŸºæ–¼ä»¥ä¸‹è©•ä¼°æº–å‰‡ï¼Œ**å®¢è§€è©•ä¼°**å…¶å»ºç¯‰å¤–æ®¼è¨­è¨ˆçš„å„ªåŠ£ã€‚"
            f"""
            **é€ å‹èˆ‡ç’°å¢ƒè„ˆçµ¡èåˆï¼šç¸½åˆ†10åˆ†
                è©•ä¼°å»ºç¯‰é€ å‹æ˜¯å¦èƒ½èå…¥å‘¨åœç’°å¢ƒè„ˆçµ¡ï¼Œä¾‹å¦‚ï¼šè‡ªç„¶æ™¯è§€ã€éƒ½å¸‚ç´‹ç†ã€åœ°åŸŸæ–‡åŒ–ã€‚
                è€ƒé‡å»ºç¯‰é€ å‹èˆ‡ç’°å¢ƒçš„å”èª¿æ€§ã€å‘¼æ‡‰æ€§ï¼Œä»¥åŠå°ç’°å¢ƒçš„å°Šé‡ç¨‹åº¦ã€‚
            **å ´æ‰€ç²¾ç¥èˆ‡ä½¿ç”¨è€…é—œæ³¨ï¼šç¸½åˆ†10åˆ†
                è©•ä¼°å»ºç¯‰è¨­è¨ˆæ˜¯å¦èƒ½ç‡Ÿé€ ç¨ç‰¹çš„å ´æ‰€ç²¾ç¥ï¼Œå›æ‡‰ä½¿ç”¨è€…çš„éœ€æ±‚èˆ‡é«”é©—ã€‚
                è€ƒé‡å»ºç¯‰ç©ºé–“çš„æ°›åœã€èˆ’é©åº¦ã€æ©Ÿèƒ½æ€§ï¼Œä»¥åŠå°ä½¿ç”¨è€…æƒ…æ„Ÿå’Œè¡Œç‚ºçš„å½±éŸ¿ã€‚
            **ææ–™åŠå·¥æ³•çš„ç’°å¢ƒåŠæ°£å€™æ‡‰å°ç¨‹åº¦ï¼šç¸½åˆ†10åˆ†
                è©•ä¼°é¸ç”¨çš„æœ¨æææ–™å’Œå·¥æ³•æ˜¯å¦èƒ½æœ‰æ•ˆæ‡‰å°ç•¶åœ°ç’°å¢ƒåŠæ°£å€™æ¢ä»¶ã€‚
                è€ƒé‡ææ–™çš„æ°¸çºŒæ€§ã€ç’°å¢ƒå‹å–„æ€§ã€æ°£å€™é©æ‡‰æ€§ï¼Œä»¥åŠå·¥æ³•çš„åˆç†æ€§ã€æ•ˆç‡æ€§ã€ææ–™æè€—ã€‚
            **å¤–æ®¼ç³»çµ±çš„ç¶­è­·æ€§èˆ‡è€ä¹…æ€§ï¼šç¸½åˆ†10åˆ†
                è©•ä¼°ç•¶å‰æ§‹é€ å½¢å¼çš„ç³»çµ±æ˜¯å¦è€ƒé‡åˆ°å¾ŒçºŒçš„ç¶­è­·èˆ‡é•·æœŸè€ä¹…æ€§ã€‚
                è€ƒé‡ç•¶å‰æ§‹é€ ç³»çµ±å…¨ç”Ÿå‘½é€±æœŸçš„å¾ªç’°æ€§ã€‚    
            **è£œå……æ¢ä»¶:{RAG_msg}
                
            è©•åˆ†æ¨™æº–:é‡å°ä»¥ä¸Šæ¯å€‹è©•ä¼°é …ç›®ï¼Œæ ¹æ“šæ–¹æ¡ˆè¡¨ç¾çµ¦äºˆ 1.0 - 10.0 åˆ†è©•åˆ† (1.0 = æ¥µå·®, 10.0 = æ¥µä½³)ã€‚
            è¼¸å‡ºæ ¼å¼:é‡å°æ¯å€‹è©•ä¼°é …ç›®æä¾›è©•åˆ†ä»¥åŠç°¡è¿°è©•åˆ†ç†ç”±ã€‚æœ€å¾Œéœ€è¨ˆç®—åŠ ç¸½å¾—åˆ†ä¸¦å¯«ç‚º**ç¸½åˆ†æ•¸:æ•¸å­—**
            """  
        )

        # èª¿ç”¨å·¥å…·é€²è¡Œæ·±åº¦è©•ä¼°
        img_eval_output = img_recognition.invoke({
            "image_paths": image_paths,
            "prompt": img_prompt
        })

        ##3Dè¾¨è­˜é‚è¼¯     
        ##é‚„éœ€è¦è¨­å®šRAGæœ¨æ§‹é€ è³‡æ–™
        vid_prompt = (   
            f"é‡å° timber curve frame pavilion è¨­è¨ˆæ–¹æ¡ˆæ¨¡å‹é€²è¡Œæ·±å…¥è©•ä¼°ã€‚"
            f"ä½œç‚ºå°ˆæ¥­çš„å»ºç¯‰å¸«ã€çµæ§‹æŠ€å¸«å…¼æ•¸ä½è£½é€ å°ˆå®¶ï¼Œè«‹é‡å°è£œå……æ¢ä»¶å‹•æ…‹èª¿æ•´è©•ä¼°æº–å‰‡ï¼Œä¸¦æä¾›**å…¬æ­£ä¸”æœ‰é‘‘åˆ¥åº¦çš„è©•åˆ†**ã€‚"
            f"ä½ çš„ä»»å‹™æ˜¯åŸºæ–¼ä»¥ä¸‹è©•ä¼°æº–å‰‡ï¼Œ**å®¢è§€è©•ä¼°**å…¶å»ºç¯‰è¨­è¨ˆçš„å„ªåŠ£ã€‚"      
            f"""
            **I.æ•´é«”æ§‹é€ ç³»çµ±ä¹‹åˆç†æ€§èˆ‡æ°¸çºŒæ€§:ç¸½åˆ†10åˆ†
                    **çµæ§‹é‚è¼¯æ€§:**  çµæ§‹ç³»çµ±æ˜¯å¦æ¸…æ™°ã€åˆç†ï¼Œèƒ½æœ‰æ•ˆå‚³éåŠ›æµä¸¦æŠµæŠ—å¤–åŠ›ï¼Ÿ (ä¾‹å¦‚ï¼šæŠ—å½ã€æŠ—å‰ªã€æŠ—æ‰­èƒ½åŠ›è©•ä¼°)
                    **çµæ§‹æ•ˆç‡:**  çµæ§‹ç³»çµ±æ˜¯å¦èƒ½ä»¥æœ€å°‘çš„ææ–™é”æˆæ‰€éœ€çš„è·¨åº¦èˆ‡æ‰¿è¼‰åŠ›ï¼Ÿ (ä¾‹å¦‚ï¼šæ§‹æç”¨é‡ã€è·¨åº¦èƒ½åŠ›æ¯”å€¼åˆ†æ)
                    **ææ–™æ°¸çºŒæ€§:**  çµæ§‹ææ–™é¸ç”¨æ˜¯å¦ç¬¦åˆæ°¸çºŒç™¼å±•åŸå‰‡ï¼Ÿ (ä¾‹å¦‚ï¼šå¯å†ç”Ÿææ–™æ¯”ä¾‹ã€ç¢³è¶³è·¡è©•ä¼°ã€ç”Ÿå‘½é€±æœŸè©•ä¼° LCA)
                    **ç’°å¢ƒå‹å–„æ€§:**  çµæ§‹ç³»çµ±çš„ç”Ÿç”¢ã€é‹è¼¸ã€å»ºé€ åŠæ‹†è§£éç¨‹å°ç’°å¢ƒçš„å½±éŸ¿ç¨‹åº¦ï¼Ÿ (ä¾‹å¦‚ï¼šç¢³æ’æ”¾é‡ã€å»¢æ£„ç‰©ç”¢ç”Ÿé‡è©•ä¼°)
            **II.ç´°éƒ¨æ§‹é€ ä¹‹æ©Ÿèƒ½æ€§èˆ‡æ•´åˆæ€§:ç¸½åˆ†10åˆ†
                    **æ©Ÿèƒ½å¯¦ç¾åº¦:**  ç´°éƒ¨æ§‹é€ æ˜¯å¦èƒ½æœ‰æ•ˆå¯¦ç¾å…¶é æœŸæ©Ÿèƒ½ï¼Ÿ (ä¾‹å¦‚ï¼šé€£æ¥å¼·åº¦ã€é˜²æ°´æ€§èƒ½ã€æ°£å¯†æ€§èƒ½è©•ä¼°)
                    **æ§‹é€ æ•´åˆæ€§:**  ç´°éƒ¨æ§‹é€ èˆ‡æ•´é«”çµæ§‹ç³»çµ±çš„å”èª¿æ€§èˆ‡æ•´åˆç¨‹åº¦ï¼Ÿ (ä¾‹å¦‚ï¼šåŠ›æµå‚³éçš„é€£çºŒæ€§ã€æ§‹é€ ç³»çµ±çš„å®Œæ•´æ€§)
                    **ç¯€é»è¨­è¨ˆ:**  ç¯€é»è¨­è¨ˆæ˜¯å¦å®‰å…¨å¯é ã€ç°¡æ½”æœ‰æ•ˆã€æ˜“æ–¼è£½é€ èˆ‡çµ„è£ï¼Ÿ (ä¾‹å¦‚ï¼šç¯€é»åŠ›å­¸æ€§èƒ½åˆ†æã€é€£æ¥æ–¹å¼æ•ˆç‡è©•ä¼°ã€çµ„è£è¤‡é›œåº¦åˆ†æ)
                    **ä»‹é¢å”èª¿æ€§:**  ç´°éƒ¨æ§‹é€ èˆ‡å…¶ä»–å»ºç¯‰ç³»çµ± (ä¾‹å¦‚ï¼šå¤–ç‰†ã€å±‹é¢ã€è¨­å‚™) çš„ä»‹é¢è™•ç†æ˜¯å¦å”èª¿åˆç†ï¼Ÿ (ä¾‹å¦‚ï¼šé˜²æ°´ç´°ç¯€ã€ä¿æº«éš”ç†±æªæ–½ã€ç®¡ç·šæ•´åˆæ–¹æ¡ˆ)
            **III. æ›²é¢é€ å‹èˆ‡æ§‹æˆå½¢å¼ä¹‹æŠ€è¡“å¯è¡Œæ€§:ç¸½åˆ†10åˆ†
                    **å¹¾ä½•è¤‡é›œåº¦:**  æ›²é¢é€ å‹çš„å¹¾ä½•å½¢å¼æ˜¯å¦éæ–¼è¤‡é›œï¼Œå¢åŠ è£½é€ èˆ‡å»ºé€ é›£åº¦ï¼Ÿ (ä¾‹å¦‚ï¼šæ›²ç‡è®ŠåŒ–åˆ†æã€æ›²é¢åˆ†æ ¼è¤‡é›œåº¦è©•ä¼°)
                    **è£½é€ æŠ€è¡“:**  æ¨¡å‹æ‰€å±•ç¾çš„æ›²é¢æ§‹æˆå½¢å¼ï¼Œåœ¨ç¾æœ‰è£½é€ æŠ€è¡“æ¢ä»¶ä¸‹æ˜¯å¦èƒ½å¯¦ç¾ï¼Ÿ (ä¾‹å¦‚ï¼šCNC åŠ å·¥å¯è¡Œæ€§ã€ç†±å£“æˆå‹å¯è¡Œæ€§ã€ç©å±¤è£½é€ å¯è¡Œæ€§è©•ä¼°)
                    **çµ„è£ç²¾åº¦:**  æ¨¡å‹æ‰€å±•ç¾çš„æ›²é¢ç²¾åº¦è¦æ±‚ï¼Œåœ¨ç¾å ´çµ„è£æ¢ä»¶ä¸‹æ˜¯å¦èƒ½é”æˆï¼Ÿ (ä¾‹å¦‚ï¼šæ§‹ä»¶åŠ å·¥ç²¾åº¦è¦æ±‚ã€çµ„è£èª¤å·®å®¹è¨±åº¦åˆ†æ)
                    **ç¶“æ¿Ÿæ€§:**  æ›²é¢é€ å‹çš„å¯¦ç¾æ˜¯å¦æœƒå°è‡´éé«˜çš„è£½é€ æˆæœ¬èˆ‡å·¥æœŸï¼Ÿ (ä¾‹å¦‚ï¼šææ–™æˆæœ¬åˆ†æã€åŠ å·¥æˆæœ¬ä¼°ç®—ã€å·¥æœŸè©•ä¼°)
            **IV. ææ–™æ‡‰ç”¨èˆ‡çµæ§‹é‚è¼¯ä¹‹å¥‘åˆæ€§:ç¸½åˆ†10åˆ†
                    **ææ–™ç‰¹æ€§ç™¼æ®:**  æ˜¯å¦å……åˆ†åˆ©ç”¨æœ¨æçš„åŠ›å­¸æ€§èƒ½ (ä¾‹å¦‚ï¼šæŠ—æ‹‰ã€æŠ—å£“ã€å½ˆæ€§æ¨¡é‡)ã€ç´‹ç†ç‰¹æ€§ã€è¼•è³ªé«˜å¼·ç­‰å„ªå‹¢ï¼Ÿ (ä¾‹å¦‚ï¼šææ–™åŠ›å­¸æ€§èƒ½åˆ†æã€ææ–™é¸ç”¨åˆç†æ€§è©•ä¼°)
                    **çµæ§‹é‚è¼¯æ¸…æ™°æ€§:**  çµæ§‹ç³»çµ±çš„è¨­è¨ˆæ˜¯å¦æ¸…æ™°åœ°å±•ç¾äº†ææ–™çš„åŠ›å­¸ç‰¹æ€§èˆ‡çµæ§‹é‚è¼¯ï¼Ÿ (ä¾‹å¦‚ï¼šçµæ§‹å—åŠ›åˆ†æã€åŠ›æµå‚³éè·¯å¾‘å¯è¦–åŒ–)
                    **ææ–™æ‡‰ç”¨æ•ˆç‡:**  ææ–™çš„æ‡‰ç”¨æ˜¯å¦ç¶“æ¿Ÿé«˜æ•ˆï¼Œé¿å…éåº¦è¨­è¨ˆæˆ–ææ–™æµªè²»ï¼Ÿ (ä¾‹å¦‚ï¼šææ–™ç”¨é‡å„ªåŒ–åˆ†æã€æ§‹ä»¶å°ºå¯¸åˆç†æ€§è©•ä¼°)
            **V. è£½é€ æµç¨‹èˆ‡çµ„è£å¯è¡Œæ€§:ç¸½åˆ†10åˆ†
                    **çµ„è£æµç¨‹å¯è¡Œæ€§:**  æ¨¡å‹çš„çµ„è£æ­¥é©Ÿæ˜¯å¦æ¸…æ™°åˆç†ã€æ˜“æ–¼ç†è§£èˆ‡æ“ä½œï¼Ÿ (ä¾‹å¦‚ï¼šçµ„è£æ­¥é©Ÿæµç¨‹åœ–ã€çµ„è£é›£åº¦åˆ†æ)
                    **è£½é€ æ•ˆç‡å„ªåŒ–:**  è¨­è¨ˆæ–¹æ¡ˆæ˜¯å¦å…·æœ‰å„ªåŒ–è£½é€ æ•ˆç‡çš„æ½›åŠ›ï¼Ÿ (ä¾‹å¦‚ï¼šé è£½åŒ–ç¨‹åº¦è©•ä¼°ã€æ¨¡çµ„åŒ–è¨­è¨ˆåˆ†æã€è‡ªå‹•åŒ–ç”Ÿç”¢æ‡‰ç”¨æ½›åŠ›)
                    **ææ–™æè€—æ§åˆ¶:**  è£½é€ éç¨‹ä¸­æ˜¯å¦èƒ½æœ‰æ•ˆæ§åˆ¶ææ–™æè€—ï¼Ÿ (ä¾‹å¦‚ï¼šææ–™åˆ‡å‰²å„ªåŒ–æ–¹æ¡ˆã€å‰©æ–™å†åˆ©ç”¨ç­–ç•¥)
                    **æ§‹ä»¶æ¬é‹:**  æ§‹ä»¶çš„å°ºå¯¸ã€é‡é‡æ˜¯å¦ä¾¿æ–¼æ¬é‹èˆ‡é‹è¼¸ï¼Ÿ (ä¾‹å¦‚ï¼šæ§‹ä»¶å°ºå¯¸é™åˆ¶åˆ†æã€é‹è¼¸æˆæœ¬ä¼°ç®—ã€ç¾å ´åŠè£å¯è¡Œæ€§)
            **VI. ç¾å­¸è¡¨ç¾èˆ‡ç©ºé–“æ„è±¡:ç¸½åˆ†10åˆ†
                    **é€ å‹ç¾æ„Ÿ:**  æ•´é«”é€ å‹æ˜¯å¦ç¬¦åˆå»ºç¯‰ç¾å­¸åŸå‰‡ï¼Œå…·æœ‰è¦–è¦ºå¸å¼•åŠ›ï¼Ÿ (ä¾‹å¦‚ï¼šæ¯”ä¾‹å”èª¿æ€§è©•ä¼°ã€ç·šæ¢æµæš¢åº¦åˆ†æã€å½¢å¼ç¾æ„Ÿè©•åƒ¹)
                    **å…‰å½±æ•ˆæœ:**  æ•´é«”ç©ºé–“æ˜¯å¦å±•ç¾è‰¯å¥½çš„å…‰å½±æ•ˆæœï¼Œæå‡ç©ºé–“çš„å±¤æ¬¡æ„Ÿèˆ‡ç”Ÿå‹•æ€§ï¼Ÿ  (ä¾‹å¦‚ï¼šä½¿ç”¨è€…é æœŸè©•åƒ¹)


            **è£œå……æ¢ä»¶:{RAG_msg}

            è©•åˆ†æ¨™æº–:é‡å°ä»¥ä¸Šæ¯å€‹è©•ä¼°é …ç›®ï¼Œæ ¹æ“šæ–¹æ¡ˆè¡¨ç¾çµ¦äºˆ 1.0 - 10.0 åˆ†è©•åˆ† (1.0 = æ¥µå·®, 10.0 = æ¥µä½³)ã€‚
            è¼¸å‡ºæ ¼å¼:é‡å°æ¯å€‹è©•ä¼°é …ç›®æä¾›è©•åˆ†ä»¥åŠç°¡è¿°è©•åˆ†ç†ç”±ã€‚æœ€å¾Œéœ€è¨ˆç®—åŠ ç¸½å¾—åˆ†ä¸¦å¯«ç‚º**ç¸½åˆ†æ•¸:æ•¸å­—**
            """  
        )

        vid_eval_output = video_recognition.invoke({
            "video_paths": video_paths,
            "prompt": vid_prompt
        })

        img_eval_text = img_eval_output.strip() if isinstance(img_eval_output, str) else ""
        vid_eval_text = vid_eval_output.strip() if isinstance(vid_eval_output, str) else ""

        ##evaluation_countå¹³å‡
        def extract_total_score(text):
            m = re.search(r"\*\*ç¸½åˆ†æ•¸:([\d.]+)\*\*", text)
            if m:
                try:
                    return float(m.group(1))
                except:
                    return 0.0
            else:
                # å¦‚æœæ²’æœ‰ç¬¦åˆçš„ç¸½åˆ†æ•¸æ ¼å¼ï¼Œå–æ–‡æœ¬ä¸­æ‰€æœ‰æµ®é»æ•¸ï¼Œä¸¦è¿”å›æœ€å¤§çš„æ•¸å­—
                numbers = re.findall(r"(\d+(?:\.\d+)?)", text)
                if numbers:
                    return max(map(float, numbers))
                return 0.0
        img_total = extract_total_score(img_eval_text)
        vid_total = extract_total_score(vid_eval_text)
        all_score = img_total + vid_total

        # çµ„åˆæœ¬è¼ªçš„è©•ä¼°çµæœï¼Œæ ¼å¼ç‚º { current_round: ç•¶å‰è¼ªæ¬¡, eval_result: imgè©•èª, eval_result2: videoè©•èª}
        current_eval_result = {
            "current_round": current_round,            
            "eval_result": img_eval_text,
            "eval_result2": vid_eval_text
        }
        eval_results_list =custom_add_messages(eval_results_list, current_eval_result)
        self.state["evaluation_result"] = eval_results_list

        # çµ„åˆæœ¬è¼ªçš„å¹³å‡åˆ†æ•¸ï¼Œæ ¼å¼ç‚º {current_round: average_score}ï¼Œé€™è£¡å°‡ current_round ä½œç‚ºå­—ä¸²éµå­˜å…¥
        current_eval_count = {str(current_round): all_score}
        eval_count_list = self.state.get("evaluation_count", [])
        eval_count_list = custom_add_messages(eval_count_list, current_eval_count)
        self.state["evaluation_count"] = eval_count_list

        # âœ… å­˜å…¥æª”æ¡ˆ
        # çµ„åˆ Markdown æ ¼å¼å…§å®¹ï¼Œå°‡è©•ä¼°çµæœå¯«å…¥ Markdown æª”æ¡ˆ
        md_content = f"""# Evaluation Result for Round {current_round}

        ## Image Evaluation
        {img_eval_text}

        ## Video Evaluation
        {vid_eval_text}

        ## Total Score: {all_score}
        """
        eval_file_path = os.path.join(OUTPUT_EVAL_DIR, f"eval_result_{current_round}.md")
        with open(eval_file_path, "w", encoding="utf-8") as f:
            f.write(md_content)

        self.state["current_round"] = current_round + 1  
        print(f"âœ… æ·±åº¦è©•ä¼°å®Œæˆï¼Œç•¶å‰è¼ªæ¬¡: {current_round}")
        print(f"ğŸ“Œ è©•ä¼°çµæœ: {current_eval_result}")
        print(f"ğŸ“Œ å¹³å‡åˆ†æ•¸: {all_score}")
        return {
            "evaluation_result": self.state["evaluation_result"],
            "evaluation_count": self.state["evaluation_count"],
            "current_round": self.state["current_round"]
        }

# è©•ä¼°æª¢æŸ¥ä»»å‹™ï¼šæ ¹æ“šè©•ä¼°æ¬¡æ•¸æ±ºå®šæµç¨‹è·¯ç”±ï¼ˆåƒè€ƒæ¢ä»¶åˆ†æ”¯ç¯„æœ¬é‚è¼¯ï¼‰
class EvaluationCheckTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state=None):
        if state is not None:
            self.state = state

        count = self.state.get("current_round", 0)
        if count < 3:
            self.state["evaluation_status"] = "NO"
            print(f"EvaluationCheckTaskï¼šè©•ä¼°æ¬¡æ•¸ {count} æœªé”æ¨™ï¼Œå°‡è¿”å› RAGdesignThinking åŸ·è¡Œä¸‹ä¸€è¼ªã€‚")
        else:
            self.state["evaluation_status"] = "YES"
            print(f"EvaluationCheckTaskï¼šè©•ä¼°æ¬¡æ•¸ {count} é”æ¨™ï¼Œæµç¨‹çµæŸã€‚")
        return {"evaluation_status": self.state["evaluation_status"]}

# ç¸½è©•ä¼°ä»»å‹™(ç”¨æˆ¶å¯ä»‹å…¥)
class FinalEvaluationTask:
    def __init__(self, state: dict, short_term=None, long_term=None):
        self.state = state
        # è‹¥æœªå‚³å…¥å‰‡ä½¿ç”¨é è¨­çš„è¨˜æ†¶ç®¡ç†å™¨
        self.short_term = short_term if short_term is not None else get_short_term_memory()
        self.long_term = long_term if long_term is not None else get_long_term_store()

    def run(self, state=None):
        if state is not None:
            self.state = state

        # å¾ state ä¸­å–å¾—è©•ä¼°çµæœèˆ‡è©•åˆ†ï¼ˆç´¯åŠ åˆ—è¡¨ï¼‰
        eval_results = self.state.get("evaluation_result", [])
        eval_counts = self.state.get("evaluation_count", [])

        # å¾è¨˜æ†¶ç®¡ç†å™¨è®€å–çŸ­æœŸèˆ‡é•·æœŸè¨˜æ†¶å…§å®¹
        short_memory = self.short_term.retrieve_all() if hasattr(self.short_term, "retrieve_all") else ""
        long_memory = self.long_term.retrieve_all() if hasattr(self.long_term, "retrieve_all") else ""

        # å»ºç«‹å„ªåŒ–å¾Œçš„ promptï¼š
        # è¦æ±‚ LLM æ ¹æ“šç•¶å‰è¼ªæ¬¡ç›´æ¥æŒ‡å‡ºå“ªå€‹è¨­è¨ˆæ–¹æ¡ˆè¼ƒå¥½ï¼Œä¸¦åˆ†æå‰©é¤˜æ–¹æ¡ˆçš„å„ªç¼ºé»ï¼Œ
        # æœ€å¾Œçµ¦å‡ºæŒçºŒåŸ·è¡Œæ–¹æ¡ˆçš„æ·±å…¥å»ºè­°ã€‚
        current_round = self.state.get("current_round", 0)
        summary_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹è©•ä¼°çµæœï¼š      
        ã€è©•ä¼°çµæœã€‘
        """
        for res in eval_results:
            round_num = res.get("current_round", "æœªçŸ¥")
            summary_prompt += f"\nè¼ªæ¬¡ {round_num}ï¼š"
            summary_prompt += f"\n - åœ–ç‰‡è©•ä¼°ï¼š{res.get('eval_result', 'ç„¡')}"
            summary_prompt += f"\n - 3D è¦–è§’è©•ä¼°ï¼š{res.get('eval_result2', 'ç„¡')}\n"
        
        summary_prompt += "\nã€è©•åˆ†çµæœã€‘\n"
        for count in eval_counts:
            for round_key, score in count.items():
                summary_prompt += f"è¼ªæ¬¡ {round_key}ï¼šç¸½åˆ† {score}\n"

        summary_prompt += "\nã€è¨˜æ†¶å…§å®¹ã€‘\n"
        summary_prompt += "çŸ­æœŸè¨˜æ†¶ï¼š" + (short_memory if short_memory else "ç„¡") + "\n"
        summary_prompt += "é•·æœŸè¨˜æ†¶ï¼š" + (long_memory if long_memory else "ç„¡") + "\n\n"

        summary_prompt += f"""è«‹ç¶œåˆä»¥ä¸Šè³‡è¨Šï¼Œè«‹ç›´æ¥æŒ‡å‡ºåœ¨ç¬¬ {current_round} è¼ªçš„æ–¹æ¡ˆè¡¨ç¾æœ€ä½³ï¼Œ
        ä¸¦è©³ç´°èªªæ˜è©²æ–¹æ¡ˆçš„å„ªé»ï¼ŒåŒæ™‚åˆ†æå…¶ä»–æ–¹æ¡ˆçš„å„ªç¼ºé»ã€‚æœ€å¾Œï¼Œè«‹æä¾›ä¸€å€‹æŒçºŒåŸ·è¡Œæ­¤æ–¹æ¡ˆçš„æ·±å…¥å»ºè­°ã€‚
        """

        # å‚³å…¥çš„ prompt å¿…é ˆæ˜¯ä¸€å€‹åˆ—è¡¨ï¼Œä¸”æ¯å€‹å…ƒç´ éœ€ç‚º BaseMessageï¼Œä¾‹å¦‚ä½¿ç”¨ SystemMessage åŒ…è£
        llm_response = llm.invoke([SystemMessage(content=summary_prompt)])
        final_text = llm_response.content if hasattr(llm_response, "content") else llm_response

        # å°‡æœ€çµ‚æ‘˜è¦å­˜å…¥ state ä¸­
        self.state["final_evaluation"] = final_text

        print("âœ… ç¸½è©•ä¼°ä»»å‹™å®Œæˆï¼")
        print(f"ğŸ“Œ ç¸½è©•ä¼°çµæœ:\n{final_text}")
        return {"final_evaluation": self.state["final_evaluation"]}


    # def interactive_query(self, query: str):
    #     """
    #     ç•¶ç”¨æˆ¶è©¢å•æ™‚ï¼Œåˆ©ç”¨ç¾æœ‰çš„æœ€çµ‚è©•ä¼°æ‘˜è¦ä»¥åŠçŸ­æœŸè¨˜æ†¶ä¾†å›è¦†å•é¡Œã€‚
    #     """
    #     # å¾ state æˆ–é•·æœŸè¨˜æ†¶ä¸­å–å‡ºæœ€çµ‚è©•ä¼°æ‘˜è¦
    #     final_eval = self.state.get("final_evaluation", "")

    #     if not final_eval:
    #         final_eval = self.long_term.get("final_evaluation") or ""

    #     interactive_prompt = (
    #         f"æ ¹æ“šä»¥ä¸‹æœ€çµ‚è©•ä¼°æ‘˜è¦ï¼Œè«‹å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š\n\n"
    #         f"{final_eval}\n\n"
    #         f"ç”¨æˆ¶å•é¡Œï¼š{query}\n"
    #         "è«‹ä»¥ä¸­æ–‡è©³ç›¡å›ç­”ï¼š"
    #     )
    #     response = llm.invoke({"prompt": interactive_prompt})
    #     answer = response.content if hasattr(response, "content") else response

    #     # å¯å°‡æ­¤äº¤äº’å…§å®¹å­˜å…¥çŸ­æœŸè¨˜æ†¶ï¼Œæ–¹ä¾¿å¾ŒçºŒå°è©±è¿½è¹¤
    #     self.short_term.save("final_evaluation_interaction", {"query": query, "answer": answer})

    #     return answer


# =============================================================================
# å»ºç«‹å·¥ä½œæµç¨‹åœ– (Graph Setup)
# =============================================================================
workflow = StateGraph(GlobalState)
# workflow.config_schema = AssistantConfig

state = {
    "è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½": [],
    "design_summary": "",
    "analysis_img": "",
    "site_analysis": "",
    "design_advice": "",
    "case_image": "",
    "outer_prompt": "",
    "future_image": "",
    "perspective_3D": "",
    "model_3D": "",
    "GATE1": 1,
    "GATE2": 1,
    "GATE_REASON1": "",
    "GATE_REASON2": "",
    "current_round": 0,
    "evaluation_count": 0,
    "evaluation_status": "",
    "evaluation_result": "",
    "final_evaluation": ""
}

question_task = QuestionTask(state)
site_analysis_task = SiteAnalysisTask(state)
rag_thinking = RAGdesignThinking(state)
gate_check1 = GateCheck1(state)
shell_prompt = OuterShellPromptTask(state)
image_render = CaseScenarioGenerationTask(state)
gate_check2 = GateCheck2(state)
future_scenario = FutureScenarioGenerationTask(state)
generate_P3D = Generate3DPerspective(state)
deep_evaluation = DeepEvaluationTask(state)
evaluation_check = EvaluationCheckTask(state)
final_eval = FinalEvaluationTask(state)
# GenerateReactFlow = GenerateReactFlowTask(state)

workflow.set_entry_point("question_summary")

workflow.add_node("question_summary", question_task.run)
workflow.add_node("analyze_site", site_analysis_task.run)
workflow.add_node("designThinking", rag_thinking.run)
workflow.add_node("GateCheck1", gate_check1.run)
workflow.add_node("shell_prompt", shell_prompt.run)
workflow.add_node("image_render", image_render.run)
workflow.add_node("GateCheck2", gate_check2.run)
workflow.add_node("generate_3D", generate_P3D.run)
workflow.add_node("future_scenario", future_scenario.run)
workflow.add_node("deep_evaluation", deep_evaluation.run)
workflow.add_node("evaluation_check", evaluation_check.run)
workflow.add_node("final_eval", final_eval.run)
# workflow.add_node("GenerateReactFlow", GenerateReactFlow.run)

workflow.add_edge("question_summary", "analyze_site")
workflow.add_edge("analyze_site", "designThinking")
workflow.add_edge("designThinking", "GateCheck1")
workflow.add_edge("shell_prompt", "image_render")
workflow.add_edge("image_render", "GateCheck2")
workflow.add_edge("future_scenario", "deep_evaluation")
workflow.add_edge("generate_3D", "deep_evaluation")
workflow.add_edge("deep_evaluation", "evaluation_check")
# workflow.add_edge("deep_evaluation", "GenerateReactFlow")
workflow.add_edge("final_eval", END)

# GateCheck1 æ¢ä»¶ç¯€é»
workflow.add_conditional_edges(
    "GateCheck1",
    lambda state: "YES" if state.get("GATE1") == "æœ‰" else "NO",
    {
        "YES": "shell_prompt", "NO": "designThinking"  
    }
)

# GateCheck2 æ¢ä»¶ç¯€é»
workflow.add_conditional_edges(
    "GateCheck2",
    lambda state: "YES" if isinstance(state.get("GATE2"), int) else "NO",
    { "YES": "future_scenario", "NO": "shell_prompt" }
)
workflow.add_edge("GateCheck2", "generate_3D") 

workflow.add_conditional_edges("evaluation_check",lambda state: state["evaluation_status"],
    { "NO": "designThinking",   "YES": "final_eval"  })

# =============================================================================
# æ§‹å»ºä¸¦é‹è¡Œæµç¨‹
# =============================================================================
graph = workflow.compile()

# graph.invoke(state)
graph.name = "Multi-Agent System"
