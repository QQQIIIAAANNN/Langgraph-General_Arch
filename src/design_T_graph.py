import os
import re
import ast
import json
import base64
import time # æ–°å¢å°å…¥
from dotenv import load_dotenv
from langgraph.graph.state import StateGraph, START, END
from typing import List
from langgraph.graph import add_messages
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from typing import TypedDict, Annotated, Sequence
from typing_extensions import TypedDict
from src.memory import get_long_term_store, get_short_term_memory
from src.tools.img_recognition import img_recognition
from src.tools.prompt_generation import prompt_generation
from src.tools.case_render_image import case_render_image
from src.tools.simulate_future_image import simulate_future_image
from src.tools.ARCH_rag_tool import ARCH_rag_tool
from src.tools.video_recognition import video_recognition
from src.tools.generate_3D import generate_3D
from src.tools.gemini_search_tool import perform_grounded_search
from src.tools.gemini_image_generation_tool import generate_gemini_image
from src.T_config import GraphOverallConfig # LLM_INSTANCE å’Œ PROMPTS_INSTANCE ä»ç„¶å¯ä»¥å°å…¥ï¼Œä½†ç¯€é»å…§ä¸»è¦ç”¨ config

# è¼‰å…¥ .env è¨­å®š
load_dotenv()

# å…¨å±€çš„ LLM_INSTANCE å’Œ PROMPTS_INSTANCE ä¸»è¦ä½œç‚ºå¾Œå‚™æˆ–ç”¨æ–¼éç¯€é»ä¸Šä¸‹æ–‡
# ç¯€é»å…§éƒ¨æ‡‰ä½¿ç”¨å¾ config å‚³å…¥çš„å¯¦ä¾‹
# default_llm = LLM_INSTANCE # å¯ä»¥ä¿ç•™ï¼Œä½†ç¯€é»å…§ä¸ç›´æ¥ç”¨
# default_prompts = PROMPTS_INSTANCE # å¯ä»¥ä¿ç•™ï¼Œä½†ç¯€é»å…§ä¸ç›´æ¥ç”¨


# =============================================================================
# è¼”åŠ©å‡½æ•¸ï¼Œç”¨æ–¼ç¢ºä¿é…ç½®æ˜¯ GraphOverallConfig çš„å¯¦ä¾‹
# =============================================================================
def ensure_graph_overall_config(config_input: any) -> GraphOverallConfig: # æ¥å—æ›´é€šç”¨çš„é¡å‹
    if isinstance(config_input, GraphOverallConfig):
        print("DEBUG ensure_graph_overall_config: Input is already GraphOverallConfig instance.")
        return config_input

    actual_config_dict = None
    print(f"DEBUG ensure_graph_overall_config: Received config_input type: {type(config_input)}, value: {repr(config_input)}")

    if hasattr(config_input, 'get') and callable(getattr(config_input, 'get')): # æª¢æŸ¥æ˜¯å¦åƒå­—å…¸ä¸€æ¨£æ“ä½œ
        # æ¨™æº–çš„ LangGraph RunnableConfig å°‡é…ç½®æ”¾åœ¨ 'configurable' éµä¸‹
        # æˆ‘å€‘ä¹Ÿè™•ç†ç›´æ¥å‚³éæ™®é€šå­—å…¸çš„æƒ…æ³
        if 'configurable' in config_input and isinstance(config_input['configurable'], dict):
            print("DEBUG ensure_graph_overall_config: Detected RunnableConfig-like structure, using config_input['configurable']")
            actual_config_dict = config_input['configurable']
        elif all(isinstance(k, str) for k in config_input.keys()): # ç²—ç•¥æª¢æŸ¥æ˜¯å¦ç‚ºæ™®é€šå­—å…¸
            print("DEBUG ensure_graph_overall_config: Assuming config_input is the plain config dictionary.")
            actual_config_dict = dict(config_input) # ç¢ºä¿æ˜¯æ™®é€šå­—å…¸
        else:
            print(f"DEBUG ensure_graph_overall_config: config_input is dict-like but not recognized structure: {repr(config_input)}")
            # å¦‚æœä¸æ˜¯æœŸæœ›çš„çµæ§‹ï¼Œä½†ä»ç„¶æ˜¯ dict-likeï¼Œå˜—è©¦ç›´æ¥ä½¿ç”¨å®ƒ
            # é€™å¯èƒ½åœ¨æŸäº›æƒ…æ³ä¸‹æœ‰æ•ˆï¼Œä½†åœ¨å…¶ä»–æƒ…æ³ä¸‹å¯èƒ½å°è‡´ Pydantic éŒ¯èª¤
            actual_config_dict = dict(config_input)


    if actual_config_dict is None:
        raise TypeError(f"Could not extract a valid configuration dictionary from input type {type(config_input)}. Value: {repr(config_input)}")

    print(f"DEBUG ensure_graph_overall_config: Final dictionary for Pydantic instantiation: {repr(actual_config_dict)}")
    print(f"DEBUG ensure_graph_overall_config: Value of 'run_site_analysis' in final dict for Pydantic: {actual_config_dict.get('run_site_analysis')}")
    
    try:
        # åœ¨å¯¦ä¾‹åŒ–å‰æ‰“å° GraphOverallConfig ä¸­ run_site_analysis_raw_value å­—æ®µçš„é è¨­å€¼å’Œåˆ¥å
        field_info_raw = GraphOverallConfig.model_fields.get("run_site_analysis_raw_value")
        if field_info_raw:
            print(f"DEBUG ensure_graph_overall_config: Model field 'run_site_analysis_raw_value' - default: {field_info_raw.default}, alias: {field_info_raw.alias}")
        else:
            print("DEBUG ensure_graph_overall_config: Could not get field_info for 'run_site_analysis_raw_value'")

        instance = GraphOverallConfig(**actual_config_dict)
        
        # æ‰“å°å¯¦ä¾‹åŒ–å¾Œçš„å€¼
        print(f"DEBUG ensure_graph_overall_config: Instance created. instance.run_site_analysis_raw_value = {getattr(instance, 'run_site_analysis_raw_value', 'N/A')}, instance.run_site_analysis = {instance.run_site_analysis}")
        return instance
    except Exception as e:
        print(f"DEBUG ensure_graph_overall_config: Error during GraphOverallConfig instantiation: {e}")
        print(f"DEBUG ensure_graph_overall_config: Failing dictionary was: {repr(actual_config_dict)}")
        raise e


# =============================================================================
# å»ºç«‹ LLM å¯¦ä¾‹ï¼ˆä¸å†è¨­å®š system_message å±¬æ€§ï¼‰
# =============================================================================
# é€™éƒ¨åˆ†ç¶å®šå·¥å…·çš„ LLM å¯¦ä¾‹ï¼Œå¦‚æœå·¥å…·èª¿ç”¨ä¹Ÿéœ€è¦é…ç½®åŒ–ï¼Œå‰‡éœ€è¦æ›´è¤‡é›œçš„è™•ç†
# ç›®å‰å‡è¨­å·¥å…·ç¶å®šçš„ LLM å¯ä»¥ä½¿ç”¨é è¨­é…ç½®çš„ LLM
# æˆ–è€…ï¼Œé€™äº›ç¶å®šå¯ä»¥åœ¨ invoke æ™‚å‹•æ…‹å‰µå»ºï¼ŒåŸºæ–¼å‚³å…¥çš„ config.llm_config
# ç‚ºç°¡åŒ–ï¼Œæš«æ™‚ä¿ç•™å…¨å±€ llm çš„ç”¨æ³•é€²è¡Œå·¥å…·ç¶å®š

# ä½¿ç”¨ T_config ä¸­çš„é è¨­é…ç½®ä¾†åˆå§‹åŒ–ä¸€å€‹ LLM å¯¦ä¾‹ï¼Œä¸»è¦ç”¨æ–¼å·¥å…·ç¶å®š
_temp_default_config_for_tools = GraphOverallConfig()
_tool_binding_llm = _temp_default_config_for_tools.llm_config.get_llm()

llm_with_img = _tool_binding_llm.bind_tools([img_recognition])
llm_with_ARCHrag = _tool_binding_llm.bind_tools([ARCH_rag_tool])
llm_with_prompt_gen = _tool_binding_llm.bind_tools([prompt_generation])
llm_with_gen2 = _tool_binding_llm.bind_tools([simulate_future_image])


# =============================================================================
# å®šç¾©å…¨å±€ç‹€æ…‹ (GlobalState)
# =============================================================================
class GlobalState(TypedDict, total=False):
    è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½ : Annotated[Sequence[BaseMessage], add_messages]
    design_summary: str
    analysis_img: str
    site_analysis: str
    design_advice: list
    outer_prompt: list
    case_image: list
    future_image: list
    perspective_3D: list
    model_3D: list
    GATE1: str
    GATE2: str 
    GATE_REASON1: str
    GATE_REASON2: str
    current_round: int
    evaluation_count: int
    evaluation_result: list
    evaluation_status: str
    final_evaluation: str

def custom_add_messages(existing: list, new: list) -> list:
    # ç¢ºä¿ new ç‚ºåˆ—è¡¨
    if not isinstance(new, list):
        new = [new]
    processed_new = []
    for msg in new:
        # å¦‚æœè¨Šæ¯å…·æœ‰ role å±¬æ€§ä¸”é è¨­ç‚º "human"ï¼Œå‰‡ä¿®æ”¹ç‚º "system"
        if hasattr(msg, "role"):
            if msg.role == "human":
                msg.role = "system"
        processed_new.append(msg)
    return existing + processed_new

# =============================================================================
# å„ä»»å‹™å®šç¾©
# =============================================================================

# å•é¡Œä»£ç†ä»»å‹™ï¼šæç¤ºç”¨æˆ¶è¼¸å…¥è¨­è¨ˆç›®æ¨™ã€è¨­è¨ˆéœ€æ±‚ã€æ–¹æ¡ˆé¡å‹ã€æ–¹æ¡ˆåå¥½ OK
class QuestionTask:
    def __init__(self, state: GlobalState):
        self.state = state
    
    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state
        
        active_config = ensure_graph_overall_config(config)

        current_llm = active_config.llm_config.get_llm()
        active_language = active_config.llm_output_language

        user_input = self.state["è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½"][0].content
        print("âœ… ç”¨æˆ¶çš„è¨­è¨ˆéœ€æ±‚å·²è¨˜éŒ„ï¼š", user_input)

        # ç”Ÿæˆç”¨æ–¼æª¢ç´¢çš„æŸ¥è©¢ï¼Œç¾åœ¨æœƒæ›´é€šç”¨ï¼Œä¸åƒ…é™æ–¼ ARCH_rag_tool
        # åŸä¾†çš„ keyword_prompt_content å¯ä»¥ç¹¼çºŒä½¿ç”¨æˆ–å¾®èª¿
        keyword_prompt_content = active_config.question_task_keyword_prompt_template.format(
            user_input=user_input,
            llm_output_language=active_language
        )
        keywords_msg = current_llm.invoke([SystemMessage(content=keyword_prompt_content)])
        # keywords_text ç¾åœ¨ä½œç‚ºä¸€å€‹é€šç”¨çš„æœç´¢æŸ¥è©¢
        search_query_text = keywords_msg.content.strip()
        print("ç”Ÿæˆçš„é€šç”¨æœç´¢æŸ¥è©¢ï¼š", search_query_text)

        # 1. ä½¿ç”¨ ARCH_rag_tool
        arch_rag_results = ""
        try:
            arch_rag_msg_content = ARCH_rag_tool.invoke(search_query_text) # æˆ–è€…ä½¿ç”¨æ›´ç²¾ç¢ºçš„æŸ¥è©¢
            if isinstance(arch_rag_msg_content, str):
                arch_rag_results = arch_rag_msg_content
            print("ARCH_rag_tool æª¢ç´¢çµæœï¼š", arch_rag_results)
        except Exception as e:
            print(f"âš ï¸ ARCH_rag_tool èª¿ç”¨å¤±æ•—: {e}")
            arch_rag_results = "ARCH RAG å·¥å…·æª¢ç´¢å¤±æ•—ã€‚"

        # 2. ä½¿ç”¨ perform_grounded_search
        grounded_search_results_text = ""
        # grounded_search_files = [] # å¦‚æœéœ€è¦è™•ç†åœ–ç‰‡ç­‰æ–‡ä»¶
        try:
            # å‡è¨­ perform_grounded_search è¿”å›ä¸€å€‹å­—å…¸ï¼ŒåŒ…å« text_content å’Œ images
            # æˆ‘å€‘ä¸»è¦é—œå¿ƒ text_content
            # æŸ¥è©¢çš„ prompt å¯ä»¥èˆ‡ ARCH_rag_tool çš„æŸ¥è©¢ç›¸åŒï¼Œæˆ–è€…é‡å°æ€§èª¿æ•´
            # é€™è£¡æˆ‘å€‘ä½¿ç”¨ç›¸åŒçš„ search_query_text
            # æç¤ºï¼šperform_grounded_search çš„æŸ¥è©¢å¯ä»¥æ›´è‡ªç„¶èªè¨€åŒ–
            # ä¾‹å¦‚ï¼š"æŸ¥è©¢é—œæ–¼ {æœ¨æ§‹é€ } {æ•¸ä½è£½é€ } çš„ {pavilion è¨­è¨ˆæ¡ˆä¾‹} å’Œ {æ§‹é€ ç´°ç¯€}"
            # é€™è£¡çš„ search_query_text å·²ç¶“æ˜¯ LLM ç”Ÿæˆçš„é—œéµè©ï¼Œå¯èƒ½éœ€è¦åŒ…è£ä¸€ä¸‹
            
            # å»ºç«‹ä¸€å€‹æ›´é©åˆ perform_grounded_search çš„æŸ¥è©¢
            # å¯ä»¥ç›´æ¥ç”¨ç”¨æˆ¶è¼¸å…¥ï¼Œæˆ–è€…çµåˆ LLM ç”Ÿæˆçš„é—œéµè©
            grounded_search_query = (
                f"é‡å°ç”¨æˆ¶çš„å»ºç¯‰è¨­è¨ˆéœ€æ±‚ '{user_input}'ï¼Œ"
                f"å°‹æ‰¾ç›¸é—œçš„æ¡ˆä¾‹ã€æ§‹é€ ç´°ç¯€ã€è£½é€ å·¥æ³•ï¼ˆç‰¹åˆ¥æ˜¯æœ¨æ§‹é€ ã€æ•¸ä½è£½é€ ã€å‚³çµ±è£½é€ å·¥æ³•ã€æœ¨çµæ§‹ï¼‰ã€"
                f"æ¸›ç¢³èˆ‡å¾ªç’°æ°¸çºŒæ€§ç­–ç•¥ã€æŠ€è¡“ç´°ç¯€åŠç ”ç©¶ç†è«–ã€‚"
                f"ç”Ÿæˆçš„é—œéµè©åƒè€ƒï¼š{search_query_text}"
            )
            print(f"Grounded Search æŸ¥è©¢: {grounded_search_query}")

            search_tool_output = perform_grounded_search({"query": grounded_search_query})
            
            if isinstance(search_tool_output, dict):
                grounded_search_results_text = search_tool_output.get("text_content", "")
                # å¦‚æœéœ€è¦è™•ç†åœ–ç‰‡:
                # returned_images = search_tool_output.get("images", [])
                # for img_info in returned_images:
                #     # è™•ç†åœ–ç‰‡é‚è¼¯...
                #     pass
            elif isinstance(search_tool_output, str): # å‘ä¸‹å…¼å®¹ï¼Œå¦‚æœå·¥å…·ç›´æ¥è¿”å›å­—ç¬¦ä¸²
                grounded_search_results_text = search_tool_output
            
            print("perform_grounded_search æª¢ç´¢çµæœ (æ–‡æœ¬éƒ¨åˆ†)ï¼š", grounded_search_results_text)
        except Exception as e:
            print(f"âš ï¸ perform_grounded_search èª¿ç”¨å¤±æ•—: {e}")
            grounded_search_results_text = "Grounded Search å·¥å…·æª¢ç´¢å¤±æ•—ã€‚"

        # åˆä½µå…©å€‹ RAG å·¥å…·çš„çµæœ
        # å¯ä»¥ç°¡å–®æ‹¼æ¥ï¼Œæˆ–è€…è®“ LLM ç¨å¾Œåœ¨ç¸½çµæ™‚è‡ªè¡Œåˆ¤æ–·é‡è¦æ€§
        combined_rag_info = f"å‚³çµ±çŸ¥è­˜åº«æª¢ç´¢çµæœ:\n{arch_rag_results}\n\nç¶²è·¯èˆ‡æ–‡ç»æœç´¢çµæœ:\n{grounded_search_results_text}"
        
        # æ›´æ–° summary prompt ä»¥åŒ…å«åˆä½µå¾Œçš„ RAG ä¿¡æ¯
        summary_input_content = active_config.question_task_summary_prompt_template.format(
            user_input=user_input,
            rag_msg=combined_rag_info, # ä½¿ç”¨åˆä½µå¾Œçš„è³‡è¨Š
            llm_output_language=active_language
        )
        summary_msg = current_llm.invoke([SystemMessage(content=summary_input_content)])
        self.state["design_summary"] = f"ç”¨æˆ¶éœ€æ±‚:\n{user_input}\n\nè¨­è¨ˆç›®æ¨™åˆæ­¥ç¸½çµèˆ‡åˆ†æ (åŸºæ–¼æª¢ç´¢è³‡è¨Š):\n{summary_msg.content}"
        print("âœ… è¨­è¨ˆç›®æ¨™ç¸½çµå®Œæˆï¼")
        print(self.state["design_summary"])
        return {          
            "è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½": self.state["è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½"], 
            "design_summary": self.state["design_summary"]
            }

# å ´åœ°åˆ†æä»»å‹™ï¼šè®€å– JSON ä¸¦åˆ†æåŸºåœ°è³‡è¨Šï¼ŒåŒæ™‚é€é LLM å‘¼å«é€²ä¸€æ­¥è§£è®€è³‡æ–™ OK
class SiteAnalysisTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        print("DEBUG: SiteAnalysisTask.run CALLED")
        if state is not None:
            self.state = state
        
        active_config = ensure_graph_overall_config(config)
        print(f"DEBUG SiteAnalysisTask: Processed active_config.run_site_analysis: {active_config.run_site_analysis}")

        if not active_config.run_site_analysis:
            skip_message = "ç”¨æˆ¶é…ç½®è¦æ±‚è·³éåŸºåœ°åˆ†æ (run_site_analysis=False)ã€‚"
            print(f"â„¹ï¸ {skip_message}")
            self.state["site_analysis"] = skip_message
            self.state["analysis_img"] = "ç„¡åœ–ç‰‡ (åŸºåœ°åˆ†æå·²è·³é)"
            return {
                "site_analysis": self.state["site_analysis"],
                "analysis_img": self.state["analysis_img"],
            }

        current_llm = active_config.llm_config.get_llm()
        active_language = active_config.llm_output_language
        
        # --- æ­¥é©Ÿ 1: åˆ†æ user input çš„å…§å®¹ç²å–ç¶“ç·¯åº¦å’Œåœ°é» ---
        user_design_input = ""
        if self.state.get("è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½") and isinstance(self.state["è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½"], Sequence) and len(self.state["è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½"]) > 0:
            user_design_input = self.state["è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½"][0].content
            print(f"â„¹ï¸ SiteAnalysisTask [Step 1]: å–å¾—ç”¨æˆ¶è¨­è¨ˆéœ€æ±‚å…§å®¹: '{user_design_input[:100]}...'")
        else:
            error_message = "âš ï¸ SiteAnalysisTask [Step 1]: æœªèƒ½åœ¨ç‹€æ…‹ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ç”¨æˆ¶è¨­è¨ˆéœ€æ±‚è¼¸å…¥ã€‚ç„¡æ³•æå–åŸºåœ°è³‡è¨Šã€‚"
            print(error_message)
            self.state["site_analysis"] = error_message
            self.state["analysis_img"] = "ç„¡åœ–ç‰‡ (ç¼ºå°‘ç”¨æˆ¶è¨­è¨ˆéœ€æ±‚)"
            return {"site_analysis": self.state["site_analysis"], "analysis_img": self.state["analysis_img"]}

        region = "æœªçŸ¥"
        geo_location = "æœªçŸ¥"
        try:
            extract_prompt_content = active_config.site_analysis_extract_site_info_prompt_template.format(
                user_design_input=user_design_input,
                llm_output_language=active_language
            )
            print(f"DEBUG SiteAnalysisTask [Step 1]: Prompt for extracting site info: {extract_prompt_content}")
            site_info_msg = current_llm.invoke([SystemMessage(content=extract_prompt_content)])
            site_info_json_str = site_info_msg.content.strip()
            print(f"â„¹ï¸ SiteAnalysisTask [Step 1]: LLM æå–çš„åŸºåœ°è³‡è¨Š (åŸå§‹å­—ä¸²): {site_info_json_str}")
            
            if site_info_json_str.startswith("```json"):
                site_info_json_str = site_info_json_str[7:]
            if site_info_json_str.endswith("```"):
                site_info_json_str = site_info_json_str[:-3]
            site_info_json_str = site_info_json_str.strip()

            parsed_site_info = json.loads(site_info_json_str)
            region = parsed_site_info.get("region", "æœªçŸ¥")
            geo_location = parsed_site_info.get("geo_location", "æœªçŸ¥")
            print(f"âœ… SiteAnalysisTask [Step 1]: è§£æå¾Œçš„ Region: {region}, GeoLocation: {geo_location}")

        except AttributeError as e:
            # æ•ç² active_config.site_analysis_extract_site_info_prompt_template ä¸å­˜åœ¨çš„éŒ¯èª¤
            error_message = f"âš ï¸ SiteAnalysisTask [Step 1]: æå–åŸºåœ°è³‡è¨Šæ™‚ç™¼ç”Ÿ AttributeError (å¯èƒ½ Prompt æ¨¡æ¿æœªåœ¨ T_config.py ä¸­æ­£ç¢ºå®šç¾©æˆ–åŠ è¼‰): {e}"
            print(error_message)
            self.state["site_analysis"] = error_message
            self.state["analysis_img"] = "ç„¡åœ–ç‰‡ (é…ç½®éŒ¯èª¤)"
            return {"site_analysis": self.state["site_analysis"], "analysis_img": self.state["analysis_img"]}
        except json.JSONDecodeError as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 1]: è§£æLLMæå–çš„åŸºåœ°è³‡è¨ŠJSONå¤±æ•—: {e}. åŸå§‹å­—ä¸²: '{site_info_json_str}'")
        except Exception as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 1]: æå–åŸºåœ°è³‡è¨Šæ™‚ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤: {e}")

        if region == "æœªçŸ¥" and geo_location == "æœªçŸ¥":
            print("âš ï¸ SiteAnalysisTask [Step 1]: æœªèƒ½å¾ç”¨æˆ¶è¼¸å…¥ä¸­æ˜ç¢ºæå–æœ‰æ•ˆçš„åœ°å€æˆ–åœ°ç†ä½ç½®è³‡è¨Šã€‚å¾ŒçºŒåˆ†æå¯èƒ½å—å½±éŸ¿ï¼Œä½†å°‡ç¹¼çºŒå˜—è©¦ã€‚")

        # --- æ­¥é©Ÿ 2: ä½¿ç”¨ä»¥ä¸Šè³‡è¨ŠåŠåœ–ç‰‡ "D:/MA system/LangGraph/input/2D/map.png" é€²è¡Œåœ–ç‰‡è¾¨è­˜åˆ†æ ---
        # æ³¨æ„ï¼šæª”åå·²å¾ base_map.png æ”¹ç‚º map.png
        base_map_path_str = "./input/2D/map.png" 
        
        print(f"DEBUG SiteAnalysisTask [Step 2]: Checking for image file at: '{base_map_path_str}'")
        if not os.path.exists(base_map_path_str):
            error_message = f"âŒ SiteAnalysisTask [Step 2]: åœ–ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°æ–¼ '{base_map_path_str}'ã€‚ç„¡æ³•é€²è¡Œåœ–ç‰‡è¾¨è­˜ã€‚"
            print(error_message)
            self.state["site_analysis"] = self.state.get("site_analysis", "") + " " + error_message # é™„åŠ éŒ¯èª¤
            self.state["analysis_img"] = "ç¼ºå°‘åœ–ç‰‡æ–‡ä»¶ (map.png)ï¼Œç„¡æ³•é€²è¡Œè¾¨è­˜ã€‚"
            # å³ä½¿åœ–ç‰‡ç¼ºå¤±ï¼Œæˆ‘å€‘ä»ç„¶å¯ä»¥å˜—è©¦åŸºæ–¼æ–‡æœ¬çš„RAGå’Œåˆ†æï¼Œæ‰€ä»¥ä¸ä¸€å®šç«‹å³è¿”å›
            # ä½†å¦‚æœåœ–ç‰‡æ˜¯æ ¸å¿ƒï¼Œå‰‡æ‡‰è©²è¿”å›
            # ç‚ºäº†ç¬¦åˆæµç¨‹ï¼Œå¦‚æœåœ–ç‰‡è¾¨è­˜æ˜¯å¿…è¦çš„ï¼Œé€™è£¡æ‡‰è©²è¿”å›
            return {"site_analysis": self.state["site_analysis"], "analysis_img": self.state["analysis_img"]}
        
        print(f"âœ… SiteAnalysisTask [Step 2]: åœ–ç‰‡æ–‡ä»¶ '{base_map_path_str}' å·²æ‰¾åˆ°ã€‚")
        
        initial_img_analysis_content = "åœ–ç‰‡è¾¨è­˜å¤±æ•—æˆ–æœªåŸ·è¡Œã€‚" # é è¨­å€¼
        try:
            img_rec_prompt_content = active_config.site_analysis_img_recognition_prompt_template.format(
                region=region, 
                geo_location=geo_location, 
                llm_output_language=active_language
            )
            print(f"DEBUG SiteAnalysisTask [Step 2]: Prompt for image recognition: {img_rec_prompt_content}")
            initial_img_analysis_content = img_recognition.invoke({ 
                "image_paths": base_map_path_str, # ä½¿ç”¨çµ•å°è·¯å¾‘
                "prompt": img_rec_prompt_content,
            })
            self.state["analysis_img"] = initial_img_analysis_content 
            print(f"âœ… SiteAnalysisTask [Step 2]: åˆæ­¥åœ–ç‰‡è¾¨è­˜çµæœ: '{str(initial_img_analysis_content)[:200]}...'")
        except Exception as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 2]: åŸ·è¡Œåœ–ç‰‡è¾¨è­˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            self.state["analysis_img"] = f"åœ–ç‰‡è¾¨è­˜å¤±æ•—: {e}"
            # æ ¹æ“šéœ€æ±‚æ±ºå®šæ˜¯å¦åœ¨æ­¤è™•è¿”å›

        # --- æ­¥é©Ÿ 3: æ ¹æ“šè¾¨è­˜çµæœç”Ÿæˆé—œéµå­— ---
        site_rag_keywords = "æœªçŸ¥é—œéµå­—" # é è¨­å€¼
        try:
            rag_keywords_gen_prompt = active_config.site_analysis_rag_keywords_prompt_template.format(
                region=region,
                geo_location=geo_location,
                initial_img_analysis_summary=str(initial_img_analysis_content), #ç¢ºä¿æ˜¯å­—ç¬¦ä¸² 
                llm_output_language=active_language
            )
            print(f"DEBUG SiteAnalysisTask [Step 3]: Prompt for RAG keyword generation: {rag_keywords_gen_prompt}")
            keywords_msg = current_llm.invoke([SystemMessage(content=rag_keywords_gen_prompt)])
            site_rag_keywords = keywords_msg.content.strip()
            print(f"âœ… SiteAnalysisTask [Step 3]: ç”Ÿæˆçš„åŸºåœ°åˆ†æRAGé—œéµå­—: {site_rag_keywords}")
        except Exception as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 3]: ç”ŸæˆRAGé—œéµå­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # --- æ­¥é©Ÿ 4: é€²è¡ŒARCH_rag_toolä»¥åŠperform_grounded_search ---
        arch_rag_site_results = "ARCH RAG å·¥å…·æª¢ç´¢ç„¡çµæœæˆ–å¤±æ•—ã€‚"
        try:
            print(f"DEBUG SiteAnalysisTask [Step 4]: Invoking ARCH_rag_tool with keywords: {site_rag_keywords}")
            arch_rag_msg_content = ARCH_rag_tool.invoke(site_rag_keywords)
            if isinstance(arch_rag_msg_content, str):
                arch_rag_site_results = arch_rag_msg_content
            print(f"âœ… SiteAnalysisTask [Step 4]: ARCH_rag_tool åŸºåœ°è³‡è¨Šæª¢ç´¢çµæœ: '{arch_rag_site_results[:200]}...'")
        except Exception as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 4]: ARCH_rag_tool èª¿ç”¨å¤±æ•—: {e}")

        grounded_search_site_results_text = "Grounded Search å·¥å…·æª¢ç´¢ç„¡çµæœæˆ–å¤±æ•—ã€‚"
        try:
            grounded_search_site_query = (
                f"æŸ¥è©¢é—œæ–¼åœ°é» '{region}' (ç¶“ç·¯åº¦: {geo_location}) çš„è©³ç´°èƒŒæ™¯è³‡æ–™ï¼Œ"
                f"åŒ…æ‹¬éƒ½å¸‚è¨ˆç•«è¦ç¯„ã€å»ºç¯‰æ³•è¦ã€æ°£å€™è³‡æ–™ã€æ—¥ç…§ã€äººæ–‡æ­·å²ã€æ°´æ–‡åœ°è³ªã€å‘¨é‚Šç’°å¢ƒç­‰ã€‚"
                f"åƒè€ƒé—œéµå­—ï¼š{site_rag_keywords}"
            )
            print(f"DEBUG SiteAnalysisTask [Step 4]: Invoking perform_grounded_search with query: {grounded_search_site_query}")
            search_tool_output = perform_grounded_search({"query": grounded_search_site_query})
            if isinstance(search_tool_output, dict):
                grounded_search_site_results_text = search_tool_output.get("text_content", "")
            elif isinstance(search_tool_output, str):
                grounded_search_site_results_text = search_tool_output
            print(f"âœ… SiteAnalysisTask [Step 4]: perform_grounded_search æª¢ç´¢çµæœ (æ–‡æœ¬éƒ¨åˆ†): '{grounded_search_site_results_text[:200]}...'")
        except Exception as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 4]: perform_grounded_search èª¿ç”¨å¤±æ•—: {e}")

        combined_site_rag_info = (
            f"å…§éƒ¨çŸ¥è­˜åº«æª¢ç´¢ (ARCH_rag_tool):\n{arch_rag_site_results}\n\n"
            f"ç¶²è·¯èˆ‡æ–‡ç»æœç´¢ (perform_grounded_search):\n{grounded_search_site_results_text}"
        )

        # --- æ­¥é©Ÿ 5: åŸ·è¡Œæœ€çµ‚çš„ LLM åˆ†æï¼Œæ•´åˆåœ–ç‰‡è¾¨è­˜å’Œæ‰€æœ‰ RAG è³‡è¨Š ---
        final_analysis_result_content = "æœ€çµ‚åŸºåœ°åˆ†æå ±å‘Šç”Ÿæˆå¤±æ•—ã€‚"
        try:
            llm_analysis_prompt_content = active_config.site_analysis_llm_prompt_template.format(
                region=region,
                geo_location=geo_location,
                analysis_img=str(initial_img_analysis_content), # ç¢ºä¿æ˜¯å­—ç¬¦ä¸²
                rag_supplementary_info=combined_site_rag_info,
                llm_output_language=active_language
            )
            print(f"DEBUG SiteAnalysisTask [Step 5]: Prompt for final LLM analysis: {llm_analysis_prompt_content}")
            analysis_result_msg = current_llm.invoke([SystemMessage(content=llm_analysis_prompt_content)])
            final_analysis_result_content = analysis_result_msg.content
            self.state["site_analysis"] = final_analysis_result_content
            print(f"âœ… SiteAnalysisTask [Step 5]: æœ€çµ‚çš„ã€æ•´åˆäº†RAGçš„åˆ†æå ±å‘Š: '{final_analysis_result_content[:200]}...'")
        except Exception as e:
            print(f"âš ï¸ SiteAnalysisTask [Step 5]: ç”Ÿæˆæœ€çµ‚åˆ†æå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            self.state["site_analysis"] = f"æœ€çµ‚åŸºåœ°åˆ†æå ±å‘Šç”Ÿæˆå¤±æ•—: {e}"

        print("âœ… SiteAnalysisTask.run COMPLETED")
        return {
            "analysis_img": self.state["analysis_img"], # ä¿æŒç‚ºåˆæ­¥åœ–ç‰‡è¾¨è­˜çµæœ
            "site_analysis": self.state["site_analysis"], # æ›´æ–°ç‚ºæœ€çµ‚åˆ†æå ±å‘Š
        }

# è¨­è¨ˆæ–¹æ¡ˆä»»å‹™ OK
class RAGdesignThinking:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state        
        
        active_config = ensure_graph_overall_config(config)
        current_llm = active_config.llm_config.get_llm()
        active_language = active_config.llm_output_language

        design_goal_summary = self.state.get("design_summary", "ç„¡è¨­è¨ˆç›®æ¨™ç¸½çµ") # å¾ QuestionTask ç²å–çš„ç¸½çµ
        analysis_result = self.state.get("site_analysis", "ç„¡åŸºåœ°åˆ†æçµæœ")
        current_round = self.state.get("current_round", 0)
        improvement = self.state.get("GATE_REASON1", "") 

        # æ–°çš„ "keywords_prompt_content" - ç¾åœ¨æ˜¯å¼•å°è¨­è¨ˆæ–¹å‘ï¼Œè€Œä¸æ˜¯ç”Ÿæˆ RAG é—œéµè©
        # å®ƒæœƒåƒè€ƒ design_goal_summary å’Œ analysis_result
        # rag_design_thinking_keywords_prompt_template åœ¨ T_config.py ä¸­ä¹Ÿéœ€è¦æ›´æ–°
        design_directions_prompt_content = active_config.rag_design_thinking_keywords_prompt_template.format(
            design_goal_summary=design_goal_summary, # æ›´æ–°è®Šæ•¸å
            analysis_result=analysis_result,
            llm_output_language=active_language
        )
        response_design_directions_msg = current_llm.invoke([SystemMessage(content=design_directions_prompt_content)])
        # design_directions_text ç¾åœ¨æ˜¯è¨­è¨ˆæ–¹å‘çš„æ–‡æœ¬æè¿°
        design_directions_text = response_design_directions_msg.content.strip()
        print("ç”Ÿæˆçš„è¨­è¨ˆæ–¹å‘æŒ‡å¼•ï¼š", design_directions_text)

        # ä¸å†èª¿ç”¨ ARCH_rag_tool
        # RAG_msg_content ç¾åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨ design_directions_text æˆ–å…¶ä»–ç›¸é—œå…§å®¹
        # å¦‚æœ complete_scheme_prompt_template ä»ç„¶éœ€è¦ rag_msg è®Šæ•¸ï¼Œ
        # æˆ‘å€‘å¯ä»¥å°‡ design_directions_text ä½œç‚º rag_msg å‚³å…¥ï¼Œ
        # æˆ–è€…ä¿®æ”¹ complete_scheme_prompt_template ä»¥æ¥å— design_directions
        
        # å‡è¨­ complete_scheme_prompt_template ä¸­çš„ rag_msg ç¾åœ¨ä»£è¡¨æ›´å»£æ³›çš„åƒè€ƒè³‡æ–™æˆ–è¨­è¨ˆæŒ‡å¼•
        # æˆ‘å€‘å°‡ design_directions_text ç”¨æ–¼æ­¤è™•ï¼Œæˆ–è€…æ‚¨å¯ä»¥é¸æ“‡å…¶ä»–æ›´æœ‰æ„ç¾©çš„å…§å®¹
        # T_config.py ä¸­çš„ rag_design_thinking_complete_scheme_prompt_template çš„æè¿°ä¹Ÿæ‡‰è©²æ›´æ–°
        complete_scheme_prompt_content = active_config.rag_design_thinking_complete_scheme_prompt_template.format(
            design_goal_summary=design_goal_summary, # æ›´æ–°è®Šæ•¸å
            improvement=improvement,
            analysis_result=analysis_result,
            # rag_msg=RAG_msg_content, # åŸä¾†çš„ RAG çµæœ
            design_directions=design_directions_text, # æ–°çš„è¨­è¨ˆæ–¹å‘æŒ‡å¼•
            llm_output_language=active_language
        )
        complete_response_msg = current_llm.invoke([SystemMessage(content=complete_scheme_prompt_content)])
        complete_scheme_content = complete_response_msg.content

        new_scheme_entry = {"round": int(current_round), "proposal": str(complete_scheme_content)}
        existing_advice = self.state.get("design_advice", [])
        if not isinstance(existing_advice, list): 
            existing_advice = []
        updated_advice = custom_add_messages(existing_advice, [new_scheme_entry])
        self.state["design_advice"] = updated_advice

        print("âœ… è¨­è¨ˆå»ºè­°å·²å®Œæˆï¼")
        print(f"æœ€çµ‚è¨­è¨ˆå»ºè­°ï¼š{self.state['design_advice']}")
        return {"design_advice": self.state["design_advice"]}

# GATE æª¢æŸ¥æ–¹æ¡ˆï¼ˆè«‹å›ç­”ï¼šæœ‰/æ²’æœ‰ï¼‰ OK
class GateCheck1:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state
        
        active_config = ensure_graph_overall_config(config)
        current_llm = active_config.llm_config.get_llm()
        active_language = active_config.llm_output_language

        design_advice_raw = self.state.get("design_advice", [])
        design_summary = self.state.get("design_summary", "ç„¡ç›®æ¨™")

        def ensure_dict(item):
            if isinstance(item, dict):
                return item
            if isinstance(item, str):
                try:
                    # Try to parse if it's a JSON string representing a dict
                    parsed_item = json.loads(item)
                    if isinstance(parsed_item, dict):
                        return parsed_item
                except json.JSONDecodeError:
                    # If it's not a JSON string, and we expect dicts, this might be an issue.
                    # For now, we'll assume items are either dicts or JSON strings of dicts.
                    print(f"âš ï¸ ç„¡æ³•è§£æé …ç›®ç‚ºå­—å…¸: {item}")
                    return None # Or handle as per logic, e.g., wrap in a default dict structure
            print(f"âš ï¸ é …ç›®ä¸æ˜¯å­—å…¸æˆ–JSONå­—ä¸²: {item}")
            return None

        design_advice_list = []
        if isinstance(design_advice_raw, list):
            for item in design_advice_raw:
                d = ensure_dict(item)
                if d is not None:
                    design_advice_list.append(d)
        else:
             # Handle case where design_advice_raw is not a list (e.g. initial empty string)
            print(f"âš ï¸ design_advice_raw ä¸æ˜¯åˆ—è¡¨: {design_advice_raw}")

        # ä½¿ç”¨æ–°æ¢ä»¶ï¼šæ²’æœ‰ "state" éµçš„å°è±¡ä½œç‚º current proposals
        current_proposals = [
            advice for advice in design_advice_list if isinstance(advice, dict) and "state" not in advice
        ]
        historical_proposals = [
            advice for advice in design_advice_list if isinstance(advice, dict) and "state" in advice
        ]

        if not current_proposals:
            print("âš ï¸ ç•¶å‰ç„¡ç¬¦åˆæ¢ä»¶çš„è¨­è¨ˆå»ºè­°æ–¹æ¡ˆï¼ˆæœªæ‰¾åˆ°ä¸å« state éµçš„å°è±¡ï¼‰ã€‚")
            self.state["GATE1"] = "æ²’æœ‰"
            self.state["GATE_REASON1"] = "ç•¶å‰ç„¡ç¬¦åˆæ¢ä»¶çš„è¨­è¨ˆå»ºè­°æ–¹æ¡ˆï¼ˆæœªæ‰¾åˆ°ä¸å« state éµçš„å°è±¡ï¼‰"
            # Ensure design_advice remains a list
            if not isinstance(self.state.get("design_advice"), list):
                self.state["design_advice"] = []
            return {"GATE1": self.state["GATE1"], "GATE_REASON1": self.state["GATE_REASON1"], "design_advice": self.state["design_advice"]}

        # æ ¼å¼åŒ–è¼¸å‡ºä»¥ä¾› prompt ä½¿ç”¨
        formatted_current = json.dumps(current_proposals, ensure_ascii=False, indent=2)
        formatted_previous = json.dumps(historical_proposals, ensure_ascii=False, indent=2)

        gate1_prompt_content = active_config.gate_check1_prompt_template.format(
            design_summary=design_summary,
            formatted_current=formatted_current,
            formatted_previous=formatted_previous,
            llm_output_language=active_language
        )

        llm_response_msg = current_llm.invoke([SystemMessage(content=gate1_prompt_content)])
        response_content = llm_response_msg.content
        response_lines = [line.strip() for line in response_content.splitlines() if line.strip()]
        
        evaluation_result = "æ²’æœ‰" # Default
        reason = "LLM å›è¦†æ ¼å¼ä¸ç¬¦æˆ–ç‚ºç©º" # Default

        if not response_lines:
            print("âš ï¸ LLM å›è¦†ç‚ºç©ºï¼Œè«‹æª¢æŸ¥æç¤ºæ ¼å¼ã€‚")
        else:
            evaluation_result = response_lines[0]
            reason = response_lines[1] if len(response_lines) > 1 else "ç©º"

        # åˆ¤æ–·è©•ä¼°çµæœï¼Œæ±ºå®š state éµå€¼
        state_value = True if evaluation_result == "æœ‰" else False

        # ç‚ºæ¯å€‹ç•¶å‰æ–¹æ¡ˆå­—å…¸æ–°å¢ state éµ
        for advice_dict_item in current_proposals: # Renamed to avoid conflict
            if isinstance(advice_dict_item, dict): # Ensure it's a dict before assigning
                 advice_dict_item["state"] = state_value

        # è¦†è“‹ design_advice: å°‡ historical_proposals èˆ‡æ›´æ–°å¾Œçš„ current_proposals åˆä½µ
        # existing_advice was already processed into design_advice_list.
        # We need to reconstruct design_advice based on historical and updated current.
        updated_design_advice_list = historical_proposals + current_proposals
        self.state["design_advice"] = updated_design_advice_list

        # æœ€å¾Œ self.state["GATE1"] åƒ…è¿”å›è©•åˆ¤çµæœï¼ˆ"æœ‰" æˆ– "æ²¡æœ‰"ï¼‰
        self.state["GATE1"] = evaluation_result
        self.state["GATE_REASON1"] = reason

        print(f"ã€GateCheckã€‘å·²æ”¶åˆ°è©•å¯©çµæœï¼š{evaluation_result}ï¼ŒåŸå› ï¼š{reason}")
        return {"GATE1": self.state["GATE1"], "GATE_REASON1": self.state["GATE_REASON1"], "design_advice": self.state["design_advice"]}
        

# å¤–æ®¼ Prompt ç”Ÿæˆï¼šå‘¼å« LLMï¼ˆä½¿ç”¨ prompt ç”Ÿæˆå·¥å…·ï¼‰æ ¹æ“šåŸºåœ°è³‡è¨Šèˆ‡èåˆåœ–ç”Ÿæˆè¨­è¨ˆ prompt OK
class OuterShellPromptTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state

        active_config = ensure_graph_overall_config(config)
        current_llm = active_config.llm_config.get_llm()
        active_language = active_config.llm_output_language

        current_round = self.state.get("current_round", 0)
        design_advice_list_raw = self.state.get("design_advice", [])
        
        # æ•´åˆä¾†è‡ª GateCheck1 å’Œ GateCheck2 çš„æ”¹é€²å»ºè­°
        improvement_from_gate1 = self.state.get("GATE_REASON1", "")
        improvement_from_gate2 = self.state.get("GATE_REASON2", "")

        improvement_texts = []
        if improvement_from_gate1:
            improvement_texts.append(f"å°æ–‡å­—è¨­è¨ˆæ–¹æ¡ˆçš„æ”¹é€²å»ºè­°: {improvement_from_gate1}")
        if improvement_from_gate2:
            improvement_texts.append(f"å°ä¸Šä¸€æ‰¹ç”Ÿæˆåœ–åƒçš„æ”¹é€²å»ºè­°: {improvement_from_gate2}")

        improvement = "\n".join(improvement_texts) if improvement_texts else "ç„¡"
        
        design_advice_list = []
        if isinstance(design_advice_list_raw, list):
            design_advice_list = [item for item in design_advice_list_raw if isinstance(item, dict)]


        # éæ¿¾å‡ºç•¶å‰è¼ªæ¬¡ä¸” state ç‚º True çš„è¨­è¨ˆæ–¹æ¡ˆï¼ˆå¿…é ˆæ˜¯å­—å…¸æ ¼å¼ï¼‰
        valid_advices = [
            advice for advice in design_advice_list
            if advice.get("round") == current_round and advice.get("state") == True
        ]
        
        advice_text = "ç„¡ç›®æ¨™"
        if valid_advices:
            selected_advice = valid_advices[0]
            advice_text = selected_advice.get("proposal", "ç„¡ç›®æ¨™")
        else:
            print(f"âš ï¸ OuterShellPromptTask: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} ä¸” state ç‚º True çš„æœ‰æ•ˆè¨­è¨ˆå»ºè­°ã€‚")


        gpt_prompt_content = active_config.outer_shell_gpt_prompt_template.format(
            advice_text=advice_text,
            improvement=improvement,
            llm_output_language=active_language
        )

        gpt_output_msg = current_llm.invoke([SystemMessage(content=gpt_prompt_content)])
        final_prompt_text = gpt_output_msg.content if hasattr(gpt_output_msg, "content") else "âŒ GPT ç”Ÿæˆå¤±æ•—"

        lora_guidance_prompt_content = active_config.outer_shell_lora_prompt_template.format(
            final_prompt=final_prompt_text,
            llm_output_language=active_language
        )

        gpt_output2_msg = current_llm.invoke([SystemMessage(content=lora_guidance_prompt_content)])
        lora_value_str = gpt_output2_msg.content.strip() if hasattr(gpt_output2_msg, "content") else "0.5"
        
        try:
            # Attempt to convert to float, ensure it's a number
            float(lora_value_str)
        except ValueError:
            print(f"âš ï¸ LoRAæ¬Šé‡ç”Ÿæˆéæ•¸å­— '{lora_value_str}', ä½¿ç”¨é è¨­å€¼ 0.5")
            lora_value_str = "0.5"


        new_prompt_entry = {"round":int(current_round),"prompt": str(final_prompt_text),"lora":str(lora_value_str)}
        
        existing_prompts_list = self.state.get("outer_prompt", [])
        if not isinstance(existing_prompts_list, list):
            existing_prompts_list = []
        
        updated_prompts = existing_prompts_list + [new_prompt_entry]
        self.state["outer_prompt"] = updated_prompts


        print("âœ… ç”Ÿæˆå¤–æ®¼ Prompt å®Œæˆï¼")
        print(f"ğŸ“Œ å¤–æ®¼ Prompt: {final_prompt_text}, LoRA: {lora_value_str}")
        return {"outer_prompt": self.state["outer_prompt"]}

# æ–¹æ¡ˆæƒ…å¢ƒç”Ÿæˆï¼šå‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡ç”Ÿæˆå·¥å…·ï¼‰æ ¹æ“šå¤–æ®¼ prompt èˆ‡èåˆåœ–ç”Ÿæˆæœªä¾†æƒ…å¢ƒåœ– OK
class CaseScenarioGenerationTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state

        active_config = ensure_graph_overall_config(config)
        current_round = self.state.get("current_round", 0)
        outer_prompt_list_raw = self.state.get("outer_prompt", [])
        
        # å¾é…ç½®ä¸­è®€å–è¦ç”Ÿæˆçš„åœ–ç‰‡æ•¸é‡
        num_images_to_generate = active_config.case_scenario_image_count

        outer_prompt_list = []
        if isinstance(outer_prompt_list_raw, list):
            outer_prompt_list = [item for item in outer_prompt_list_raw if isinstance(item, dict)]

        # ç¯©é¸å‡ºç•¶å‰è¼ªæ¬¡ä¸”ä¸å« "state" éµçš„å­—å…¸ (i.e., the latest one for this round)
        current_round_prompts = [
            item for item in outer_prompt_list 
            if item.get("round") == current_round and "state" not in item
        ]

        prompt_to_use = ""
        lora_to_use = "0.5" # Default

        if current_round_prompts:
            latest_prompt_entry = current_round_prompts[-1] # Get the last one for the current round
            prompt_to_use = latest_prompt_entry.get("prompt", "")
            lora_to_use = latest_prompt_entry.get("lora", "0.5")
        else:
            print(f"âš ï¸ CaseScenarioGenerationTask: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} çš„å¤–æ®¼ promptã€‚")
            if outer_prompt_list:
                latest_prompt_entry = outer_prompt_list[-1]
                prompt_to_use = latest_prompt_entry.get("prompt", "")
                lora_to_use = latest_prompt_entry.get("lora", "0.5")
                print(f"â†ªï¸  ä½¿ç”¨æœ€å¾Œä¸€å€‹å¯ç”¨çš„ prompt: {prompt_to_use}")


        if not prompt_to_use:
            print("âŒ CaseScenarioGenerationTask: ç„¡å¯ç”¨ prompt ç”Ÿæˆåœ–ç‰‡ã€‚")
            if not isinstance(self.state.get("case_image"), list):
                self.state["case_image"] = []
            # è¿”å›ä¸€å€‹è¡¨ç¤ºå¤±æ•—çš„æ¢ç›®æˆ–ä¿æŒç‚ºç©ºåˆ—è¡¨
            self.state["case_image"] = custom_add_messages(
                self.state.get("case_image", []), 
                [{"round": current_round, "id_in_round": 1, "filename": "ç„¡Promptç”Ÿæˆå¤±æ•—", "image_url": "æœªç”Ÿæˆ", "path": "ç„¡", "description": "ç„¡å¯ç”¨ prompt ç”Ÿæˆåœ–ç‰‡ã€‚"}]
            )
            return {"case_image": self.state["case_image"]}

        generated_image_infos = []
        # èˆ‡ GateCheck2 å’Œ UnifiedImageGenerationTask çµ±ä¸€å¿«å–ç›®éŒ„
        render_cache_dir = os.path.join(os.getcwd(), "output", "render_cache")
        os.makedirs(render_cache_dir, exist_ok=True)

        # èª¿ç”¨ä¸€æ¬¡ case_render_image å·¥å…·ï¼Œè®“å®ƒæ ¹æ“š num_images_to_generate ç”Ÿæˆæ‰€æœ‰åœ–ç‰‡
        all_generated_filenames_str = case_render_image.invoke({
            "current_round": current_round,
            "outer_prompt": prompt_to_use,
            "i": num_images_to_generate, # ç”Ÿæˆç¸½æ•¸
            "strength": lora_to_use
        })

        generated_filenames_list = []
        if all_generated_filenames_str and isinstance(all_generated_filenames_str, str):
            # å·¥å…·è¿”å›é€—è™Ÿåˆ†éš”çš„æª”å
            generated_filenames_list = [fn.strip() for fn in all_generated_filenames_str.split(',') if fn.strip()]
        
        if not generated_filenames_list:
            print(f"âš ï¸ åœ–ç‰‡ç”Ÿæˆå·¥å…·æœªè¿”å›ä»»ä½•æœ‰æ•ˆæ–‡ä»¶å (å·¥å…·è¿”å›: {all_generated_filenames_str})ã€‚")
            generated_image_infos.append({
                "round": current_round,
                "id_in_round": 1, # æ¨™è¨˜ä¸€å€‹éŒ¯èª¤æ¢ç›®
                "filename": "å·¥å…·æœªè¿”å›æ–‡ä»¶å",
                "image_url": "æœªç”Ÿæˆ",
                "path": "ç„¡",
                "description": f"åœ–ç‰‡ç”Ÿæˆå·¥å…·æœªè¿”å›ä»»ä½•æœ‰æ•ˆæ–‡ä»¶å (å·¥å…·è¿”å›: {all_generated_filenames_str})ã€‚"
            })
        else:
            print(f"ğŸ–¼ï¸ å·¥å…·è¿”å›äº† {len(generated_filenames_list)} å€‹æ–‡ä»¶å: {generated_filenames_list}")
            for idx, generated_filename in enumerate(generated_filenames_list):
                image_url = "æœªç”Ÿæˆ"
                path_for_state = "è™•ç†å¤±æ•—æˆ–æ–‡ä»¶æœªæ‰¾åˆ°"
                description = f"Round {current_round}, Image {idx+1}/{num_images_to_generate}."
                # ç¢ºä¿æ–‡ä»¶åæ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²ä¸”ä¸æ˜¯éŒ¯èª¤æ¨™è¨˜
                if generated_filename and isinstance(generated_filename, str) and \
                   generated_filename not in ["ç”Ÿæˆå¤±æ•—", "æ–‡ä»¶æœªæ‰¾åˆ°", "å·¥å…·æœªè¿”å›æ–‡ä»¶å"]: # æ·»åŠ æ›´å¤šå¯èƒ½çš„éŒ¯èª¤æ¨™è¨˜
                    
                    image_path_in_cache = os.path.join(render_cache_dir, generated_filename)
                    if os.path.exists(image_path_in_cache):
                        try:
                            with open(image_path_in_cache, "rb") as image_file:
                                encoded_image = base64.b64encode(image_file.read()).decode('utf-8')
                            image_url = f"data:image/png;base64,{encoded_image}" # å‡è¨­ç¸½æ˜¯PNGï¼Œæˆ–å¾æ–‡ä»¶åæ¨æ–·
                            path_for_state = image_path_in_cache
                            description += f" Successfully processed file '{generated_filename}'."
                            print(f"âœ… æˆåŠŸè™•ç†åœ–ç‰‡: {generated_filename}")
                        except Exception as e:
                            print(f"âš ï¸ ç„¡æ³•è®€å–æˆ–ç·¨ç¢¼åœ–ç‰‡æ–‡ä»¶ {generated_filename}: {e}")
                            image_url = "è®€å–æˆ–ç·¨ç¢¼å¤±æ•—"
                            path_for_state = image_path_in_cache # è·¯å¾‘å­˜åœ¨ä½†è™•ç†å¤±æ•—
                            description += f" Failed to read or encode file '{generated_filename}': {e}."
                    else:
                        print(f"âš ï¸ å·¥å…·è²ç¨±ç”Ÿæˆäº†åœ–ç‰‡ '{generated_filename}' ä½†åœ¨è·¯å¾‘ '{image_path_in_cache}' æœªæ‰¾åˆ°ã€‚")
                        image_url = "æ–‡ä»¶æ–¼å¿«å–æœªæ‰¾åˆ°"
                        path_for_state = image_path_in_cache # è¨˜éŒ„ä¸‹å˜—è©¦çš„è·¯å¾‘
                        description += f" File '{generated_filename}' not found at expected path."
                else:
                    print(f"âš ï¸ å·¥å…·è¿”å›äº†ç„¡æ•ˆæˆ–éŒ¯èª¤æ¨™è¨˜çš„æª”å: '{generated_filename}'")
                    image_url = "ç”Ÿæˆå¤±æ•—ï¼ˆå·¥å…·å ±å‘Šï¼‰"
                    description += f" Tool returned an invalid or error filename: '{generated_filename}'."

                generated_image_infos.append({
                    "round": current_round,
                    "id_in_round": idx + 1, # ä½¿ç”¨åˆ—è¡¨ç´¢å¼•+1ä½œç‚ºè¼ªæ¬¡å…§IDï¼Œèˆ‡å·¥å…·å…§éƒ¨è¿­ä»£å°æ‡‰
                    "filename": generated_filename,
                    "image_url": image_url,
                    "path": path_for_state,
                    "description": description
                })

        existing_images_list = self.state.get("case_image", [])
        if not isinstance(existing_images_list, list):
            existing_images_list = []
        
        updated_images_list = existing_images_list + generated_image_infos # ç›´æ¥æ·»åŠ åˆ—è¡¨
        self.state["case_image"] = updated_images_list

        print(f"âœ… æ–¹æ¡ˆæƒ…å¢ƒåœ–è™•ç†å®Œæˆï¼Œå…±è™•ç† {len(generated_image_infos)} æ¢åœ–ç‰‡è³‡è¨Šã€‚")
        print(f"è©³ç´°åœ–ç‰‡è³‡è¨Š: {generated_image_infos}")
        return {"case_image": self.state["case_image"]}    

# class UnifiedImageGenerationTask:
#     def __init__(self, state: GlobalState):
#         self.state = state

#     def run(self, state: GlobalState, config: GraphOverallConfig | dict):
#         if state is not None:
#             self.state = state

#         active_config = ensure_graph_overall_config(config)
#         current_llm = active_config.llm_config.get_llm() # ç”¨æ–¼æ ¼å¼åŒ– prompt
#         active_language = active_config.llm_output_language
        
#         current_round = self.state.get("current_round", 0)
#         design_advice_list_raw = self.state.get("design_advice", [])
#         improvement = self.state.get("GATE_REASON1", "") 
#         num_tool_calls = active_config.case_scenario_image_count 

#         design_advice_list = []
#         if isinstance(design_advice_list_raw, list):
#             design_advice_list = [item for item in design_advice_list_raw if isinstance(item, dict)]

#         valid_advices = [
#             advice for advice in design_advice_list
#             if advice.get("round") == current_round and advice.get("state") == True
#         ]
        
#         advice_text = "A creative timber pavilion." 
#         if valid_advices:
#             selected_advice = valid_advices[0] 
#             advice_text = selected_advice.get("proposal", advice_text)
#         else:
#             print(f"âš ï¸ UnifiedImageGenerationTask: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} ä¸” state ç‚º True çš„æœ‰æ•ˆè¨­è¨ˆå»ºè­°ã€‚å°‡ä½¿ç”¨é è¨­ææ¡ˆã€‚")

#         image_gen_prompt_text_template = active_config.outer_shell_gpt_prompt_template.format(
#             advice_text=advice_text,
#             improvement=improvement,
#             llm_output_language=active_language 
#         )
        
#         final_image_prompt_msg = current_llm.invoke([SystemMessage(content=image_gen_prompt_text_template)])
#         final_image_prompt = final_image_prompt_msg.content.strip() if hasattr(final_image_prompt_msg, "content") else "Error generating image prompt."
        
#         if "Error generating image prompt" in final_image_prompt:
#              print(f"âŒ UnifiedImageGenerationTask: LLM ç”Ÿæˆåœ–åƒ Prompt å¤±æ•—ã€‚")
#         else:
#             print(f"âœ… UnifiedImageGenerationTask: ç”Ÿæˆçš„æœ€çµ‚åœ–åƒ Prompt (ç”¨æ–¼æ‰€æœ‰èª¿ç”¨): '{final_image_prompt[:200]}...'")

#         existing_outer_prompts = self.state.get("outer_prompt", [])
#         if not isinstance(existing_outer_prompts, list):
#             existing_outer_prompts = []
        
#         prompts_from_other_rounds = [
#             p for p in existing_outer_prompts if isinstance(p, dict) and p.get("round") != current_round
#         ]
#         new_prompt_entry = {"round": current_round, "prompt": final_image_prompt}
#         self.state["outer_prompt"] = prompts_from_other_rounds + [new_prompt_entry]
#         print(f"â„¹ï¸ UnifiedImageGenerationTask: outer_prompt å·²æ›´æ–°ï¼Œç•¶å‰è¼ªæ¬¡ {current_round} çš„ prompt: '{final_image_prompt[:100]}...'")

#         generated_image_infos = []
        
#         base_render_cache_dir = os.path.join(os.getcwd(), "output", "cache", "render_cache")
#         os.makedirs(base_render_cache_dir, exist_ok=True) 


#         if "Error generating image prompt" in final_image_prompt or not final_image_prompt:
#             print(f"âŒ UnifiedImageGenerationTask: å›  Prompt ç”Ÿæˆå¤±æ•—ï¼Œè·³éæ‰€æœ‰åœ–åƒç”Ÿæˆèª¿ç”¨ã€‚")
#             existing_case_images = self.state.get("case_image", [])
#             if not isinstance(existing_case_images, list): existing_case_images = []
#             error_entry = {
#                 "round": current_round,
#                 "id_in_round": 1, 
#                 "filename": "Promptç”Ÿæˆå¤±æ•—", # Basename
#                 "image_url": "æœªç”Ÿæˆ",
#                 "description": "LLM failed to generate a valid image prompt.",
#                 "path": "ç„¡" 
#             }
#             images_from_other_rounds_img = [img for img in existing_case_images if isinstance(img, dict) and img.get("round") != current_round]
#             self.state["case_image"] = images_from_other_rounds_img + [error_entry]
#             return {
#                 "case_image": self.state["case_image"],
#                 "outer_prompt": self.state["outer_prompt"]
#             }

#         for call_idx in range(num_tool_calls):
#             print(f"â„¹ï¸ UnifiedImageGenerationTask: é–‹å§‹ç¬¬ {call_idx + 1}/{num_tool_calls} æ¬¡åœ–åƒç”Ÿæˆèª¿ç”¨...")
            
#             image_path_to_store = f"è™•ç†éŒ¯èª¤_{call_idx + 1}.png" 
#             image_url = "æœªç”Ÿæˆ"
#             full_description = ""
#             tool_error_desc = None
#             current_call_tool_text_response = None
#             file_type_for_url = "image/png" 

#             try:
#                 tool_output = generate_gemini_image.invoke({
#                     "prompt": final_image_prompt, 
#                     "image_inputs": [], 
#                     "i": 1
#                 })

#                 current_call_tool_generated_files = []
#                 current_call_tool_image_bytes_list = [] 
                
#                 if isinstance(tool_output, dict):
#                     current_call_tool_generated_files = tool_output.get("generated_files", [])
#                     current_call_tool_image_bytes_list = tool_output.get("image_bytes", []) 
#                     current_call_tool_text_response = tool_output.get("text_response")
#                     tool_error_desc = tool_output.get("error")

#                     if current_call_tool_text_response:
#                         print(f"  â†ªï¸ èª¿ç”¨ {call_idx + 1}: å·¥å…·æ–‡å­—å›é¥‹: '{current_call_tool_text_response[:100]}...'")

#                     if tool_error_desc:
#                         print(f"  âš ï¸ èª¿ç”¨ {call_idx + 1}: åœ–åƒç”Ÿæˆå·¥å…·å ±å‘ŠéŒ¯èª¤: {tool_error_desc}")
#                         image_path_to_store = f"å·¥å…·éŒ¯èª¤_èª¿ç”¨{call_idx + 1}.png" # This will become a basename later
#                     elif not current_call_tool_generated_files:
#                         print(f"  âš ï¸ èª¿ç”¨ {call_idx + 1}: å·¥å…·æœªè¿”å›ä»»ä½•æ–‡ä»¶è³‡è¨Šã€‚ Files: {current_call_tool_generated_files}")
#                         tool_error_desc = "Tool did not return any file information."
#                         image_path_to_store = f"ç„¡æ–‡ä»¶è³‡è¨Š_èª¿ç”¨{call_idx + 1}.png" # Basename
#                     else:
#                         file_info = current_call_tool_generated_files[0]
#                         file_type_for_url = file_info.get("file_type", "image/png") 
                        
#                         path_from_file_info = file_info.get("path")
#                         filename_from_file_info = file_info.get("filename") 

#                         resolved_image_path = None
#                         if isinstance(path_from_file_info, str) and os.path.isabs(path_from_file_info):
#                             resolved_image_path = path_from_file_info
#                             print(f"  DEBUG UnifiedImageGenerationTask: ä½¿ç”¨å·¥å…·æä¾›çš„çµ•å°è·¯å¾‘ 'path': '{resolved_image_path}'")
#                         elif isinstance(filename_from_file_info, str):
#                             resolved_image_path = os.path.join(base_render_cache_dir, os.path.basename(filename_from_file_info))
#                             print(f"  DEBUG UnifiedImageGenerationTask: å¾ 'filename' ('{filename_from_file_info}') å’Œ cache_dir æ§‹é€ è·¯å¾‘: '{resolved_image_path}'")
#                         else:
#                             tool_error_desc = "Tool returned invalid 'path' or 'filename' in file_info."
#                             image_path_to_store = f"è·¯å¾‘ç„¡æ•ˆ_èª¿ç”¨{call_idx + 1}.png" # Basename
#                             print(f"  âš ï¸ èª¿ç”¨ {call_idx + 1}: å·¥å…·è¿”å›çš„æ–‡ä»¶è³‡è¨Šä¸­ 'path' å’Œ 'filename' å‡ç„¡æ•ˆã€‚ Path: {path_from_file_info}, Filename: {filename_from_file_info}")
                        
#                         if resolved_image_path:
#                             image_path_to_store = resolved_image_path 
#                             print(f"  DEBUG UnifiedImageGenerationTask: è§£æå¾—åˆ°çš„å¾…æª¢æŸ¥è·¯å¾‘: '{image_path_to_store}' (é¡å‹: {type(image_path_to_store)}) for call {call_idx + 1}")

#                             img_bytes_data_for_url = None
#                             if current_call_tool_image_bytes_list and isinstance(current_call_tool_image_bytes_list[0].get("data"), bytes):
#                                 img_bytes_data_for_url = current_call_tool_image_bytes_list[0].get("data")
#                                 print(f"  â„¹ï¸ èª¿ç”¨ {call_idx + 1}: å·¥å…·ç›´æ¥è¿”å›äº†åœ–ç‰‡å­—ç¯€æ•¸æ“šã€‚")
                            
#                             if not os.path.exists(image_path_to_store):
#                                  print(f"  âš ï¸ èª¿ç”¨ {call_idx + 1}: è§£æå¾Œçš„åœ–ç‰‡è·¯å¾‘ '{image_path_to_store}' æ–‡ä»¶ä¸å­˜åœ¨ã€‚")
#                                  tool_error_desc = tool_error_desc or f"Resolved image file does not exist: {os.path.basename(image_path_to_store)}"
#                             else:
#                                 if not img_bytes_data_for_url:
#                                     print(f"  â„¹ï¸ èª¿ç”¨ {call_idx + 1}: æ–‡ä»¶ '{os.path.basename(image_path_to_store)}' å­˜åœ¨ï¼Œä½†å·¥å…·æœªç›´æ¥è¿”å›å­—ç¯€ã€‚å˜—è©¦å¾æ–‡ä»¶è®€å–ä»¥ç”ŸæˆURL...")
#                                     try:
#                                         with open(image_path_to_store, "rb") as f_read:
#                                             img_bytes_data_for_url = f_read.read()
#                                         print(f"    âœ… æˆåŠŸå¾æ–‡ä»¶è®€å–å­—ç¯€æ•¸æ“š: {os.path.basename(image_path_to_store)}")
#                                     except Exception as e_read_file:
#                                         print(f"    âš ï¸ å¾æ–‡ä»¶è®€å–å­—ç¯€æ•¸æ“šå¤±æ•—: {os.path.basename(image_path_to_store)}, Error: {e_read_file}")
#                                         tool_error_desc = tool_error_desc or f"Failed to read file bytes: {e_read_file}"
                                
#                                 if img_bytes_data_for_url:
#                                     try:
#                                         encoded_image = base64.b64encode(img_bytes_data_for_url).decode('utf-8')
#                                         image_url = f"data:{file_type_for_url};base64,{encoded_image}" 
#                                         print(f"  âœ… èª¿ç”¨ {call_idx + 1}: æˆåŠŸè™•ç†åœ–ç‰‡ä¸¦ç”ŸæˆURL: {os.path.basename(image_path_to_store)}")
#                                     except Exception as e_encode:
#                                         print(f"  âš ï¸ èª¿ç”¨ {call_idx + 1}: ç„¡æ³•ç·¨ç¢¼åœ–ç‰‡æ•¸æ“š for {os.path.basename(image_path_to_store)}: {e_encode}")
#                                         image_url = "ç·¨ç¢¼å¤±æ•—"
#                                         tool_error_desc = tool_error_desc or f"Encoding failed: {e_encode}"
#                                 elif not tool_error_desc : 
#                                      image_url = "è®€å–å­—ç¯€å¤±æ•—"
#                 else: 
#                     print(f"  âš ï¸ èª¿ç”¨ {call_idx + 1}: å·¥å…·è¿”å›äº†æ„å¤–çš„è¼¸å‡ºæ ¼å¼: {type(tool_output)}")
#                     tool_error_desc = "Unexpected tool output format."
#                     image_path_to_store = f"æ ¼å¼éŒ¯èª¤_èª¿ç”¨{call_idx + 1}.png" # Basename

#             except Exception as e_invoke:
#                 print(f"  ğŸ’¥ èª¿ç”¨ {call_idx + 1} æœŸé–“èª¿ç”¨å·¥å…·æ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e_invoke}")
#                 tool_error_desc = f"Exception during tool call: {e_invoke}"
#                 image_path_to_store = f"èª¿ç”¨ç•°å¸¸_{call_idx+1}.png" # Basename
            
#             base_description = (
#                 f"Agent: UnifiedImageGeneration; Round: {current_round}; "
#                 f"CallNum: {call_idx + 1}/{num_tool_calls}; " 
#                 f"Prompt: {final_image_prompt[:50]}..."
#             )
#             full_description = base_description
#             if current_call_tool_text_response: 
#                 full_description += f" | ToolTextResponse: {current_call_tool_text_response}"
#             if tool_error_desc:
#                 full_description += f" | Error: {tool_error_desc}"
#                 if "è™•ç†éŒ¯èª¤" in image_path_to_store and not os.path.isabs(image_path_to_store): 
#                     image_path_to_store = f"å…·é«”éŒ¯èª¤_{call_idx + 1}_{tool_error_desc[:20].replace(' ','_')}.png" # Basename

#             # æº–å‚™å­˜å„²åˆ° state çš„æ•¸æ“š
#             # final_path_for_state æ‡‰è©²æ˜¯çµ•å°è·¯å¾‘æˆ–æ¨™æº–åŒ–çš„éŒ¯èª¤æ¨™è¨˜
#             # final_filename_for_state æ‡‰è©²æ˜¯ basename æˆ–æ¨™æº–åŒ–çš„éŒ¯èª¤æ¨™è¨˜
            
#             final_path_for_state = "è·¯å¾‘éŒ¯èª¤æˆ–ç”Ÿæˆå¤±æ•—" # Default error path
#             final_filename_for_state = f"è™•ç†éŒ¯èª¤_{call_idx + 1}.png" # Default error filename (basename)

#             if os.path.isabs(image_path_to_store): # å¦‚æœ image_path_to_store å·²ç¶“æ˜¯çµ•å°è·¯å¾‘
#                 if os.path.exists(image_path_to_store):
#                     final_path_for_state = image_path_to_store
#                     final_filename_for_state = os.path.basename(image_path_to_store)
#                 else: # çµ•å°è·¯å¾‘ä½†æ–‡ä»¶ä¸å­˜åœ¨
#                     final_path_for_state = image_path_to_store # å­˜å„²å˜—è©¦çš„è·¯å¾‘
#                     final_filename_for_state = os.path.basename(image_path_to_store) + "_æ–‡ä»¶ä¸å­˜åœ¨"
#                     # image_url æ‡‰å·²æ˜¯ "æœªç”Ÿæˆ" æˆ–éŒ¯èª¤ç‹€æ…‹
#             elif not any(err_tag in image_path_to_store for err_tag in ["éŒ¯èª¤", "ç„¡æ•ˆ", "ç•°å¸¸"]):
#                 # å¦‚æœ image_path_to_store ä¸æ˜¯çµ•å°è·¯å¾‘ä¸”ä¸æ˜¯å·²çŸ¥éŒ¯èª¤æ¨™è¨˜ (ä¾‹å¦‚ï¼Œå®ƒæ˜¯å¾å·¥å…·è¿”å›çš„ basename)
#                 potential_abs_path = os.path.join(base_render_cache_dir, os.path.basename(image_path_to_store))
#                 if os.path.exists(potential_abs_path):
#                     final_path_for_state = potential_abs_path
#                     final_filename_for_state = os.path.basename(potential_abs_path)
#                 else:
#                     final_path_for_state = potential_abs_path # å­˜å„²å˜—è©¦çš„è·¯å¾‘
#                     final_filename_for_state = os.path.basename(image_path_to_store) + "_æ–‡ä»¶ä¸å­˜åœ¨"
#             else: # image_path_to_store æœ¬èº«å°±æ˜¯ä¸€å€‹éŒ¯èª¤æ¨™è¨˜ (ä¾‹å¦‚ "å·¥å…·éŒ¯èª¤_...")
#                 final_filename_for_state = image_path_to_store # ä½¿ç”¨é€™å€‹éŒ¯èª¤æ¨™è¨˜ä½œç‚ºæª”å
#                 # final_path_for_state ä¿æŒç‚º "è·¯å¾‘éŒ¯èª¤æˆ–ç”Ÿæˆå¤±æ•—"

#             generated_image_infos.append({
#                 "round": current_round,
#                 "id_in_round": call_idx + 1, 
#                 "filename": final_filename_for_state, # å­˜å„² basename æˆ–éŒ¯èª¤æ¨™è¨˜
#                 "image_url": image_url,
#                 "description": full_description,
#                 "path": final_path_for_state # å­˜å„²çµ•å°è·¯å¾‘æˆ–æ¨™æº–åŒ–éŒ¯èª¤æ¨™è¨˜
#             })
            
#             if call_idx < num_tool_calls - 1:
#                 print(f"  â„¹ï¸ èª¿ç”¨ {call_idx + 1} å®Œæˆå¾Œå»¶é² 5 ç§’...") 
#                 time.sleep(5)

#         existing_images_list = self.state.get("case_image", [])
#         if not isinstance(existing_images_list, list):
#             existing_images_list = []
        
#         images_from_other_rounds_img_final = [img for img in existing_images_list if isinstance(img, dict) and img.get("round") != current_round]
#         updated_images_list = images_from_other_rounds_img_final + generated_image_infos
#         self.state["case_image"] = updated_images_list

#         print(f"âœ… UnifiedImageGenerationTask: æ‰€æœ‰åœ–åƒç”Ÿæˆèª¿ç”¨å®Œæˆï¼Œå…±è™•ç† {len(generated_image_infos)} æ¢åœ–ç‰‡è³‡è¨Šã€‚")
#         if generated_image_infos:
#             for idx, info in enumerate(generated_image_infos):
#                  # åœ¨æœ€çµ‚æ—¥èªŒä¸­ï¼Œfilename æ‡‰è©²åªé¡¯ç¤ºæª”åéƒ¨åˆ†
#                  display_filename = info.get('filename', 'æœªçŸ¥æª”å')
#                  if isinstance(display_filename, str) and os.path.isabs(display_filename) and not any(err_tag in display_filename for err_tag in ["éŒ¯èª¤", "ç„¡æ•ˆ", "ç•°å¸¸", "å¤±æ•—"]):
#                      display_filename = os.path.basename(display_filename)

#                  print(f"  è©³ç´°åœ–ç‰‡è³‡è¨Š ({idx+1}): Filename='{display_filename}', Path='{info.get('path')}', URL: {'æœ‰å…§å®¹' if info.get('image_url') and info.get('image_url').startswith('data:image') else info.get('image_url', 'æœªå®šç¾©')}")
        
#         return {
#             "case_image": self.state["case_image"],
#             "outer_prompt": self.state["outer_prompt"] 
#         }

# GATE æª¢æŸ¥æ–¹æ¡ˆï¼ˆè«‹å›ç­”ï¼šæœ‰/æ²’æœ‰ï¼‰ OK
class GateCheck2:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state

        active_config = ensure_graph_overall_config(config)
        current_llm = active_config.llm_config.get_llm() # é›–ç„¶ img_recognition ä¸»è¦ä½¿ç”¨ï¼Œä½† prompt æ¨¡æ¿å¯èƒ½éœ€è¦èªè¨€
        active_language = active_config.llm_output_language

        current_round = self.state.get("current_round", 0) 
        case_images_raw = self.state.get("case_image", [])
        design_advice_list_raw = self.state.get("design_advice", [])
        
        case_images_list = []
        if isinstance(case_images_raw, list):
            case_images_list = [item for item in case_images_raw if isinstance(item, dict)]

        design_advice_list = []
        if isinstance(design_advice_list_raw, list):
            design_advice_list = [item for item in design_advice_list_raw if isinstance(item, dict)]


        # éæ¿¾å‡ºç•¶å‰è¼ªæ¬¡ä¸” state ç‚º True çš„è¨­è¨ˆæ–¹æ¡ˆï¼ˆå¿…é ˆæ˜¯å­—å…¸æ ¼å¼ï¼‰
        valid_advices = [
            advice for advice in design_advice_list
            if advice.get("round") == current_round and advice.get("state") == True
        ]
        
        advice_text = "ç„¡ç›®æ¨™"
        if valid_advices:
            selected_advice = valid_advices[0]
            advice_text = selected_advice.get("proposal", "ç„¡ç›®æ¨™")
        else:
            print(f"âš ï¸ GateCheck2: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} ä¸” state ç‚º True çš„æœ‰æ•ˆè¨­è¨ˆå»ºè­°ã€‚")


        # Filter images for the current round from self.state["case_image"]
        current_round_image_infos = [
            img_info for img_info in case_images_list
            if img_info.get("round") == current_round and 
               isinstance(img_info.get("filename"), str) and # Ensure filename is a string
               img_info.get("filename") not in ["æœªç”Ÿæˆ", "æ–‡ä»¶æœªæ‰¾åˆ°", "ç”Ÿæˆå¤±æ•—", "å·¥å…·å ±å‘ŠéŒ¯èª¤", "å·¥å…·æœªè¿”å›æ–‡ä»¶å", "Promptç”Ÿæˆå¤±æ•—", "ç„¡æ•ˆæ–‡ä»¶å"] # æ›´å¤šå¯èƒ½çš„éŒ¯èª¤æ¨™è¨˜
        ]

        if not current_round_image_infos:
            print(f"âš ï¸ GateCheck2: ç•¶å‰è¼ªæ¬¡ {current_round} ç„¡ç¬¦åˆæ¢ä»¶çš„ç”Ÿæˆåœ–ã€‚ç¯©é¸å¾Œçš„åˆ—è¡¨: {current_round_image_infos}")
            self.state["GATE2"] = "æ²¡æœ‰"
            self.state["GATE_REASON2"] = f"ç•¶å‰è¼ªæ¬¡ {current_round} ç„¡ç¬¦åˆæ¢ä»¶çš„ç”Ÿæˆåœ–ã€‚"
            # outer_prompt æ˜¯èˆŠçš„é‚è¼¯ï¼Œé€™è£¡æ‡‰è©²ä¸éœ€è¦å†è™•ç†å®ƒï¼Œå› ç‚º UnifiedImageGenerationTask ä¸ä¾è³´ outer_prompt
            # if not isinstance(self.state.get("outer_prompt"), list):
            #     self.state["outer_prompt"] = []
            # return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"], "outer_prompt": self.state["outer_prompt"]}
            return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"]}


        # ä¸å†éœ€è¦æ‰‹å‹•çµ„åˆè·¯å¾‘ï¼Œç›´æ¥å¾ state è®€å–
        image_paths_for_tool = []
        image_filenames_for_prompt_list = []

        current_round_image_infos.sort(key=lambda x: x.get("id_in_round", 0))

        for img_info in current_round_image_infos:
            image_path = img_info.get("path")
            filename = img_info.get("filename")

            # ä¸»è¦æª¢æŸ¥ path æ¬„ä½
            if image_path and isinstance(image_path, str) and os.path.exists(image_path):
                image_paths_for_tool.append(image_path)
                image_filenames_for_prompt_list.append(f"{os.path.basename(filename)} (ID: {img_info.get('id_in_round')})")
            else:
                print(f"âš ï¸ GateCheck2: åœ–ç‰‡æ–‡ä»¶åœ¨ state æä¾›çš„è·¯å¾‘ '{image_path}' ä¸­æœªæ‰¾åˆ°ã€‚ImgInfo: {img_info}")


        if not image_paths_for_tool:
            print(f"âš ï¸ GateCheck2: ç•¶å‰è¼ªæ¬¡ {current_round} æ‰€æœ‰åœ–ç‰‡æ–‡ä»¶å‡æœªæ‰¾åˆ°æˆ–è·¯å¾‘ç„¡æ•ˆã€‚")
            self.state["GATE2"] = "æ²¡æœ‰"
            self.state["GATE_REASON2"] = f"ç•¶å‰è¼ªæ¬¡ {current_round} æ‰€æœ‰åœ–ç‰‡æ–‡ä»¶å‡æœªæ‰¾åˆ°æˆ–è·¯å¾‘ç„¡æ•ˆã€‚"
            # outer_prompt è™•ç†åŒä¸Š
            # if not isinstance(self.state.get("outer_prompt"), list):
            #     self.state["outer_prompt"] = []
            # return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"], "outer_prompt": self.state["outer_prompt"]}
            return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"]}

        image_list_str_for_prompt = "\n".join(image_filenames_for_prompt_list)

        gate2_prompt_content = active_config.gate_check2_img_recognition_prompt_template.format(
            image_list_str=image_list_str_for_prompt,
            advice_text=advice_text,
            llm_output_language=active_language,
            current_round=current_round 
        )

        analysis_result_str = img_recognition.invoke({
            "image_paths": image_paths_for_tool,
            "prompt": gate2_prompt_content,
        })

        result_content = analysis_result_str.strip() if isinstance(analysis_result_str, str) else ""
        lines = [line.strip() for line in result_content.splitlines() if line.strip()]
        
        best_id_from_llm = "æ²¡æœ‰"
        reason_from_llm = ""

        if lines:
            first_line = lines[0]
            if "æ²¡æœ‰" in first_line or "no" in first_line.lower():
                best_id_from_llm = "æ²¡æœ‰"
            else:
                # å¾ filename (ID: X) ä¸­æå– ID
                # ä¾‹å¦‚ "gemini_gen_xxxx.png (ID: 1)" -> æå– 1
                id_matches = re.findall(r'\(ID:\s*(\d+)\)', first_line)
                if id_matches: # å¦‚æœæ˜¯ç›´æ¥æä¾›ID
                    try:
                        best_id_from_llm = int(id_matches[0])
                    except ValueError:
                         print(f"âš ï¸ GateCheck2: ç„¡æ³•å¾LLMå›è¦†çš„ç¬¬ä¸€è¡Œè§£æID (æ ¼å¼ä¸ç¬¦): '{first_line}'")
                         best_id_from_llm = "æ²¡æœ‰"
                else: # å˜—è©¦å¾ç´”æ•¸å­—ä¸­æå–
                    digit_matches = re.findall(r'\b\d+\b', first_line)
                    if digit_matches:
                        try:
                            best_id_from_llm = int(digit_matches[0])
                        except ValueError:
                            print(f"âš ï¸ GateCheck2: ç„¡æ³•å¾LLMå›è¦†çš„ç¬¬ä¸€è¡Œè§£ææ•¸å­—ID: '{first_line}'")
                            best_id_from_llm = "æ²¡æœ‰"
                    else:
                        print(f"âš ï¸ GateCheck2: LLM å›è¦†çš„ç¬¬ä¸€è¡Œæœªæ‰¾åˆ°æ•¸å­—ID: '{first_line}'")
                        best_id_from_llm = "æ²¡æœ‰"
            
            if len(lines) >= 2:
                reason_from_llm = lines[1]
            elif best_id_from_llm != "æ²¡æœ‰":
                 reason_from_llm = "LLM æœªæä¾›é¸æ“‡åŸå› ã€‚"
            else: # best_id_from_llm æ˜¯ "æ²¡æœ‰"
                 reason_from_llm = lines[1] if len(lines) >= 2 else "LLM æœªæä¾›æ”¹é€²å»ºè­°ã€‚"


        self.state["GATE2"] = best_id_from_llm
        self.state["GATE_REASON2"] = reason_from_llm

        # èˆŠçš„ outer_prompt ç‹€æ…‹æ›´æ–°é‚è¼¯å·²ä¸å†éœ€è¦ï¼Œå› ç‚º UnifiedImageGenerationTask ä¸ä¾è³´ outer_prompt
        # prompt_state_value = False if self.state["GATE2"] == "æ²¡æœ‰" else True
        # outer_prompt_list_for_state = self.state.get("outer_prompt", [])
        # if not isinstance(outer_prompt_list_for_state, list):
        #     outer_prompt_list_for_state = []
        # updated_outer_prompt = False
        # for prompt_entry in reversed(outer_prompt_list_for_state):
        #     if isinstance(prompt_entry, dict) and prompt_entry.get("round") == current_round:
        #         if "state" not in prompt_entry:
        #             prompt_entry["state"] = prompt_state_value
        #             updated_outer_prompt = True
        #             break 
        # if not updated_outer_prompt and outer_prompt_list_for_state:
        #     print(f"âš ï¸ GateCheck2: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} çš„å¤–æ®¼ prompt ä¾†æ›´æ–°ç‹€æ…‹ã€‚")
        # self.state["outer_prompt"] = outer_prompt_list_for_state

        print(f"ã€GateCheckCaseImageã€‘å·²æ”¶åˆ°æœ€ä½³è©•ä¼°çµæœï¼š{self.state.get('GATE2')}ï¼ŒåŸå› ï¼š{self.state.get('GATE_REASON2')} ğŸ˜Š")
        # return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"], "outer_prompt": self.state["outer_prompt"]}
        return {"GATE2": self.state["GATE2"], "GATE_REASON2": self.state["GATE_REASON2"]}

# æœªä¾†æƒ…å¢ƒç”Ÿæˆï¼šä½¿ç”¨ generate_gemini_image ç”Ÿæˆæ–¹æ¡ˆç´°ç¯€å’Œæœªä¾†è®ŠåŒ–åœ–
class FutureScenarioGenerationTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def _save_and_encode_image(self, image_bytes: bytes, absolute_filepath: str, extension: str, description: str, current_round: int, sub_id: str) -> dict:
        """è¼”åŠ©å‡½æ•¸ï¼šè™•ç†å·¥å…·è¿”å›çš„å–®å€‹åœ–ç‰‡å­—ç¯€å’Œæ–‡ä»¶åï¼Œé€²è¡Œç·¨ç¢¼ä¸¦æ§‹å»ºæ¨™æº–åœ–ç‰‡è³‡è¨Šå­—å…¸ã€‚
        absolute_filepath æ‡‰ç‚ºåœ–ç‰‡çš„çµ•å°è·¯å¾‘ã€‚
        """
        try:
            encoded_image = base64.b64encode(image_bytes).decode('utf-8')
            image_url = f"data:image/{extension};base64,{encoded_image}"
            
            return {
                "round": current_round,
                "id_in_round": sub_id, 
                "filename": os.path.basename(absolute_filepath), # å„²å­˜ç´”æª”æ¡ˆåç¨±
                "image_url": image_url,
                "description": description,
                "path": absolute_filepath # å„²å­˜çµ•å°è·¯å¾‘
            }
        except Exception as e:
            print(f"âš ï¸ FutureScenario (_save_and_encode_image): ç„¡æ³•ç·¨ç¢¼åœ–ç‰‡æ•¸æ“š for {absolute_filepath}: {e}")
            return {
                "round": current_round,
                "id_in_round": sub_id,
                "filename": os.path.basename(absolute_filepath) if absolute_filepath else "ç·¨ç¢¼å¤±æ•—.png",
                "image_url": "ç„¡",
                "description": f"åœ–ç‰‡æ•¸æ“šç·¨ç¢¼å¤±æ•—: {description}",
                "error": str(e),
                "path": absolute_filepath if absolute_filepath else "ç·¨ç¢¼å¤±æ•—è·¯å¾‘" 
            }

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state

        active_config = ensure_graph_overall_config(config)
        active_language = active_config.llm_output_language
        
        current_round = self.state.get("current_round", 0)
        design_advice_list_raw = self.state.get("design_advice", [])
        outer_prompt_list_raw = self.state.get("outer_prompt", [])
        case_images_list_raw = self.state.get("case_image", [])
        gate2_result_id = self.state.get("GATE2") 

        generated_future_images = []
        
        base_render_cache_dir = os.path.join(os.getcwd(), "output", "cache", "render_cache")
        os.makedirs(base_render_cache_dir, exist_ok=True)

        base_design_text_for_prompt = "ä¸€å€‹å…·æœ‰å‰µæ–°æ€§çš„æœ¨æ§‹é€ äº­å­ã€‚"
        design_advice_list = [item for item in design_advice_list_raw if isinstance(item, dict)]
        valid_current_round_advice = [
            adv for adv in design_advice_list 
            if adv.get("round") == current_round and adv.get("state") == True
        ]
        if valid_current_round_advice:
            base_design_text_for_prompt = valid_current_round_advice[0].get("proposal", base_design_text_for_prompt)
            print(f"FutureScenario: ä½¿ç”¨ä¾†è‡ª design_advice çš„åŸºç¤è¨­è¨ˆæ–‡æœ¬: {base_design_text_for_prompt[:100]}...")
        else:
            outer_prompt_list = [item for item in outer_prompt_list_raw if isinstance(item, dict)]
            current_round_outer_prompts = [p for p in outer_prompt_list if p.get("round") == current_round]
            if current_round_outer_prompts:
                base_design_text_for_prompt = current_round_outer_prompts[-1].get("prompt", base_design_text_for_prompt)
                print(f"FutureScenario: ä½¿ç”¨ä¾†è‡ª outer_prompt (è¼ªæ¬¡ {current_round}) çš„åŸºç¤è¨­è¨ˆæ–‡æœ¬: {base_design_text_for_prompt[:100]}...")
            else:
                print(f"FutureScenario: æœªæ‰¾åˆ°ç•¶å‰è¼ªæ¬¡æœ‰æ•ˆçš„ design_advice æˆ– outer_promptï¼Œä½¿ç”¨é è¨­è¨­è¨ˆæ–‡æœ¬ã€‚")
        
        base_image_bytes_for_input = None
        base_image_mime_type_for_input = "image/png" 
        base_image_filename_for_desc = "ç„¡åŸºç¤åœ–"
        image_inputs_for_tool = []

        print(f"FutureScenario: å˜—è©¦æŸ¥æ‰¾ GATE2 ID: {gate2_result_id} (é¡å‹: {type(gate2_result_id)}) åœ¨è¼ªæ¬¡ {current_round} çš„åŸºç¤åœ–ç‰‡ã€‚")
        if isinstance(gate2_result_id, int) and case_images_list_raw:
            case_images_list = [item for item in case_images_list_raw if isinstance(item, dict)]
            found_base_image = False
            for img_info in case_images_list:
                img_id_in_round = img_info.get("id_in_round")
                img_round = img_info.get("round")
                
                if img_round == current_round and img_id_in_round == gate2_result_id:
                    selected_path_from_case_image = img_info.get("path")
                    print(f"  FutureScenario: æ‰¾åˆ°å€™é¸åœ–ç‰‡è³‡è¨Š: ID={img_id_in_round}, Round={img_round}, Path='{selected_path_from_case_image}'")

                    if isinstance(selected_path_from_case_image, str) and \
                       selected_path_from_case_image.strip() and \
                       selected_path_from_case_image.lower() not in ["ç„¡", "è·¯å¾‘éŒ¯èª¤æˆ–ç”Ÿæˆå¤±æ•—", "none", "ç·¨ç¢¼å¤±æ•—"] and \
                       not any(err_tag in selected_path_from_case_image.lower() for err_tag in ["å¤±æ•—", "ç•°å¸¸", "ç„¡æ•ˆ", "é”™è¯¯"]) and \
                       os.path.exists(selected_path_from_case_image):
                        try:
                            with open(selected_path_from_case_image, "rb") as f_img:
                                base_image_bytes_for_input = f_img.read()
                            
                            image_url_from_case = img_info.get("image_url")
                            if isinstance(image_url_from_case, str) and image_url_from_case.startswith("data:image/"):
                                base_image_mime_type_for_input = image_url_from_case.split(';')[0].split(':')[1]
                            elif selected_path_from_case_image.lower().endswith((".jpg", ".jpeg")):
                                base_image_mime_type_for_input = "image/jpeg"
                            elif selected_path_from_case_image.lower().endswith(".png"):
                                base_image_mime_type_for_input = "image/png"
                            
                            base_image_filename_for_desc = os.path.basename(selected_path_from_case_image)
                            image_inputs_for_tool = [{"data": base_image_bytes_for_input, "mime_type": base_image_mime_type_for_input}]
                            print(f"FutureScenario: âœ… æˆåŠŸæ‰¾åˆ°ä¸¦åŠ è¼‰åŸºç¤åœ–ç‰‡: '{base_image_filename_for_desc}' (type: {base_image_mime_type_for_input}) ä½¿ç”¨è·¯å¾‘: {selected_path_from_case_image}")
                            found_base_image = True
                            break 
                        except Exception as e_read:
                            print(f"FutureScenario: âš ï¸ å˜—è©¦è®€å–åŸºç¤åœ–ç‰‡ '{selected_path_from_case_image}' å¤±æ•—: {e_read}")
                            base_image_bytes_for_input = None 
                            base_image_filename_for_desc = "è®€å–å¤±æ•—"
                            image_inputs_for_tool = []
                    else:
                        print(f"  FutureScenario: å€™é¸åœ–ç‰‡è·¯å¾‘ '{selected_path_from_case_image}' ç„¡æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨ã€‚")
            
            if not found_base_image:
                 print(f"FutureScenario: â„¹ï¸ åœ¨ case_image è¼ªæ¬¡ {current_round} ä¸­æœªæ‰¾åˆ° ID ç‚º {gate2_result_id} çš„æœ‰æ•ˆåŸºç¤åœ–ç‰‡æ¢ç›®ã€‚")
        else:
            if not isinstance(gate2_result_id, int):
                 print(f"FutureScenario: Gate2 çµæœ '{gate2_result_id}' ä¸æ˜¯æœ‰æ•ˆçš„æ•´æ•¸ IDã€‚")
            if not case_images_list_raw:
                 print("FutureScenario: case_image åˆ—è¡¨ç‚ºç©ºã€‚")
        
        if not image_inputs_for_tool: 
            print(f"FutureScenario: âš ï¸ æœªæ‰¾åˆ°æˆ–ç„¡æ³•è®€å–æœ‰æ•ˆçš„åŸºç¤åœ–ç‰‡ (GATE2 ID: {gate2_result_id})ï¼Œæˆ–åŸºç¤åœ–ç‰‡åˆ—è¡¨ç‚ºç©ºã€‚Phase 1 å’Œ Phase 2 å°‡ä¸ä½¿ç”¨åŸºç¤åœ–ç‰‡ã€‚")
            
        # --- Phase 1: Facade Detail and Construction Method Generation (Realigned with Phase 2 Logic) ---
        num_detail_images_to_generate = active_config.future_scenario_detail_image_count
        print(f"\n--- FutureScenario: Phase 1 - ç”Ÿæˆç«‹é¢ç´°ç¯€èˆ‡æ§‹é€ å·¥æ³•åœ– (è«‹æ±‚ {num_detail_images_to_generate} å¼µ) ---")
        
        detail_prompt_template = active_config.future_scenario_detail_generation_prompt_template # Use the new unified template
        facade_detail_prompt_text = detail_prompt_template.format(
            base_design_description=base_design_text_for_prompt, # Always provide base text
            num_images=num_detail_images_to_generate,
            llm_output_language=active_language
        )
        
        if image_inputs_for_tool:
            facade_detail_prompt_text += "\nA base image has been provided; please show detail modifications on it or generate details inspired by it."
            print(f"  ç«‹é¢ç´°ç¯€ Prompt (å«åŸºç¤åœ–æç¤º): {facade_detail_prompt_text[:200]}...")
        else:
            facade_detail_prompt_text += "\nNo base image was provided; generate details based on the text description."
            print(f"  ç«‹é¢ç´°ç¯€ Prompt (ç´”æ–‡å­—æç¤º): {facade_detail_prompt_text[:200]}...")

        try:
            tool_result_details = generate_gemini_image.invoke({
                "prompt": facade_detail_prompt_text,
                "image_inputs": image_inputs_for_tool, 
                "i": num_detail_images_to_generate 
            })

            if tool_result_details.get("error"):
                print(f"  âš ï¸ Phase 1 åœ–åƒç”Ÿæˆå¤±æ•—: {tool_result_details.get('error')}")
                for i_err in range(num_detail_images_to_generate):
                    generated_future_images.append({
                        "round": current_round, "id_in_round": f"detail_err_batch_img{i_err+1}",
                        "filename": f"ç´°ç¯€ç”Ÿæˆå¤±æ•—_img{i_err+1}.png", "image_url": "ç„¡", "path": "ç„¡",
                        "description": f"ç«‹é¢ç´°ç¯€åœ–æ‰¹æ¬¡ç”Ÿæˆå¤±æ•—: {tool_result_details.get('error')}",
                        "error": tool_result_details.get('error')
                    })
            else:
                returned_files_info_detail = tool_result_details.get("generated_files", [])
                returned_bytes_info_detail = tool_result_details.get("image_bytes", []) 
                
                print(f"  DEBUG Phase 1: å·¥å…·è¿”å› {len(returned_files_info_detail)} å€‹æ–‡ä»¶è³‡è¨Š, {len(returned_bytes_info_detail)} å€‹å­—ç¯€é …ç›®ã€‚é æœŸ {num_detail_images_to_generate} å€‹ã€‚")

                if returned_files_info_detail : 
                    print(f"  Phase 1 å·¥å…·è¿”å› {len(returned_files_info_detail)} å€‹æ–‡ä»¶è³‡è¨Šã€‚")
                    if len(returned_files_info_detail) != num_detail_images_to_generate:
                        print(f"  âš ï¸ Phase 1 è­¦å‘Š: å·¥å…·è¿”å›çš„æ–‡ä»¶æ•¸é‡ ({len(returned_files_info_detail)}) èˆ‡é æœŸ ({num_detail_images_to_generate}) ä¸ç¬¦ã€‚")

                    for idx, file_info in enumerate(returned_files_info_detail):
                        filename_from_tool = file_info.get("filename") 
                        img_mime = file_info.get("file_type", "image/png") 
                        img_bytes = None
                        img_abs_path = None

                        if isinstance(filename_from_tool, str) and filename_from_tool.strip():
                            img_abs_path = os.path.join(base_render_cache_dir, os.path.basename(filename_from_tool))
                        else:
                            print(f"    âš ï¸ Phase 1: å·¥å…·è¿”å›çš„ç¬¬ {idx+1} å€‹æ–‡ä»¶è³‡è¨Šä¸­æª”åç„¡æ•ˆ: '{filename_from_tool}'")
                            generated_future_images.append({
                                "round": current_round, "id_in_round": f"detail_badfilename_img{idx+1}", 
                                "filename": f"ç´°ç¯€æª”åç„¡æ•ˆ{idx+1}.png", "image_url":"ç„¡", "path": "ç„¡", 
                                "description": f"ç´°ç¯€åœ– {idx+1} æª”åç„¡æ•ˆ"
                            })
                            continue 

                        if returned_bytes_info_detail and idx < len(returned_bytes_info_detail) and isinstance(returned_bytes_info_detail[idx], dict):
                            img_bytes = returned_bytes_info_detail[idx].get("data")
                        
                        if not img_bytes and os.path.exists(img_abs_path):
                            print(f"    Phase 1: å­—ç¯€æ•¸æ“šæœªç”±å·¥å…·ç›´æ¥æä¾›ï¼Œå˜—è©¦å¾è·¯å¾‘è®€å–: {img_abs_path}")
                            try:
                                with open(img_abs_path, "rb") as f_read_bytes:
                                    img_bytes = f_read_bytes.read()
                                print(f"      âœ… æˆåŠŸå¾æ–‡ä»¶è®€å–å­—ç¯€: {os.path.basename(img_abs_path)}")
                            except Exception as e_read_manual:
                                print(f"      âš ï¸ å¾æ–‡ä»¶è®€å–å­—ç¯€å¤±æ•—: {os.path.basename(img_abs_path)}, éŒ¯èª¤: {e_read_manual}")
                                img_bytes = None 

                        if img_bytes: 
                            extension = img_mime.split('/')[-1] if '/' in img_mime else 'png'
                            desc_detail = (f"ç«‹é¢/æ§‹é€ ç´°ç¯€åœ– {idx+1}/{len(returned_files_info_detail)} "
                                           f"(åŸºæ–¼: {base_image_filename_for_desc}, "
                                           f"Prompté¡å‹: {'åœ–ç”Ÿæ–‡+åœ–èª¿æ•´' if image_inputs_for_tool else 'ç´”æ–‡ç”Ÿåœ–'}, " # Adjusted description
                                           f"åŸå§‹æè¿°: {base_design_text_for_prompt[:30]}...)")
                            
                            saved_image_info_detail = self._save_and_encode_image(
                                image_bytes=img_bytes, absolute_filepath=img_abs_path, extension=extension,
                                description=desc_detail, current_round=current_round,
                                sub_id=f"detail_img{idx+1}" # Consistent sub_id
                            )
                            generated_future_images.append(saved_image_info_detail)
                            print(f"    âœ… æˆåŠŸè™•ç†ç´°ç¯€åœ–: {saved_image_info_detail.get('filename')}") 
                        else:
                            err_reason = "ç„¡æœ‰æ•ˆå­—ç¯€æ•¸æ“š (å·¥å…·æœªæä¾›ä¸”ç„¡æ³•å¾æ–‡ä»¶è®€å–)"
                            if not os.path.exists(img_abs_path): 
                                err_reason = f"æ–‡ä»¶æ–¼è·¯å¾‘ {img_abs_path} æœªæ‰¾åˆ°æˆ–ç„¡æ³•è®€å–"
                            print(f"    âš ï¸ Phase 1 ç„¡æ³•è™•ç†ç¬¬ {idx+1} å€‹ç´°ç¯€åœ–ç‰‡ (æª”å: {os.path.basename(filename_from_tool if filename_from_tool else 'æœªçŸ¥')}, åŸå› : {err_reason})ã€‚")
                            generated_future_images.append({
                                "round": current_round, "id_in_round": f"detail_nodata_img{idx+1}", 
                                "filename": os.path.basename(filename_from_tool) if filename_from_tool else f"ç´°ç¯€æ•¸æ“šç„¡æ•ˆ{idx+1}.png", 
                                "image_url":"ç„¡", 
                                "path": img_abs_path if img_abs_path else "ç„¡æ•ˆè·¯å¾‘", 
                                "description": f"ç´°ç¯€åœ– {idx+1} æ•¸æ“šç„¡æ•ˆæˆ–æ–‡ä»¶ç¼ºå¤± ({err_reason})"
                            })
                else: 
                    print(f"  âš ï¸ Phase 1 åœ–åƒç”Ÿæˆå·¥å…·æœªè¿”å›ä»»ä½•æ–‡ä»¶è³‡è¨Šã€‚")
                    for i_miss in range(num_detail_images_to_generate):
                         generated_future_images.append({"round": current_round, "id_in_round": f"detail_missing_all_files_img{i_miss+1}", "filename": f"ç´°ç¯€æ–‡ä»¶è³‡è¨Šç¼ºå¤±{i_miss+1}.png", "image_url":"ç„¡", "path": "ç„¡", "description": f"ç´°ç¯€åœ– {i_miss+1} æ‰€æœ‰æ–‡ä»¶è³‡è¨Šç¼ºå¤±"})
        except Exception as e:
            print(f"  ğŸ’¥ Phase 1 èª¿ç”¨ generate_gemini_image æ‰¹è™•ç†ç•°å¸¸: {e}")
            for i_exc in range(num_detail_images_to_generate):
                generated_future_images.append({
                    "round": current_round, "id_in_round": f"detail_exc_batch_img{i_exc+1}",
                    "filename": f"ç´°ç¯€ç”Ÿæˆç•°å¸¸_img{i_exc+1}.png", "image_url": "ç„¡", "path": "ç„¡",
                    "description": f"ç«‹é¢ç´°ç¯€åœ–æ‰¹æ¬¡ç”Ÿæˆç•°å¸¸: {e}", "error": str(e)
                })
        
        print(f"    â„¹ï¸ Phase 1 ç´°ç¯€åœ–åƒç”Ÿæˆå®Œæˆï¼Œå»¶é² 3 ç§’...")
        time.sleep(5)

        # --- Phase 2: Aging Scenario Generation (10, 20, 30 years) - BATCH MODE ---
        print(f"\n--- FutureScenario: Phase 2 - ç”Ÿæˆæœªä¾†10ã€20ã€30å¹´è®ŠåŒ–åœ– (æ‰¹æ¬¡è«‹æ±‚ 3 å¼µ) ---")
        years_to_simulate = [10, 20, 30]
        num_aging_images_to_generate = len(years_to_simulate)
        
        aging_prompt_template_text = active_config.future_scenario_aging_generation_prompt_template
        
        final_aging_prompt_for_tool = aging_prompt_template_text.format(
            base_design_description=base_design_text_for_prompt, 
            num_images=num_aging_images_to_generate, 
            llm_output_language=active_language
        )
        final_aging_prompt_for_tool += (
            f"\nImportant: Generate exactly {num_aging_images_to_generate} images, "
            "representing the aging at 10, 20, and 30 years respectively. "
            "Maintain consistency in the base structure across the aging sequence. "
            "The images should be returned in the order of 10 years, then 20 years, then 30 years."
        )
        if image_inputs_for_tool:
            final_aging_prompt_for_tool += "\nA base image has been provided; please show aging modifications on it."
        else:
            final_aging_prompt_for_tool += "\nNo base image was provided; generate based on the text description."

        print(f"    æœªä¾†è€åŒ–å ´æ™¯ (æ‰¹æ¬¡) Prompt: {final_aging_prompt_for_tool[:300]}...")


        try:
            tool_result_aging_batch = generate_gemini_image.invoke({
                "prompt": final_aging_prompt_for_tool,
                "image_inputs": image_inputs_for_tool, 
                "i": num_aging_images_to_generate 
            })

            if tool_result_aging_batch.get("error"):
                print(f"    âš ï¸ Phase 2 æ‰¹æ¬¡åœ–åƒç”Ÿæˆå¤±æ•—: {tool_result_aging_batch.get('error')}")
                for i_err_aging in range(num_aging_images_to_generate):
                    year_val_err = years_to_simulate[i_err_aging] if i_err_aging < len(years_to_simulate) else "unknown"
                    generated_future_images.append({
                        "round": current_round, "id_in_round": f"aging_batch_err_img{i_err_aging+1}_{year_val_err}yr",
                        "filename": f"è€åŒ–æ‰¹æ¬¡å¤±æ•—_img{i_err_aging+1}_{year_val_err}yr.png", "image_url": "ç„¡", "path": "ç„¡",
                        "description": f"è€åŒ–åœ–æ‰¹æ¬¡ç”Ÿæˆå¤±æ•— ({year_val_err} yr): {tool_result_aging_batch.get('error')}",
                        "error": tool_result_aging_batch.get('error')
                    })
            else:
                returned_files_info_aging = tool_result_aging_batch.get("generated_files", [])
                returned_bytes_info_aging = tool_result_aging_batch.get("image_bytes", [])

                print(f"  DEBUG Phase 2: å·¥å…·è¿”å› {len(returned_files_info_aging)} å€‹è€åŒ–æ–‡ä»¶è³‡è¨Š, {len(returned_bytes_info_aging)} å€‹è€åŒ–å­—ç¯€é …ç›®ã€‚é æœŸ {num_aging_images_to_generate} å€‹ã€‚")

                if returned_files_info_aging: 
                    print(f"  Phase 2 å·¥å…·è¿”å› {len(returned_files_info_aging)} å€‹è€åŒ–åœ–ç‰‡æ–‡ä»¶è³‡è¨Šã€‚")
                    if len(returned_files_info_aging) != num_aging_images_to_generate:
                         print(f"  âš ï¸ Phase 2 è­¦å‘Š: å·¥å…·è¿”å›çš„æ–‡ä»¶æ•¸é‡ ({len(returned_files_info_aging)}) èˆ‡é æœŸ ({num_aging_images_to_generate}) ä¸ç¬¦ã€‚")

                    for idx, file_info_aging in enumerate(returned_files_info_aging):
                        filename_from_tool_aging = file_info_aging.get("filename")
                        img_mime_aging = file_info_aging.get("file_type", "image/png")
                        img_bytes_aging = None
                        img_abs_path_aging = None
                        current_year_for_desc = years_to_simulate[idx] if idx < len(years_to_simulate) else f"batch_idx{idx+1}"


                        if isinstance(filename_from_tool_aging, str) and filename_from_tool_aging.strip():
                            img_abs_path_aging = os.path.join(base_render_cache_dir, os.path.basename(filename_from_tool_aging))
                        else:
                            print(f"    âš ï¸ Phase 2: å·¥å…·è¿”å›çš„ç¬¬ {idx+1} å€‹è€åŒ–æ–‡ä»¶è³‡è¨Šä¸­æª”åç„¡æ•ˆ: '{filename_from_tool_aging}'")
                            generated_future_images.append({
                                "round": current_round, "id_in_round": f"aging_badfilename_img{idx+1}_{current_year_for_desc}yr",
                                "filename": f"è€åŒ–æª”åç„¡æ•ˆ{idx+1}_{current_year_for_desc}yr.png", "image_url": "ç„¡", "path": "ç„¡",
                                "description": f"{current_year_for_desc}å¹´å¾Œè®ŠåŒ–åœ–æª”åç„¡æ•ˆ"
                            })
                            continue

                        if returned_bytes_info_aging and idx < len(returned_bytes_info_aging) and isinstance(returned_bytes_info_aging[idx], dict):
                            img_bytes_aging = returned_bytes_info_aging[idx].get("data")
                        
                        if not img_bytes_aging and os.path.exists(img_abs_path_aging):
                            print(f"    Phase 2: å­—ç¯€æ•¸æ“šæœªç”±å·¥å…·ç›´æ¥æä¾› ({current_year_for_desc}yr)ï¼Œå˜—è©¦å¾è·¯å¾‘è®€å–: {img_abs_path_aging}")
                            try:
                                with open(img_abs_path_aging, "rb") as f_read_bytes_aging:
                                    img_bytes_aging = f_read_bytes_aging.read()
                                print(f"      âœ… æˆåŠŸå¾æ–‡ä»¶è®€å–å­—ç¯€ ({current_year_for_desc}yr): {os.path.basename(img_abs_path_aging)}")
                            except Exception as e_read_manual_aging:
                                print(f"      âš ï¸ å¾æ–‡ä»¶è®€å–å­—ç¯€å¤±æ•— ({current_year_for_desc}yr): {os.path.basename(img_abs_path_aging)}, éŒ¯èª¤: {e_read_manual_aging}")
                                img_bytes_aging = None

                        if img_bytes_aging: 
                            extension_aging = img_mime_aging.split('/')[-1] if '/' in img_mime_aging else 'png'
                            desc_aging = (f"æ–¹æ¡ˆ {current_year_for_desc} å¹´å¾Œè®ŠåŒ–åœ– "
                                          f"(åŸºæ–¼: {base_image_filename_for_desc}, "
                                          f"Prompté¡å‹: {'åœ–ç”Ÿåœ–' if image_inputs_for_tool else 'æ–‡ç”Ÿåœ–'}, "
                                          f"åŸå§‹æè¿°: {base_design_text_for_prompt[:30]}...)")
                            
                            saved_image_info_aging = self._save_and_encode_image(
                                image_bytes=img_bytes_aging, absolute_filepath=img_abs_path_aging, extension=extension_aging,
                                description=desc_aging, current_round=current_round,
                                sub_id=f"aging_{current_year_for_desc}yr_img{idx+1}" 
                            )
                            generated_future_images.append(saved_image_info_aging)
                            print(f"    âœ… æˆåŠŸè™•ç† {current_year_for_desc} å¹´å¾Œè®ŠåŒ–åœ–: {saved_image_info_aging.get('filename')}")
                        else:
                            err_reason_aging = "ç„¡æœ‰æ•ˆå­—ç¯€æ•¸æ“š (å·¥å…·æœªæä¾›ä¸”ç„¡æ³•å¾æ–‡ä»¶è®€å–)"
                            if not os.path.exists(img_abs_path_aging): # Check again
                                err_reason_aging = f"æ–‡ä»¶æ–¼è·¯å¾‘ {img_abs_path_aging} æœªæ‰¾åˆ°æˆ–ç„¡æ³•è®€å–"
                            print(f"    âš ï¸ Phase 2 ç„¡æ³•è™•ç†ç¬¬ {idx+1} å€‹è€åŒ–åœ–ç‰‡ ({current_year_for_desc}yr, æª”å: {os.path.basename(filename_from_tool_aging if filename_from_tool_aging else 'æœªçŸ¥')}, åŸå› : {err_reason_aging})ã€‚")
                            generated_future_images.append({
                                "round": current_round, "id_in_round": f"aging_nodata_batch_img{idx+1}_{current_year_for_desc}yr", 
                                "filename": os.path.basename(filename_from_tool_aging) if filename_from_tool_aging else f"è€åŒ–æ•¸æ“šç„¡æ•ˆ{idx+1}.png", 
                                "image_url":"ç„¡", 
                                "path": img_abs_path_aging if img_abs_path_aging else "ç„¡æ•ˆè·¯å¾‘", 
                                "description": f"{current_year_for_desc}å¹´å¾Œè®ŠåŒ–åœ–æ•¸æ“šç„¡æ•ˆæˆ–æ–‡ä»¶ç¼ºå¤± ({err_reason_aging})"
                            })
                else: 
                    print(f"  âš ï¸ Phase 2 åœ–åƒç”Ÿæˆå·¥å…·æœªè¿”å›ä»»ä½•è€åŒ–åœ–ç‰‡æ–‡ä»¶è³‡è¨Šã€‚")
                    for i_miss_aging in range(num_aging_images_to_generate):
                        year_val_miss = years_to_simulate[i_miss_aging] if i_miss_aging < len(years_to_simulate) else f"batch_idx{i_miss_aging+1}"
                        generated_future_images.append({
                            "round": current_round, "id_in_round": f"aging_missing_all_files_img{i_miss_aging+1}_{year_val_miss}yr", 
                            "filename": f"è€åŒ–æ–‡ä»¶è³‡è¨Šç¼ºå¤±{i_miss_aging+1}.png", "image_url":"ç„¡", "path": "ç„¡", 
                            "description": f"è€åŒ–åœ– {year_val_miss}yr æ‰€æœ‰æ–‡ä»¶è³‡è¨Šç¼ºå¤±"
                        })
        except Exception as e_batch_aging:
            print(f"  ğŸ’¥ Phase 2 èª¿ç”¨ generate_gemini_image æ‰¹è™•ç†è€åŒ–å ´æ™¯ç•°å¸¸: {e_batch_aging}")
            for i_exc_aging in range(num_aging_images_to_generate):
                year_val_exc = years_to_simulate[i_exc_aging] if i_exc_aging < len(years_to_simulate) else f"batch_idx{i_exc_aging+1}"
                generated_future_images.append({
                    "round": current_round, "id_in_round": f"aging_exc_batch_img{i_exc_aging+1}_{year_val_exc}yr",
                    "filename": f"è€åŒ–ç”Ÿæˆç•°å¸¸_img{i_exc_aging+1}.png", "image_url": "ç„¡", "path": "ç„¡",
                    "description": f"è€åŒ–åœ–æ‰¹æ¬¡ç”Ÿæˆç•°å¸¸ ({year_val_exc}yr): {e_batch_aging}", "error": str(e_batch_aging)
                })

        existing_future_images = self.state.get("future_image", [])
        if not isinstance(existing_future_images, list):
            existing_future_images = []
        
        images_from_other_rounds_future = [
            img for img in existing_future_images if isinstance(img, dict) and img.get("round") != current_round
        ]
        self.state["future_image"] = images_from_other_rounds_future + generated_future_images

        print(f"\nâœ… FutureScenarioGenerationTask å®Œæˆï¼Œç¸½å…±è™•ç† {len(generated_future_images)} å¼µåœ–ç‰‡è³‡è¨Š (åŒ…å«ç´°ç¯€åœ–èˆ‡è€åŒ–åœ–)ã€‚")
        if generated_future_images:
            for idx, info in enumerate(generated_future_images):
                 print(f"  è©³ç´°åœ–ç‰‡è³‡è¨Š ({idx+1}): ID='{info.get('id_in_round')}', Filename='{info.get('filename')}', Path='{info.get('path')}', URL: {'æœ‰å…§å®¹' if info.get('image_url') and info.get('image_url').startswith('data:image/') else info.get('image_url', 'æœªå®šç¾©')}")

        return {"future_image": self.state["future_image"]}

# ç”Ÿæˆ 3D =ï¼šæ ¹æ“š Glb æª”å‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡ç”Ÿæˆå·¥å…·ï¼‰ç”Ÿæˆ 3D 
class Generate3DPerspective:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict): 
        if state is not None:
            self.state = state        
        
        active_config = ensure_graph_overall_config(config) 

        current_round = self.state.get("current_round", 0)
        case_images_raw = self.state.get("case_image", [])
        selected_image_id_from_gate2 = self.state.get("GATE2") 
        
        selected_image_full_path_for_3d = None

        print(f"Generate3DPerspective: å˜—è©¦æŸ¥æ‰¾ GATE2 ID: {selected_image_id_from_gate2} (é¡å‹: {type(selected_image_id_from_gate2)}) åœ¨è¼ªæ¬¡ {current_round} çš„åŸºç¤åœ–ç‰‡ã€‚")
        if isinstance(selected_image_id_from_gate2, int) and case_images_raw:
            case_images_list = [item for item in case_images_raw if isinstance(item, dict)]
            found_base_image_3d = False
            for img_info in case_images_list:
                img_id_in_round = img_info.get("id_in_round")
                img_round = img_info.get("round")
                # UnifiedImageGenerationTask æ‡‰è©²åœ¨ "path" ä¸­å­˜å„²æœ‰æ•ˆè·¯å¾‘
                raw_filename_path = img_info.get("path") 
                
                print(f"  æª¢æŸ¥ case_image é …ç›®: id_in_round={img_id_in_round}, round={img_round}, path='{raw_filename_path}' (é¡å‹: {type(raw_filename_path)})")

                if (img_round == current_round and
                    img_id_in_round == selected_image_id_from_gate2 and
                    isinstance(raw_filename_path, str) and
                    raw_filename_path not in ["ç„¡æ•ˆè·¯å¾‘æˆ–éŒ¯èª¤", "ç„¡"] and # æ’é™¤ä½”ä½ç¬¦
                    not any(err_placeholder in raw_filename_path.lower() for err_placeholder in 
                             ["promptç”Ÿæˆå¤±æ•—", "å·¥å…·éŒ¯èª¤_", "ç„¡æ–‡ä»¶_", "æ ¼å¼éŒ¯èª¤_", "èª¿ç”¨ç•°å¸¸_", "è™•ç†éŒ¯èª¤_"]) and
                    os.path.exists(raw_filename_path)): 
                    
                    selected_image_full_path_for_3d = raw_filename_path
                    print(f"â„¹ï¸ Generate3DPerspective: âœ… æˆåŠŸé¸ä¸­åœ–ç‰‡ (ä¾†è‡ªGateCheck2 ID {selected_image_id_from_gate2}), "
                          f"çµ•å°è·¯å¾‘ '{selected_image_full_path_for_3d}' ç”¨æ–¼3Dç”Ÿæˆã€‚")
                    found_base_image_3d = True
                    break
            if not found_base_image_3d:
                 print(f"âš ï¸ Generate3DPerspective: é›–ç„¶ GateCheck2 é¸æ“‡äº† ID {selected_image_id_from_gate2}, "
                       f"ä½†åœ¨ case_image è¼ªæ¬¡ {current_round} ä¸­æœªæ‰¾åˆ°å°æ‡‰çš„æœ‰æ•ˆåœ–ç‰‡æ–‡ä»¶è·¯å¾‘ã€‚")
        else:
            print(f"âš ï¸ Generate3DPerspective: GateCheck2 æœªæä¾›æœ‰æ•ˆçš„åœ–ç‰‡ ID (GATE2: {selected_image_id_from_gate2}) "
                  f"æˆ– case_image ç‚ºç©ºï¼Œç„¡æ³•é¸æ“‡ç”¨æ–¼3Dç”Ÿæˆçš„åœ–ç‰‡ã€‚")


        if not selected_image_full_path_for_3d:
            print(f"âš ï¸ Generate3DPerspective: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} çš„æœ‰æ•ˆæ¸²æŸ“åœ–æ–‡ä»¶è·¯å¾‘ç”¨æ–¼3Dç”Ÿæˆã€‚")
            if not isinstance(self.state.get("perspective_3D"), list): self.state["perspective_3D"] = []
            if not isinstance(self.state.get("model_3D"), list): self.state["model_3D"] = []
            no_result_entry = {"round": current_round, "status": "æ— æœ‰æ•ˆæ¸²æŸ“å›¾ç‰‡è¿›è¡Œ3Dç”Ÿæˆ", "filename":"æ— ", "path":"æ— "}
            self.state["perspective_3D"] = custom_add_messages(self.state.get("perspective_3D", []), [no_result_entry])
            self.state["model_3D"] = custom_add_messages(self.state.get("model_3D", []), [no_result_entry])
            return {"perspective_3D": self.state["perspective_3D"], "model_3D": self.state["model_3D"]}

        
        # å®šç¾© 3D æª”æ¡ˆçš„å¿«å–ç›®éŒ„
        base_3d_cache_dir = os.path.join(os.getcwd(), "output", "model_cache")
        os.makedirs(base_3d_cache_dir, exist_ok=True)

        gen_3d_output_dict = generate_3D.invoke({
            "image_path": str(selected_image_full_path_for_3d), 
            "current_round": current_round,
            # "prompt": active_config.llm_output_language # ç§»é™¤ï¼Œé™¤é generate_3D å·¥å…·æ˜ç¢ºéœ€è¦æ­¤ prompt éµ
        })

        video_filename_from_tool = "æ— ç”Ÿæˆç»“æœ"
        model_filename_from_tool = "æ— æ¨¡å‹"
        video_path_from_tool = "æ— "
        model_path_from_tool = "æ— "

        if isinstance(gen_3d_output_dict, dict):
            # å‡è¨­ generate_3D è¿”å›çš„ video å’Œ model æ˜¯åŒ…å« 'filename' (å¯èƒ½æ˜¯çµ•å°è·¯å¾‘æˆ–åƒ…æª”å) çš„å­—å…¸æˆ–ç›´æ¥æ˜¯è·¯å¾‘/æª”åå­—ç¬¦ä¸²
            video_output = gen_3d_output_dict.get("video")
            model_output = gen_3d_output_dict.get("model")

            # è¼”åŠ©å‡½æ•¸ä¾†è§£ææª”åå’Œè·¯å¾‘
            def process_tool_output(output, cache_dir):
                raw_path = None
                if isinstance(output, dict) and isinstance(output.get("filename"), str):
                    raw_path = output.get("filename")
                elif isinstance(output, str):
                    raw_path = output
                
                if raw_path and raw_path.strip():
                    filename = os.path.basename(raw_path)
                    # å¦‚æœå·¥å…·å›å‚³çš„ä¸æ˜¯çµ•å°è·¯å¾‘ï¼Œå‰‡å°‡å…¶èˆ‡å¿«å–ç›®éŒ„çµ„åˆ
                    if os.path.isabs(raw_path):
                        return filename, raw_path
                    else:
                        return filename, os.path.join(cache_dir, filename)
                return None, None

            video_filename_from_tool, video_path_from_tool = process_tool_output(video_output, base_3d_cache_dir)
            model_filename_from_tool, model_path_from_tool = process_tool_output(model_output, base_3d_cache_dir)

            if not video_filename_from_tool:
                video_filename_from_tool = "è¿”å›æ ¼å¼æ— æ•ˆ(video)"
                video_path_from_tool = "æ— "
            
            if not model_filename_from_tool:
                model_filename_from_tool = "è¿”å›æ ¼å¼æ— æ•ˆ(model)"
                model_path_from_tool = "æ— "
            
            if gen_3d_output_dict.get("error"):
                 print(f"âš ï¸ Generate3DPerspective: 3Dç”Ÿæˆå·¥å…·å ±å‘ŠéŒ¯èª¤: {gen_3d_output_dict.get('error')}")
                 # å¦‚æœæœ‰éŒ¯èª¤ï¼Œæ¨™è¨˜æª”åä¸¦å°‡è·¯å¾‘è¨­ç‚ºç„¡æ•ˆ
                 video_filename_from_tool = f"å·¥å…·éŒ¯èª¤_{video_filename_from_tool}" if video_filename_from_tool else "å·¥å…·éŒ¯èª¤"
                 model_filename_from_tool = f"å·¥å…·éŒ¯èª¤_{model_filename_from_tool}" if model_filename_from_tool else "å·¥å…·éŒ¯èª¤"
                 video_path_from_tool = "æ— "
                 model_path_from_tool = "æ— "


        else:
            print(f"âš ï¸ Generate3DPerspective: 3Dç”Ÿæˆå·¥å…·æœªè¿”å›å­—å…¸ã€‚è¿”å›: {gen_3d_output_dict}")
            video_filename_from_tool = "å·¥å…·è¿”å›æ ¼å¼é”™è¯¯"
            model_filename_from_tool = "å·¥å…·è¿”å›æ ¼å¼é”™è¯¯"


        video_entry = {"round": current_round, "type": "video", "filename": video_filename_from_tool, "path": video_path_from_tool}
        model_entry = {"round": current_round, "type": "model", "filename": model_filename_from_tool, "path": model_path_from_tool}
        
        existing_perspective_3d_list = self.state.get("perspective_3D", [])
        if not isinstance(existing_perspective_3d_list, list):
            existing_perspective_3d_list = []
            
        existing_model_3d_list = self.state.get("model_3D", [])
        if not isinstance(existing_model_3d_list, list):
            existing_model_3d_list = []

        self.state["perspective_3D"] = custom_add_messages(existing_perspective_3d_list, [video_entry])
        self.state["model_3D"] = custom_add_messages(existing_model_3d_list, [model_entry])


        print(f"âœ… ç”Ÿæˆ 3D å®Œæˆ: å½±ç‰‡æ–‡ä»¶:{video_filename_from_tool} (è·¯å¾‘: {video_path_from_tool})ã€æ¨¡å‹æ–‡ä»¶:{model_filename_from_tool} (è·¯å¾‘: {model_path_from_tool})")
        return {"perspective_3D": self.state["perspective_3D"], "model_3D": self.state["model_3D"]}

# æ·±åº¦è©•ä¼°ä»»å‹™ï¼šå‘¼å« LLMï¼ˆä½¿ç”¨åœ–ç‰‡è¾¨è­˜å·¥å…·ï¼‰å°ç”Ÿæˆåœ–èˆ‡æœªä¾†æƒ…å¢ƒåœ–é€²è¡Œæ·±åº¦è©•ä¼° OK
class DeepEvaluationTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def _extract_total_score(self, text_score_str: str) -> float:
        """å¾è©•ä¼°æ–‡æœ¬ä¸­æå–ç¸½åˆ†ã€‚"""
        if not isinstance(text_score_str, str):
            return 0.0
            
        match = re.search(r"\*\*ç¸½åˆ†æ•¸:([\d.]+)\*\*", text_score_str)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                return 0.0
        else:
            # ä½œç‚ºå‚™ç”¨ï¼ŒæŸ¥æ‰¾æ–‡æœ¬ä¸­å¯èƒ½å‡ºç¾çš„æ‰€æœ‰æ•¸å­—ä¸¦å–æœ€å¤§å€¼
            numbers = re.findall(r"(\d+(?:\.\d+)?)", text_score_str)
            if numbers:
                try:
                    return max(map(float, numbers))
                except ValueError:
                    return 0.0
            return 0.0

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state      

        active_config = ensure_graph_overall_config(config)
        active_language = active_config.llm_output_language
        
        # OUTPUT_EVAL_DIR = "./output/" # å·²åœ¨ DeepEvalTask å†…éƒ¨å¤„ç†
        # os.makedirs(OUTPUT_EVAL_DIR, exist_ok=True) 

        current_round = self.state.get("current_round", 0) 
        eval_results_list_raw = self.state.get("evaluation_result", []) 
        if not isinstance(eval_results_list_raw, list):
            eval_results_list_raw = []
            
        future_img_list_raw = self.state.get("future_image", []) 
        perspective_3d_list_raw = self.state.get("perspective_3D", []) 
        design_advice_list_raw = self.state.get("design_advice", []) 

        future_img_list = [item for item in future_img_list_raw if isinstance(item, dict)]
        perspective_3d_list = [item for item in perspective_3d_list_raw if isinstance(item, dict)] # åŒ…å« 3D å½±ç‰‡/æ¨¡å‹ä¿¡æ¯
        design_advice_list = [item for item in design_advice_list_raw if isinstance(item, dict)]

        valid_advices = [
            advice for advice in design_advice_list
            if advice.get("round") == current_round and advice.get("state") == True 
        ]
        advice_text = "ç„¡ç›®æ¨™" 
        if valid_advices:
            advice_text = valid_advices[0].get("proposal", "ç„¡ç›®æ¨™")
        else:
            print(f"âš ï¸ DeepEvaluationTask: æœªæ‰¾åˆ°è¼ªæ¬¡ {current_round} ä¸” state ç‚º True çš„æœ‰æ•ˆè¨­è¨ˆå»ºè­°ã€‚")

        # æå–æœ‰æ•ˆåœ–ç‰‡è·¯å¾‘ç”¨æ–¼ img_recognition
        valid_future_image_paths_for_eval = []
        future_image_filenames_for_log = [] # ç”¨æ–¼æ—¥èªŒè¨˜éŒ„
        if future_img_list:
            for img_info in future_img_list:
                # å‡è¨­ "path" å­—æ®µå­˜å„²äº†ç”± FutureScenarioGenerationTask é©—è­‰éçš„çµ•å°è·¯å¾‘
                file_path = img_info.get("path") 
                img_filename = img_info.get("filename", "æœªçŸ¥æ–‡ä»¶å") # ç”¨æ–¼æ—¥èªŒ

                if isinstance(file_path, str) and \
                   file_path not in ["ç„¡æ•ˆè·¯å¾‘æˆ–éŒ¯èª¤", "ç„¡", "ç´°ç¯€æ•¸æ“šç„¡æ•ˆ", "è€åŒ–æ•¸æ“šç„¡æ•ˆ", "ç´°ç¯€åˆ—è¡¨ç„¡æ•ˆ", "è€åŒ–åˆ—è¡¨ç„¡æ•ˆ"] and \
                   not any(err_tag in file_path for err_tag in ["å¤±æ•—", "ç•°å¸¸", "ç„¡æ•ˆ"]) and \
                   os.path.exists(file_path):
                    valid_future_image_paths_for_eval.append(file_path)
                    future_image_filenames_for_log.append(os.path.basename(img_filename))
                else:
                    print(f"âš ï¸ DeepEvaluationTask: å¾ future_image ä¸­éæ¿¾æ‰ç„¡æ•ˆæ¢ç›®: path='{file_path}', filename='{img_filename}'")
        
        img_keywords_content = "ç„¡æœ‰æ•ˆæœªä¾†åœ–ç‰‡å¯ä¾›åˆ†æé—œéµå­—ã€‚"
        img_eval_text = "ç„¡æœ‰æ•ˆæœªä¾†åœ–ç‰‡å¯ä¾›è©•ä¼°ã€‚"

        if valid_future_image_paths_for_eval:
            print(f"â„¹ï¸ DeepEvaluationTask: ä½¿ç”¨ {len(valid_future_image_paths_for_eval)} å¼µæœ‰æ•ˆæœªä¾†åœ–ç‰‡é€²è¡Œè©•ä¼°: {future_image_filenames_for_log}")
            try:
                keyword_prompt_for_img = active_config.deep_eval_keyword_img_recognition_prompt_template.format(
                    llm_output_language=active_language
                )
                img_key_output_str = img_recognition.invoke({
                    "image_paths": valid_future_image_paths_for_eval, 
                    "prompt": keyword_prompt_for_img
                })        
                img_keywords_content = img_key_output_str.strip() if isinstance(img_key_output_str, str) else "åœ–ç‰‡é—œéµè©ç”Ÿæˆå¤±æ•—æˆ–ç‚ºç©ºã€‚"
                print(f"  åŸºæ–¼åœ–ç‰‡ç”Ÿæˆçš„é—œéµè©ï¼š{img_keywords_content[:200]}...")

                if "ç„¡æ³•è­˜åˆ¥" in img_keywords_content or not img_keywords_content.strip() : # æª¢æŸ¥æ˜¯å¦æœ‰æ„ç¾©çš„é—œéµè©
                     print(f"  âš ï¸ é—œéµè©ç”Ÿæˆå¯èƒ½æœªæˆåŠŸï¼Œé—œéµè©å…§å®¹ç‚º: '{img_keywords_content}'")
                     # å¯ä»¥é¸æ“‡ä¸é€²è¡Œå¾ŒçºŒçš„åœ–ç‰‡è©•ä¼°ï¼Œæˆ–è€…è®“LLMå˜—è©¦è©•ä¼°
                
                # å³ä½¿é—œéµè©ç”Ÿæˆä¸ä½³ï¼Œä¹Ÿå˜—è©¦é€²è¡Œåœ–ç‰‡è©•ä¼°
                img_eval_prompt_content = active_config.deep_eval_img_eval_img_recognition_prompt_template.format(
                    rag_msg=img_keywords_content, # rag_msg å¯ä»¥æ˜¯ç©ºå­—ç¬¦ä¸²æˆ–æç¤ºä¿¡æ¯
                    llm_output_language=active_language
                )
                img_eval_output_str = img_recognition.invoke({
                    "image_paths": valid_future_image_paths_for_eval,
                    "prompt": img_eval_prompt_content
                })
                img_eval_text = img_eval_output_str.strip() if isinstance(img_eval_output_str, str) else "åœ–ç‰‡è©•ä¼°å·¥å…·è¿”å›ç©ºã€‚"
            except Exception as e_img_rec:
                print(f"âŒ DeepEvaluationTask: åœ–ç‰‡è¾¨è­˜éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e_img_rec}")
                img_keywords_content = f"åœ–ç‰‡è¾¨è­˜éŒ¯èª¤: {e_img_rec}"
                img_eval_text = f"åœ–ç‰‡è©•ä¼°éŒ¯èª¤: {e_img_rec}"
        else:
            print("âš ï¸ DeepEvaluationTask: ç„¡æœ‰æ•ˆæœªä¾†åœ–ç‰‡å‚³éçµ¦ img_recognitionã€‚")
        

        # æå–æœ‰æ•ˆ3Då½±ç‰‡/æ¨¡å‹è·¯å¾‘ç”¨æ–¼ video_recognition
        valid_perspective_3d_paths_for_eval = []
        perspective_3d_filenames_for_log = []
        if perspective_3d_list:
            for p3d_info in perspective_3d_list:
                # å‡è¨­ "path" å­—æ®µå­˜å„²äº†ç”± Generate3DPerspective é©—è­‰éçš„çµ•å°è·¯å¾‘
                file_path = p3d_info.get("path")
                p3d_filename = p3d_info.get("filename", "æœªçŸ¥3Dæ–‡ä»¶")

                if isinstance(file_path, str) and \
                   file_path not in ["ç„¡æ•ˆè·¯å¾‘æˆ–éŒ¯èª¤", "ç„¡"] and \
                   not any(err_tag in file_path for err_tag in ["å¤±æ•—", "ç•°å¸¸", "ç„¡æ•ˆ", "é”™è¯¯"]) and \
                   os.path.exists(file_path):
                    valid_perspective_3d_paths_for_eval.append(file_path)
                    perspective_3d_filenames_for_log.append(os.path.basename(p3d_filename))
                else:
                    print(f"âš ï¸ DeepEvaluationTask: å¾ perspective_3D ä¸­éæ¿¾æ‰ç„¡æ•ˆæ¢ç›®: path='{file_path}', filename='{p3d_filename}'")

        vid_eval_text = "ç„¡æœ‰æ•ˆ3Dæ¨¡å‹/å½±ç‰‡å¯ä¾›è©•ä¼°ã€‚"
        vid_total_score = 0.0  # åˆå§‹åŒ–å½±ç‰‡è©•ä¼°ç¸½åˆ†

        if valid_perspective_3d_paths_for_eval:
            print(f"â„¹ï¸ DeepEvaluationTask: ä½¿ç”¨ {len(valid_perspective_3d_paths_for_eval)} å€‹æœ‰æ•ˆ3Dæ–‡ä»¶é€²è¡Œè©•ä¼°: {perspective_3d_filenames_for_log}")
            
            vid_eval_prompt_content = active_config.deep_eval_vid_eval_video_recognition_prompt_template.format(
                rag_msg=img_keywords_content, 
                llm_output_language=active_language
            )

            all_vid_eval_texts = []
            # è¿­ä»£è™•ç†æ¯ä¸€å€‹æœ‰æ•ˆçš„ 3D æª”æ¡ˆ
            for video_path in valid_perspective_3d_paths_for_eval:
                try:
                    print(f"  - æ­£åœ¨è©•ä¼°æ–‡ä»¶: {os.path.basename(video_path)}")
                    # ä¿®æ­£ï¼šä½¿ç”¨ 'video_path' (å–®æ•¸) ä¸¦å‚³å…¥å–®ä¸€è·¯å¾‘
                    vid_eval_output_str = video_recognition.invoke({ 
                        "video_path": video_path, 
                        "prompt": vid_eval_prompt_content
                    })
                    
                    current_vid_eval_text = vid_eval_output_str.strip() if isinstance(vid_eval_output_str, str) else f"å½±ç‰‡è©•ä¼°å·¥å…·å° {os.path.basename(video_path)} è¿”å›ç©ºã€‚"
                    
                    # å¾æœ¬æ¬¡è©•ä¼°ä¸­æå–åˆ†æ•¸ä¸¦ç´¯åŠ 
                    current_score = self._extract_total_score(current_vid_eval_text)
                    vid_total_score += current_score
                    
                    # ç‚ºå ±å‘Šæ·»åŠ æ¨™é¡Œä»¥ä¾¿å€åˆ†
                    all_vid_eval_texts.append(f"--- è©•ä¼°å ±å‘Š: {os.path.basename(video_path)} ---\n{current_vid_eval_text}\nScore from this file: {current_score}")

                except Exception as e_vid_rec:
                    error_text = f"å½±ç‰‡/3Dæ¨¡å‹ '{os.path.basename(video_path)}' è¾¨è­˜éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e_vid_rec}"
                    print(f"âŒ {error_text}")
                    all_vid_eval_texts.append(error_text)

            if all_vid_eval_texts:
                vid_eval_text = "\n\n".join(all_vid_eval_texts)

        else:
            print("âš ï¸ DeepEvaluationTask: ç„¡æœ‰æ•ˆ3Då½±ç‰‡/æ¨¡å‹è·¯å¾‘å‚³éçµ¦ video_recognitionã€‚")


        img_total_score = self._extract_total_score(img_eval_text)
        # vid_total_score å·²åœ¨è¿´åœˆä¸­è¨ˆç®—å®Œæˆ
        all_score_for_round = img_total_score + vid_total_score

        current_eval_result_entry = {
            "current_round": current_round,            
            "eval_result_image": img_eval_text,
            "eval_result_video": vid_eval_text,
            "total_score_calculated": all_score_for_round
        }
        
        updated_eval_results_list = custom_add_messages(eval_results_list_raw, [current_eval_result_entry])
        self.state["evaluation_result"] = updated_eval_results_list

        current_eval_count_entry = {str(current_round): all_score_for_round}
        eval_count_list_raw = self.state.get("evaluation_count", [])
        if not isinstance(eval_count_list_raw, list):
            eval_count_list_raw = []
        updated_eval_count_list = custom_add_messages(eval_count_list_raw, [current_eval_count_entry])
        self.state["evaluation_count"] = updated_eval_count_list

        md_content_full = f"""# Evaluation Result for Round {current_round}

## Image Evaluation (Based on {future_img_list[0].get('filename') or 'N/A'})
{img_eval_text}
Score: {img_total_score}

## Video/3D Model Evaluation (Based on {perspective_3d_list[0].get('filename') or 'N/A'})
{vid_eval_text}
Score: {vid_total_score}

## Combined Score for Round: {all_score_for_round}
"""
        eval_file_path_md = os.path.join("./output/", f"eval_result_{current_round}.md")
        try:
            with open(eval_file_path_md, "w", encoding="utf-8") as f:
                f.write(md_content_full)
            print(f"âœ… è©•ä¼°å ±å‘Šå·²å„²å­˜è‡³: {eval_file_path_md}")
        except IOError as e:
            print(f"âŒ ç„¡æ³•å„²å­˜è©•ä¼°å ±å‘Š: {e}")

        self.state["current_round"] = current_round + 1  
        print(f"âœ… æ·±åº¦è©•ä¼°å®Œæˆï¼Œé€²å…¥ä¸‹ä¸€è¼ªæ¬¡: {self.state['current_round']}")
        print(f"ğŸ“Œ æœ¬è¼ªç¸½è©•åˆ†: {all_score_for_round}")
        return {
            "evaluation_result": self.state["evaluation_result"],
            "evaluation_count": self.state["evaluation_count"],
            "current_round": self.state["current_round"]
        }

# è©•ä¼°æª¢æŸ¥ä»»å‹™ï¼šæ ¹æ“šè©•ä¼°æ¬¡æ•¸æ±ºå®šæµç¨‹è·¯ç”±ï¼ˆåƒè€ƒæ¢ä»¶åˆ†æ”¯ç¯„æœ¬é‚è¼¯ï¼‰
class EvaluationCheckTask:
    def __init__(self, state: GlobalState):
        self.state = state

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state
        
        active_config = ensure_graph_overall_config(config) # è™•ç† config

        current_iteration_count = self.state.get("current_round", 0) # current_round ä»£è¡¨å·²å®Œæˆçš„è¼ªæ¬¡ï¼Œä¸‹ä¸€è¼ªæ˜¯ current_round + 1
        max_rounds = active_config.max_evaluation_rounds

        # current_round å¾0é–‹å§‹è¨ˆæ•¸ã€‚å¦‚æœ max_rounds æ˜¯3ï¼Œ
        # ç•¶ current_round æ˜¯ 0, 1, 2 æ™‚ï¼Œè¡¨ç¤ºé‚„å¯ä»¥ç¹¼çºŒè¿­ä»£ã€‚
        # ç•¶ current_round è®Šæˆ 3 æ™‚ï¼Œè¡¨ç¤ºå·²ç¶“å®Œæˆäº†3è¼ªï¼Œæ‡‰è©²çµæŸã€‚
        if current_iteration_count < max_rounds:
            self.state["evaluation_status"] = "NO"
            print(f"EvaluationCheckTaskï¼šç›®å‰å·²å®Œæˆ {current_iteration_count} è¼ªè©•ä¼°ï¼Œæœªé”åˆ°æœ€å¤§è¼ªæ•¸ {max_rounds}ï¼Œå°‡è¿”å› RAGdesignThinking åŸ·è¡Œä¸‹ä¸€è¼ªã€‚")
        else:
            self.state["evaluation_status"] = "YES"
            print(f"EvaluationCheckTaskï¼šç›®å‰å·²å®Œæˆ {current_iteration_count} è¼ªè©•ä¼°ï¼Œå·²é”åˆ°æœ€å¤§è¼ªæ•¸ {max_rounds}ï¼Œæµç¨‹çµæŸã€‚")
        return {"evaluation_status": self.state["evaluation_status"]}

# ç¸½è©•ä¼°ä»»å‹™(ç”¨æˆ¶å¯ä»‹å…¥)
class FinalEvaluationTask:
    def __init__(self, state: dict, short_term=None, long_term=None): 
        self.state = state
        self.short_term = short_term if short_term is not None else get_short_term_memory()
        self.long_term = long_term if long_term is not None else get_long_term_store()

    def run(self, state: GlobalState, config: GraphOverallConfig | dict):
        if state is not None:
            self.state = state

        active_config = ensure_graph_overall_config(config)
        current_llm = active_config.llm_config.get_llm()
        active_language = active_config.llm_output_language

        eval_results_list_final = self.state.get("evaluation_result", []) 
        eval_counts_list_final = self.state.get("evaluation_count", []) 
        
        if not isinstance(eval_results_list_final, list): eval_results_list_final = []
        if not isinstance(eval_counts_list_final, list): eval_counts_list_final = []

        short_memory_content = self.short_term.retrieve_all() if hasattr(self.short_term, "retrieve_all") else ""
        long_memory_content = self.long_term.retrieve_all() if hasattr(self.long_term, "retrieve_all") else ""
        current_round_for_final_eval = self.state.get("current_round", "æœªçŸ¥æœ€çµ‚è¼ªæ¬¡") 

        eval_results_formatted_str = ""
        for res_item in eval_results_list_final: 
            if isinstance(res_item, dict):
                round_num = res_item.get("current_round", "æœªçŸ¥è¼ªæ¬¡")
                eval_results_formatted_str += f"\nè¼ªæ¬¡ {round_num}ï¼š"
                eval_results_formatted_str += f"\n - åœ–ç‰‡è©•ä¼°ï¼š{res_item.get('eval_result_image', 'ç„¡')}" 
                eval_results_formatted_str += f"\n - 3D è¦–è§’è©•ä¼°ï¼š{res_item.get('eval_result_video', 'ç„¡')}\n" 
        
        eval_counts_formatted_str = ""
        for count_dict_item in eval_counts_list_final: 
             if isinstance(count_dict_item, dict):
                for round_key_str, score_val in count_dict_item.items(): 
                    eval_counts_formatted_str += f"è¼ªæ¬¡ {round_key_str}ï¼šç¸½åˆ† {score_val}\n"

        summary_prompt_content_final = active_config.final_evaluation_summary_prompt_template.format(
            eval_results_formatted=eval_results_formatted_str if eval_results_formatted_str else "ç„¡è©•ä¼°çµæœ",
            eval_counts_formatted=eval_counts_formatted_str if eval_counts_formatted_str else "ç„¡è©•åˆ†çµæœ",
            short_memory=short_memory_content if short_memory_content else "ç„¡çŸ­æœŸè¨˜æ†¶",
            long_memory=long_memory_content if long_memory_content else "ç„¡é•·æœŸè¨˜æ†¶",
            current_round=current_round_for_final_eval,
            llm_output_language=active_language
        )

        llm_response_msg_final = current_llm.invoke([SystemMessage(content=summary_prompt_content_final)])
        final_text_output = llm_response_msg_final.content if hasattr(llm_response_msg_final, "content") else "LLMç¸½è©•ä¼°ç”Ÿæˆå¤±æ•—ã€‚"

        self.state["final_evaluation"] = final_text_output

        print("âœ… ç¸½è©•ä¼°ä»»å‹™å®Œæˆï¼")
        print(f"ğŸ“Œ ç¸½è©•ä¼°çµæœ:\n{final_text_output}")
        return {"final_evaluation": self.state["final_evaluation"]}

# =============================================================================
# å»ºç«‹å·¥ä½œæµç¨‹åœ– (Graph Setup)
# =============================================================================
workflow = StateGraph(GlobalState, config_schema=GraphOverallConfig)

initial_state = {
    "è¨­è¨ˆç›®æ¨™xè¨­è¨ˆéœ€æ±‚xæ–¹æ¡ˆåå¥½": [],
    "design_summary": "",
    "analysis_img": "",
    "site_analysis": "",
    "design_advice": [],
    "case_image": [],
    "outer_prompt": [],
    "future_image": [],
    "perspective_3D": [],
    "model_3D": [],
    "GATE1": "åˆå§‹å€¼",
    "GATE2": "åˆå§‹å€¼",
    "GATE_REASON1": "",
    "GATE_REASON2": "",
    "current_round": 0,
    "evaluation_count": [],
    "evaluation_status": "",
    "evaluation_result": [],
    "final_evaluation": ""
}

question_task = QuestionTask(initial_state)
site_analysis_task = SiteAnalysisTask(initial_state)
rag_thinking = RAGdesignThinking(initial_state)
gate_check1 = GateCheck1(initial_state)
shell_prompt_task = OuterShellPromptTask(initial_state) # è¨»é‡‹æ‰
image_render_task = CaseScenarioGenerationTask(initial_state) # è¨»é‡‹æ‰
# unified_image_gen_task = UnifiedImageGenerationTask(initial_state) # æ–°å¢
gate_check2 = GateCheck2(initial_state)
future_scenario_task = FutureScenarioGenerationTask(initial_state)
generate_p3d_task = Generate3DPerspective(initial_state)
deep_evaluation_task = DeepEvaluationTask(initial_state)
evaluation_check_task = EvaluationCheckTask(initial_state)
final_eval_task = FinalEvaluationTask(initial_state)

workflow.set_entry_point("question_summary")

workflow.add_node("question_summary", question_task.run)
workflow.add_node("analyze_site", site_analysis_task.run)
workflow.add_node("designThinking", rag_thinking.run)
workflow.add_node("GateCheck1", gate_check1.run)
workflow.add_node("shell_prompt", shell_prompt_task.run) # è¨»é‡‹æ‰
workflow.add_node("image_render", image_render_task.run) # è¨»é‡‹æ‰
# workflow.add_node("img_generation", unified_image_gen_task.run) # æ–°å¢
workflow.add_node("GateCheck2", gate_check2.run)
workflow.add_node("future_scenario", future_scenario_task.run) # æ¢å¾©ç¨ç«‹ç¯€é»
workflow.add_node("generate_3D", generate_p3d_task.run)       # æ¢å¾©ç¨ç«‹ç¯€é»
workflow.add_node("deep_evaluation", deep_evaluation_task.run)
workflow.add_node("evaluation_check", evaluation_check_task.run)
workflow.add_node("final_eval", final_eval_task.run)

workflow.add_edge("question_summary", "analyze_site")
workflow.add_edge("analyze_site", "designThinking")
workflow.add_edge("designThinking", "GateCheck1")
workflow.add_edge("shell_prompt", "image_render") # è¨»é‡‹æ‰
workflow.add_edge("image_render", "GateCheck2") # è¨»é‡‹æ‰
# workflow.add_edge("img_generation", "GateCheck2") # æ–°å¢
workflow.add_edge("future_scenario", "generate_3D") # æ¢å¾©é‚Š
workflow.add_edge("generate_3D", "deep_evaluation") # æ¢å¾©é‚Š
workflow.add_edge("deep_evaluation", "evaluation_check")
workflow.add_edge("final_eval", END)

workflow.add_conditional_edges(
    "GateCheck1",
    lambda state: "YES" if state.get("GATE1") == "æœ‰" else "NO",
    {
        "YES": "shell_prompt",  # ä¿®æ”¹ï¼šæŒ‡å‘æ–°ç¯€é»
        "NO": "designThinking"  
    }
)

workflow.add_conditional_edges(
    "GateCheck2",
    lambda state: "YES" if isinstance(state.get("GATE2"), int) else "NO",
    { 
        "YES": "future_scenario", # ä¿®æ”¹ï¼šGateCheck2 çš„ YES åˆ†æ”¯æŒ‡å‘ future_scenario
        "NO": "shell_prompt" 
    }
)

workflow.add_conditional_edges("evaluation_check",lambda state: state["evaluation_status"],
    { "NO": "designThinking",   "YES": "final_eval"  })

graph = workflow.compile()

graph.name = "Multi-Agent System for Timber Pavilion Design"
